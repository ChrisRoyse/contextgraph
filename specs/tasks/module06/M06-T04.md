# M06-T04: Implement Candle EmbeddingProvider

```yaml
metadata:
  id: "M06-T04"
  title: "Implement Candle EmbeddingProvider"
  module: "module-06"
  module_name: "Stub Elimination"
  layer: "logic"
  priority: "critical"
  estimated_hours: 8
  created: "2026-01-04"
  status: "blocked"
  dependencies:
    - "M06-T02"  # EmbeddingProvider trait
  spec_refs:
    - "constitution.yaml:305-323"   # 12-model embedding
    - "contextprd.md:83-101"        # Model specs
```

## Problem Statement

Real embeddings require a production-ready provider implementing the `EmbeddingProvider` trait using Candle ML framework with GPU acceleration.

### Desired State

```rust
// Production embedding using MiniLM
let provider = CandleEmbeddingProvider::new(CandleConfig::default())?;
let result = provider.embed("test content").await?;
assert_eq!(result.dimensions, 384); // MiniLM dimension
assert!(result.latency_ms < 10.0);  // Sub-10ms latency
```

## Context

The `context-graph-embeddings` crate already has Candle models. This task wires them into the `EmbeddingProvider` trait interface.

## Input Context Files

```
crates/context-graph-embeddings/src/lib.rs              # Current API
crates/context-graph-embeddings/src/models/             # Candle models
crates/context-graph-core/src/traits/embedding_provider.rs # Trait from M06-T02
```

## Prerequisites

- [ ] M06-T02 complete (EmbeddingProvider trait exists)
- [ ] Candle dependency configured
- [ ] Model weights available or downloadable

## Scope

### In Scope

- Implement `CandleEmbeddingProvider` in embeddings crate
- Support MiniLM-L6-v2 model (384 dimensions)
- GPU acceleration when available
- CPU fallback
- Batch embedding optimization
- Model warm-up on initialization

### Out of Scope

- Multi-model fusion (Module 08)
- 12-model pipeline (future)
- Custom model training

## Definition of Done

### Signatures

```rust
// File: crates/context-graph-embeddings/src/providers/candle_provider.rs
use context_graph_core::traits::{EmbeddingProvider, EmbeddingResult};
use candle_core::{Device, Tensor};
use candle_nn::VarBuilder;

/// Configuration for Candle embedding provider
#[derive(Debug, Clone)]
pub struct CandleConfig {
    pub model_id: String,       // e.g., "sentence-transformers/all-MiniLM-L6-v2"
    pub device: DeviceConfig,   // CPU, CUDA, or Metal
    pub max_batch_size: usize,
    pub max_sequence_length: usize,
    pub cache_dir: Option<PathBuf>,
}

impl Default for CandleConfig {
    fn default() -> Self {
        Self {
            model_id: "sentence-transformers/all-MiniLM-L6-v2".to_string(),
            device: DeviceConfig::Auto,
            max_batch_size: 64,
            max_sequence_length: 512,
            cache_dir: None,
        }
    }
}

#[derive(Debug, Clone)]
pub enum DeviceConfig {
    Auto,
    Cpu,
    Cuda(usize),  // GPU index
    Metal,
}

/// Production embedding provider using Candle
pub struct CandleEmbeddingProvider {
    model: BertModel,
    tokenizer: Tokenizer,
    device: Device,
    config: CandleConfig,
}

impl CandleEmbeddingProvider {
    pub async fn new(config: CandleConfig) -> Result<Self, EmbeddingError>;
    pub fn device(&self) -> &Device;
    pub fn is_gpu(&self) -> bool;
}

#[async_trait]
impl EmbeddingProvider for CandleEmbeddingProvider {
    async fn embed(&self, content: &str) -> CoreResult<EmbeddingResult>;
    async fn embed_batch(&self, contents: &[String]) -> CoreResult<Vec<EmbeddingResult>>;
    fn dimensions(&self) -> usize;    // 384 for MiniLM
    fn model_id(&self) -> &str;
    fn is_ready(&self) -> bool;
}
```

### Constraints

- MUST support GPU (CUDA) when available
- MUST fallback to CPU gracefully
- Batch embedding MUST be more efficient than N single calls
- Embeddings MUST be normalized (unit length)
- MUST handle long text (truncation at max_sequence_length)
- Model loading MUST be cached (not reloaded per request)
- `embed()` latency target: <10ms single, <50ms batch of 64

### Verification

```bash
# 1. Compile with candle feature
cargo build -p context-graph-embeddings --features candle

# 2. Unit tests
cargo test -p context-graph-embeddings candle_provider

# 3. Verify GPU detection (if available)
cargo test -p context-graph-embeddings test_candle_device_selection

# 4. Benchmark latency
cargo bench -p context-graph-embeddings embedding_latency
```

## Pseudo Code

```
CandleEmbeddingProvider:

  async new(config):
    # Detect device
    device = match config.device:
      Auto => if cuda_available(): Device::Cuda(0) else Device::Cpu
      Cpu => Device::Cpu
      Cuda(idx) => Device::Cuda(idx)
      Metal => Device::Metal

    # Load model weights (cached)
    cache_path = config.cache_dir.unwrap_or(default_cache())
    weights_path = download_or_cache(config.model_id, cache_path)

    # Initialize model
    vb = VarBuilder::from_safe_tensors(weights_path, DType::F32, &device)
    model = BertModel::load(vb, &bert_config)?
    tokenizer = Tokenizer::from_pretrained(config.model_id)?

    # Warm up with dummy inference
    _ = model.forward(&dummy_tokens)?

    return Self { model, tokenizer, device, config }

  async embed(content):
    start = Instant::now()

    # Tokenize
    tokens = self.tokenizer.encode(content, truncation=max_seq_len)
    input_ids = Tensor::from_vec(tokens.ids, &self.device)
    attention_mask = Tensor::from_vec(tokens.attention_mask, &self.device)

    # Run model
    output = self.model.forward(&input_ids, &attention_mask)?

    # Mean pooling over sequence
    pooled = mean_pooling(output.last_hidden_state, attention_mask)

    # Normalize to unit length
    normalized = l2_normalize(pooled)

    vector = normalized.to_vec1::<f32>()?

    return EmbeddingResult {
      vector,
      model_id: self.config.model_id.clone(),
      dimensions: vector.len(),
      latency_ms: start.elapsed().as_secs_f64() * 1000.0,
    }

  async embed_batch(contents):
    # Tokenize all at once
    encodings = self.tokenizer.encode_batch(contents)

    # Pad to same length
    padded = pad_sequences(encodings, max_len=self.config.max_sequence_length)

    # Stack into batch tensor
    input_ids = Tensor::stack(padded.input_ids, dim=0)
    attention_mask = Tensor::stack(padded.attention_mask, dim=0)

    # Single forward pass
    output = self.model.forward(&input_ids, &attention_mask)?

    # Mean pool and normalize each
    results = []
    for i in 0..contents.len():
      pooled = mean_pooling(output[i], attention_mask[i])
      normalized = l2_normalize(pooled)
      results.push(EmbeddingResult { vector: normalized.to_vec1()?, ... })

    return results
```

## Files to Create

| File | Description |
|------|-------------|
| `crates/context-graph-embeddings/src/providers/mod.rs` | Provider module |
| `crates/context-graph-embeddings/src/providers/candle_provider.rs` | Candle implementation |
| `crates/context-graph-embeddings/src/providers/factory.rs` | Provider factory |

## Files to Modify

| File | Change |
|------|--------|
| `crates/context-graph-embeddings/src/lib.rs` | Add `pub mod providers;` |
| `crates/context-graph-embeddings/Cargo.toml` | Ensure candle deps are correct |

## Validation Criteria

- [ ] Model loads successfully (GPU or CPU)
- [ ] Single embedding latency <10ms
- [ ] Batch of 64 latency <50ms
- [ ] Embeddings are normalized (magnitude ~1.0)
- [ ] Different texts produce different embeddings
- [ ] Handles empty string gracefully
- [ ] Handles text longer than max_sequence_length

## Test Commands

```bash
# Run with candle feature
cargo test -p context-graph-embeddings --features candle providers

# Benchmark
cargo bench -p context-graph-embeddings --features candle embedding

# Integration test with real model
cargo test -p context-graph-embeddings test_candle_real_embedding
```

---

*Task created: 2026-01-04*
*Module: 06 - Stub Elimination*
*Layer: Logic*
*Priority: CRITICAL*
