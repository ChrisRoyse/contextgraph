# TASK-M02-017: Implement Node CRUD Operations

```xml
<task_spec id="TASK-M02-017" version="2.0">
<metadata>
  <title>Implement Node CRUD Operations</title>
  <status>complete</status>
  <layer>logic</layer>
  <module>module-02</module>
  <sequence>17</sequence>
  <priority>critical</priority>
  <estimated_hours>4</estimated_hours>
  <implements>
    <item>REQ-CORE-006: CRUD requirements</item>
    <item>TECH-CORE-002 Section 3.2: RocksDB backend specification</item>
  </implements>
  <depends_on>
    <task_ref status="complete">TASK-M02-014</task_ref>
    <task_ref status="complete">TASK-M02-016</task_ref>
  </depends_on>
  <estimated_complexity>high</estimated_complexity>
</metadata>

<context>
This task implements the core CRUD (Create, Read, Update, Delete) operations for MemoryNode storage in the RocksDB backend. These operations are the foundation of all memory storage functionality and must maintain consistency across multiple column families (nodes, embeddings, Johari indexes, temporal indexes, tag indexes, source indexes) using atomic WriteBatch operations.

CRITICAL: NO BACKWARDS COMPATIBILITY. Fail fast with robust error logging. All edge cases must return explicit errors, never silently fallback.
</context>

<input_context_files>
  <file purpose="RocksDB backend (RocksDbMemex struct, get_cf, db accessor)">crates/context-graph-storage/src/rocksdb_backend.rs</file>
  <file purpose="Serialization functions (serialize_node, serialize_embedding, serialize_uuid)">crates/context-graph-storage/src/serialization.rs</file>
  <file purpose="Column family definitions (cf_names::NODES, cf_names::EMBEDDINGS, etc.)">crates/context-graph-storage/src/column_families.rs</file>
  <file purpose="MemoryNode, NodeMetadata, ValidationError, DEFAULT_EMBEDDING_DIM, MAX_CONTENT_SIZE">crates/context-graph-core/src/types/memory_node.rs</file>
  <file purpose="JohariQuadrant::column_family() method">crates/context-graph-core/src/types/johari.rs</file>
  <file purpose="Module exports and re-exports">crates/context-graph-storage/src/lib.rs</file>
  <file purpose="Constitution for performance targets">docs2/constitution.yaml</file>
</input_context_files>

<prerequisites>
  <check>TASK-M02-014 (Bincode Serialization) completed âœ“</check>
  <check>TASK-M02-016 (RocksDB Backend Open/Close) completed âœ“</check>
  <check>RocksDbMemex::open() and RocksDbMemex::get_cf() available âœ“</check>
  <check>12 column families defined and accessible âœ“</check>
  <check>serialize_node(), deserialize_node(), serialize_embedding(), deserialize_embedding() available âœ“</check>
  <check>MemoryNode::validate() method available âœ“</check>
  <check>JohariQuadrant::column_family() returns CF name string âœ“</check>
</prerequisites>

<current_codebase_state>
  <!-- Verified 2024-01-XX - These are the ACTUAL types/functions available -->
  <available_types>
    <type name="RocksDbMemex" location="rocksdb_backend.rs">
      - db: DB (RocksDB instance)
      - cache: Cache
      - path: String
      - open(path) -> Result&lt;Self, StorageError&gt;
      - get_cf(name: &amp;str) -> Result&lt;&amp;ColumnFamily, StorageError&gt;
      - db() -> &amp;DB (raw DB access)
      - health_check() -> Result&lt;(), StorageError&gt;
      - flush_all() -> Result&lt;(), StorageError&gt;
    </type>
    <type name="StorageError" location="rocksdb_backend.rs">
      - OpenFailed { path, message }
      - ColumnFamilyNotFound { name }
      - WriteFailed(String)
      - ReadFailed(String)
      - FlushFailed(String)
      <!-- NOTE: Add NotFound variant for get_node operations -->
    </type>
    <type name="MemoryNode" location="memory_node.rs">
      - id: NodeId (Uuid)
      - content: String
      - embedding: EmbeddingVector (Vec&lt;f32&gt;)
      - quadrant: JohariQuadrant
      - importance: f32 [0.0, 1.0]
      - emotional_valence: f32 [-1.0, 1.0]
      - created_at: DateTime&lt;Utc&gt;
      - accessed_at: DateTime&lt;Utc&gt;
      - access_count: u64
      - metadata: NodeMetadata
      - validate() -> Result&lt;(), ValidationError&gt;
    </type>
    <type name="NodeMetadata" location="memory_node.rs">
      - source: Option&lt;String&gt;
      - language: Option&lt;String&gt;
      - modality: Modality
      - tags: Vec&lt;String&gt;
      - utl_score: Option&lt;f32&gt;
      - consolidated: bool
      - consolidated_at: Option&lt;DateTime&lt;Utc&gt;&gt;
      - version: u32
      - deleted: bool
      - deleted_at: Option&lt;DateTime&lt;Utc&gt;&gt;
      - parent_id: Option&lt;Uuid&gt;
      - child_ids: Vec&lt;Uuid&gt;
      - custom: HashMap&lt;String, serde_json::Value&gt;
      - rationale: Option&lt;String&gt;
      - mark_deleted() - sets deleted=true, deleted_at=now
      - restore() - clears deleted flag
      - increment_version() - saturating add
    </type>
    <type name="JohariQuadrant" location="johari.rs">
      - Open, Hidden, Blind, Unknown
      - column_family() -> &amp;'static str (returns "johari_open", etc.)
      - all() -> [JohariQuadrant; 4]
    </type>
  </available_types>
  <column_families count="12" location="column_families.rs">
    - cf_names::NODES = "nodes"
    - cf_names::EDGES = "edges"
    - cf_names::EMBEDDINGS = "embeddings"
    - cf_names::METADATA = "metadata"
    - cf_names::JOHARI_OPEN = "johari_open"
    - cf_names::JOHARI_HIDDEN = "johari_hidden"
    - cf_names::JOHARI_BLIND = "johari_blind"
    - cf_names::JOHARI_UNKNOWN = "johari_unknown"
    - cf_names::TEMPORAL = "temporal"
    - cf_names::TAGS = "tags"
    - cf_names::SOURCES = "sources"
    - cf_names::SYSTEM = "system"
  </column_families>
  <serialization_functions location="serialization.rs">
    - serialize_node(node: &amp;MemoryNode) -> Result&lt;Vec&lt;u8&gt;, SerializationError&gt; (uses MessagePack)
    - deserialize_node(bytes: &amp;[u8]) -> Result&lt;MemoryNode, SerializationError&gt;
    - serialize_embedding(embedding: &amp;EmbeddingVector) -> Vec&lt;u8&gt; (raw f32 LE bytes)
    - deserialize_embedding(bytes: &amp;[u8]) -> Result&lt;EmbeddingVector, SerializationError&gt;
    - serialize_uuid(id: &amp;Uuid) -> [u8; 16]
    - deserialize_uuid(bytes: &amp;[u8; 16]) -> Uuid
  </serialization_functions>
</current_codebase_state>

<scope>
  <in_scope>
    - store_node() method - atomic write to nodes, embeddings, Johari, temporal, tags, sources CFs
    - get_node() method - retrieve and deserialize MemoryNode by ID
    - update_node() method - handle index updates when quadrant/tags change
    - delete_node() method - remove from all indexes (soft delete support per SEC-06)
    - WriteBatch usage for atomic multi-CF operations
    - Index maintenance for all secondary indexes
    - StorageError::NotFound variant for missing nodes
    - Validation before storage (MemoryNode::validate())
    - Performance target: &lt;1ms store, &lt;500Î¼s get
  </in_scope>
  <out_of_scope>
    - Edge CRUD operations (TASK-M02-018)
    - Secondary index query methods (TASK-M02-023)
    - Embedding-specific methods (TASK-M02-024)
    - Memex trait abstraction (TASK-M02-026)
  </out_of_scope>
</scope>

<fail_fast_requirements>
  <!-- NO BACKWARDS COMPATIBILITY - explicit errors for all edge cases -->
  <requirement id="FF-001">MemoryNode::validate() MUST be called before store_node() - fail immediately if invalid</requirement>
  <requirement id="FF-002">All RocksDB operations MUST propagate errors - no silent swallowing</requirement>
  <requirement id="FF-003">Serialization errors MUST be converted to StorageError with full context</requirement>
  <requirement id="FF-004">Missing column family MUST return StorageError::ColumnFamilyNotFound</requirement>
  <requirement id="FF-005">get_node for non-existent ID MUST return StorageError::NotFound</requirement>
  <requirement id="FF-006">update_node for non-existent ID MUST return StorageError::NotFound (not create)</requirement>
  <requirement id="FF-007">delete_node for non-existent ID MUST return StorageError::NotFound</requirement>
  <requirement id="FF-008">All errors MUST include descriptive context for debugging</requirement>
</fail_fast_requirements>

<definition_of_done>
  <signatures>
    <signature file="crates/context-graph-storage/src/rocksdb_backend.rs">
// Add to StorageError enum:
#[derive(Debug, Error)]
pub enum StorageError {
    // ... existing variants ...

    /// Node not found by ID.
    #[error("Node not found: {id}")]
    NotFound { id: String },

    /// Serialization error during storage operation.
    #[error("Serialization error: {0}")]
    Serialization(String),

    /// Node validation failed.
    #[error("Validation error: {0}")]
    ValidationFailed(String),
}

impl RocksDbMemex {
    /// Stores a MemoryNode atomically across all relevant column families.
    ///
    /// Writes to: nodes, embeddings, johari_{quadrant}, temporal, tags (per tag), sources
    /// Uses WriteBatch for atomicity.
    ///
    /// # Validation
    /// Calls node.validate() before storage - returns error if validation fails.
    ///
    /// # Performance
    /// Target: &lt;1ms p95 latency
    ///
    /// # Errors
    /// - StorageError::ValidationFailed if node.validate() fails
    /// - StorageError::Serialization if serialization fails
    /// - StorageError::WriteFailed if RocksDB write fails
    /// - StorageError::ColumnFamilyNotFound if CF missing (should never happen)
    pub fn store_node(&amp;self, node: &amp;MemoryNode) -> Result&lt;(), StorageError&gt;;

    /// Retrieves a MemoryNode by its ID.
    ///
    /// # Performance
    /// Target: &lt;500Î¼s p95 latency
    ///
    /// # Errors
    /// - StorageError::NotFound if node doesn't exist
    /// - StorageError::Serialization if deserialization fails
    /// - StorageError::ReadFailed if RocksDB read fails
    pub fn get_node(&amp;self, id: &amp;NodeId) -> Result&lt;MemoryNode, StorageError&gt;;

    /// Updates an existing MemoryNode, maintaining index consistency.
    ///
    /// Handles index updates when quadrant or tags change.
    /// DOES NOT create if node doesn't exist - returns NotFound error.
    ///
    /// # Errors
    /// - StorageError::NotFound if node doesn't exist
    /// - StorageError::ValidationFailed if node.validate() fails
    /// - StorageError::Serialization if serialization fails
    /// - StorageError::WriteFailed if RocksDB write fails
    pub fn update_node(&amp;self, node: &amp;MemoryNode) -> Result&lt;(), StorageError&gt;;

    /// Deletes a MemoryNode and removes it from all indexes.
    ///
    /// # Arguments
    /// * `id` - Node ID to delete
    /// * `soft_delete` - If true, marks as deleted (SEC-06); if false, permanently removes
    ///
    /// # SEC-06 Compliance
    /// Soft delete preserves data for 30-day recovery per constitution.yaml.
    ///
    /// # Errors
    /// - StorageError::NotFound if node doesn't exist
    /// - StorageError::WriteFailed if RocksDB write fails
    pub fn delete_node(&amp;self, id: &amp;NodeId, soft_delete: bool) -> Result&lt;(), StorageError&gt;;
}
    </signature>
  </signatures>

  <constraints>
    - All writes MUST use WriteBatch for atomicity
    - Index updates MUST be consistent (old entries removed, new added)
    - Soft delete MUST preserve data for 30-day recovery (SEC-06)
    - Performance: store &lt;1ms, get &lt;500Î¼s at p95
    - MemoryNode::validate() MUST be called before store/update
    - UUID serialization: use serialize_uuid() returning [u8; 16]
    - NO silent fallbacks - all errors must propagate
    - Follow constitution.yaml coding standards
  </constraints>

  <verification>
    - cargo build --package context-graph-storage compiles without errors
    - cargo test --package context-graph-storage crud passes all tests
    - Store/get round-trip preserves all MemoryNode fields
    - Update correctly maintains index consistency
    - Delete removes from all column families (hard delete)
    - Soft delete marks but preserves data
    - get_node returns NotFound for non-existent ID
    - update_node returns NotFound for non-existent ID
    - delete_node returns NotFound for non-existent ID
    - Invalid node (fails validate()) returns ValidationFailed
  </verification>
</definition_of_done>

<pseudo_code>
// rocksdb_backend.rs additions:

// Add new StorageError variants
impl From&lt;SerializationError&gt; for StorageError {
    fn from(e: SerializationError) -> Self {
        StorageError::Serialization(e.to_string())
    }
}

impl From&lt;ValidationError&gt; for StorageError {
    fn from(e: ValidationError) -> Self {
        StorageError::ValidationFailed(e.to_string())
    }
}

impl RocksDbMemex {
    pub fn store_node(&amp;self, node: &amp;MemoryNode) -> Result&lt;(), StorageError&gt; {
        // 1. Validate node FIRST - fail fast
        node.validate().map_err(|e| StorageError::ValidationFailed(e.to_string()))?;

        // 2. Create WriteBatch for atomic operation
        let mut batch = rocksdb::WriteBatch::default();

        // 3. Get all required column families
        let cf_nodes = self.get_cf(cf_names::NODES)?;
        let cf_embeddings = self.get_cf(cf_names::EMBEDDINGS)?;
        let cf_temporal = self.get_cf(cf_names::TEMPORAL)?;
        let cf_tags = self.get_cf(cf_names::TAGS)?;
        let cf_sources = self.get_cf(cf_names::SOURCES)?;
        let cf_johari = self.get_cf(node.quadrant.column_family())?;

        // 4. Serialize and write to nodes CF
        let node_key = serialize_uuid(&amp;node.id);
        let node_value = serialize_node(node)?;
        batch.put_cf(cf_nodes, &amp;node_key, &amp;node_value);

        // 5. Write embedding to embeddings CF
        let embedding_value = serialize_embedding(&amp;node.embedding);
        batch.put_cf(cf_embeddings, &amp;node_key, &amp;embedding_value);

        // 6. Add to Johari quadrant index (empty value, key is index)
        batch.put_cf(cf_johari, &amp;node_key, &amp;[]);

        // 7. Add to temporal index: timestamp_millis:uuid format
        let temporal_key = format_temporal_key(node.created_at, &amp;node.id);
        batch.put_cf(cf_temporal, &amp;temporal_key, &amp;[]);

        // 8. Add to tag indexes (one entry per tag)
        for tag in &amp;node.metadata.tags {
            let tag_key = format_tag_key(tag, &amp;node.id);
            batch.put_cf(cf_tags, &amp;tag_key, &amp;[]);
        }

        // 9. Add to sources index if source present
        if let Some(source) = &amp;node.metadata.source {
            let source_key = format_source_key(source, &amp;node.id);
            batch.put_cf(cf_sources, &amp;source_key, &amp;[]);
        }

        // 10. Execute atomic batch write
        self.db.write(batch)
            .map_err(|e| StorageError::WriteFailed(e.to_string()))?;

        Ok(())
    }

    pub fn get_node(&amp;self, id: &amp;NodeId) -> Result&lt;MemoryNode, StorageError&gt; {
        let cf_nodes = self.get_cf(cf_names::NODES)?;
        let node_key = serialize_uuid(id);

        let node_bytes = self.db.get_cf(cf_nodes, &amp;node_key)
            .map_err(|e| StorageError::ReadFailed(e.to_string()))?
            .ok_or_else(|| StorageError::NotFound { id: id.to_string() })?;

        deserialize_node(&amp;node_bytes)
            .map_err(|e| StorageError::Serialization(e.to_string()))
    }

    pub fn update_node(&amp;self, node: &amp;MemoryNode) -> Result&lt;(), StorageError&gt; {
        // 1. Validate node FIRST
        node.validate().map_err(|e| StorageError::ValidationFailed(e.to_string()))?;

        // 2. Get existing node (MUST exist, fail if not)
        let old_node = self.get_node(&amp;node.id)?;

        // 3. Create WriteBatch
        let mut batch = rocksdb::WriteBatch::default();

        // 4. Get CFs
        let cf_nodes = self.get_cf(cf_names::NODES)?;
        let cf_embeddings = self.get_cf(cf_names::EMBEDDINGS)?;
        let cf_tags = self.get_cf(cf_names::TAGS)?;

        let node_key = serialize_uuid(&amp;node.id);

        // 5. Update node data
        batch.put_cf(cf_nodes, &amp;node_key, &amp;serialize_node(node)?);
        batch.put_cf(cf_embeddings, &amp;node_key, &amp;serialize_embedding(&amp;node.embedding));

        // 6. Handle Johari quadrant change
        if old_node.quadrant != node.quadrant {
            let old_cf = self.get_cf(old_node.quadrant.column_family())?;
            let new_cf = self.get_cf(node.quadrant.column_family())?;
            batch.delete_cf(old_cf, &amp;node_key);
            batch.put_cf(new_cf, &amp;node_key, &amp;[]);
        }

        // 7. Handle tag changes
        let old_tags: std::collections::HashSet&lt;_&gt; = old_node.metadata.tags.into_iter().collect();
        let new_tags: std::collections::HashSet&lt;_&gt; = node.metadata.tags.iter().cloned().collect();

        for removed_tag in old_tags.difference(&amp;new_tags) {
            batch.delete_cf(cf_tags, &amp;format_tag_key(removed_tag, &amp;node.id));
        }
        for added_tag in new_tags.difference(&amp;old_tags) {
            batch.put_cf(cf_tags, &amp;format_tag_key(added_tag, &amp;node.id), &amp;[]);
        }

        // 8. Write atomically
        self.db.write(batch)
            .map_err(|e| StorageError::WriteFailed(e.to_string()))?;

        Ok(())
    }

    pub fn delete_node(&amp;self, id: &amp;NodeId, soft_delete: bool) -> Result&lt;(), StorageError&gt; {
        // 1. Get existing node (MUST exist)
        let node = self.get_node(id)?;

        let mut batch = rocksdb::WriteBatch::default();
        let node_key = serialize_uuid(id);

        if soft_delete {
            // SEC-06: Mark as deleted, preserve data
            let mut updated_node = node.clone();
            updated_node.metadata.mark_deleted();

            let cf_nodes = self.get_cf(cf_names::NODES)?;
            batch.put_cf(cf_nodes, &amp;node_key, &amp;serialize_node(&amp;updated_node)?);
        } else {
            // Hard delete: Remove from ALL column families
            let cf_nodes = self.get_cf(cf_names::NODES)?;
            let cf_embeddings = self.get_cf(cf_names::EMBEDDINGS)?;
            let cf_temporal = self.get_cf(cf_names::TEMPORAL)?;
            let cf_tags = self.get_cf(cf_names::TAGS)?;
            let cf_sources = self.get_cf(cf_names::SOURCES)?;
            let cf_johari = self.get_cf(node.quadrant.column_family())?;

            batch.delete_cf(cf_nodes, &amp;node_key);
            batch.delete_cf(cf_embeddings, &amp;node_key);
            batch.delete_cf(cf_johari, &amp;node_key);
            batch.delete_cf(cf_temporal, &amp;format_temporal_key(node.created_at, id));

            for tag in &amp;node.metadata.tags {
                batch.delete_cf(cf_tags, &amp;format_tag_key(tag, id));
            }

            if let Some(source) = &amp;node.metadata.source {
                batch.delete_cf(cf_sources, &amp;format_source_key(source, id));
            }
        }

        self.db.write(batch)
            .map_err(|e| StorageError::WriteFailed(e.to_string()))?;

        Ok(())
    }
}

// Helper functions (private)
fn format_temporal_key(timestamp: DateTime&lt;Utc&gt;, id: &amp;NodeId) -> Vec&lt;u8&gt; {
    // Format: 8-byte timestamp (millis, big-endian) + 16-byte UUID
    let millis = timestamp.timestamp_millis() as u64;
    let mut key = Vec::with_capacity(24);
    key.extend_from_slice(&amp;millis.to_be_bytes());
    key.extend_from_slice(&amp;serialize_uuid(id));
    key
}

fn format_tag_key(tag: &amp;str, id: &amp;NodeId) -> Vec&lt;u8&gt; {
    // Format: tag_bytes + ':' + 16-byte UUID
    let mut key = Vec::with_capacity(tag.len() + 1 + 16);
    key.extend_from_slice(tag.as_bytes());
    key.push(b':');
    key.extend_from_slice(&amp;serialize_uuid(id));
    key
}

fn format_source_key(source: &amp;str, id: &amp;NodeId) -> Vec&lt;u8&gt; {
    // Format: source_bytes + ':' + 16-byte UUID
    let mut key = Vec::with_capacity(source.len() + 1 + 16);
    key.extend_from_slice(source.as_bytes());
    key.push(b':');
    key.extend_from_slice(&amp;serialize_uuid(id));
    key
}
</pseudo_code>

<full_state_verification>
  <!-- CRITICAL: After completing the logic, MUST verify state in RocksDB -->

  <source_of_truth>
    RocksDB column families are the source of truth:
    - nodes CF: Primary MemoryNode data (msgpack serialized)
    - embeddings CF: Embedding vectors (raw f32 bytes)
    - johari_{open|hidden|blind|unknown} CFs: Quadrant membership indexes
    - temporal CF: Time-ordered index
    - tags CF: Tag-based index
    - sources CF: Source-based index
  </source_of_truth>

  <execute_and_inspect protocol="MANDATORY">
    After each CRUD operation, perform a SEPARATE read to verify state:

    1. store_node():
       - Call get_node() to verify node is retrievable
       - Use db().get_cf(cf_embeddings, key) to verify embedding stored
       - Use db().get_cf(johari_cf, key) to verify quadrant index entry
       - Use db().get_cf(cf_temporal, temporal_key) to verify temporal index
       - For each tag: verify tag index entry exists
       - If source: verify source index entry exists

    2. update_node():
       - Verify old quadrant index entry REMOVED if changed
       - Verify new quadrant index entry ADDED if changed
       - Verify old tags REMOVED from tags CF
       - Verify new tags ADDED to tags CF

    3. delete_node(soft=true):
       - Verify node still exists in nodes CF
       - Verify metadata.deleted == true
       - Verify metadata.deleted_at is set

    4. delete_node(soft=false):
       - Verify node DOES NOT EXIST in nodes CF
       - Verify embedding DOES NOT EXIST in embeddings CF
       - Verify johari index entry DOES NOT EXIST
       - Verify temporal index entry DOES NOT EXIST
       - Verify tag index entries DO NOT EXIST
       - Verify source index entry DOES NOT EXIST
  </execute_and_inspect>

  <edge_cases count="3" requirement="MANDATORY">
    Each test MUST print BEFORE and AFTER state:

    <edge_case id="EC-001" name="Empty Tags Update">
      BEFORE: Node with tags ["a", "b", "c"]
      OPERATION: update_node with tags = []
      AFTER: All tag index entries removed, node has empty tags
      VERIFY: db().get_cf(cf_tags, "a:uuid") returns None
    </edge_case>

    <edge_case id="EC-002" name="Quadrant Transition">
      BEFORE: Node in johari_open CF
      OPERATION: update_node with quadrant = Hidden
      AFTER: Entry removed from johari_open, added to johari_hidden
      VERIFY:
        - db().get_cf(johari_open, key) returns None
        - db().get_cf(johari_hidden, key) returns Some
    </edge_case>

    <edge_case id="EC-003" name="Unicode Content">
      BEFORE: No node exists
      OPERATION: store_node with content = "æ—¥æœ¬èªž ðŸŽ‰ Ã©mojis"
      AFTER: Node stored and retrievable with exact content
      VERIFY: get_node().content == "æ—¥æœ¬èªž ðŸŽ‰ Ã©mojis"
    </edge_case>
  </edge_cases>

  <evidence_of_success>
    Tests MUST show actual data in the database:

    ```rust
    #[test]
    fn evidence_store_node_creates_all_indexes() {
        println!("=== EVIDENCE: store_node creates all indexes ===");

        // Setup
        let (tmp, db) = create_temp_db();
        let node = create_valid_test_node();
        node.metadata.tags = vec!["important".to_string(), "verified".to_string()];
        node.metadata.source = Some("test-source".to_string());

        println!("BEFORE: Storing node {}", node.id);

        // Execute
        db.store_node(&amp;node).expect("store failed");

        // Verify - MUST show actual state
        println!("AFTER: Verifying all indexes...");

        // 1. Verify node exists
        let retrieved = db.get_node(&amp;node.id).expect("get failed");
        println!("  nodes CF: Node exists âœ“");
        assert_eq!(retrieved.id, node.id);

        // 2. Verify embedding
        let cf_emb = db.get_cf(cf_names::EMBEDDINGS).unwrap();
        let emb_bytes = db.db().get_cf(cf_emb, serialize_uuid(&amp;node.id)).unwrap();
        assert!(emb_bytes.is_some(), "Embedding should exist");
        println!("  embeddings CF: Embedding exists ({} bytes) âœ“", emb_bytes.unwrap().len());

        // 3. Verify johari index
        let cf_johari = db.get_cf(node.quadrant.column_family()).unwrap();
        let johari_entry = db.db().get_cf(cf_johari, serialize_uuid(&amp;node.id)).unwrap();
        assert!(johari_entry.is_some(), "Johari index should exist");
        println!("  {} CF: Index entry exists âœ“", node.quadrant.column_family());

        // 4. Verify temporal index
        // ... print actual state ...

        // 5. Verify tag indexes
        let cf_tags = db.get_cf(cf_names::TAGS).unwrap();
        for tag in &amp;node.metadata.tags {
            let tag_key = format_tag_key(tag, &amp;node.id);
            let tag_entry = db.db().get_cf(cf_tags, &amp;tag_key).unwrap();
            assert!(tag_entry.is_some(), "Tag index for '{}' should exist", tag);
            println!("  tags CF: '{}' index exists âœ“", tag);
        }

        // 6. Verify source index
        let cf_sources = db.get_cf(cf_names::SOURCES).unwrap();
        let source_key = format_source_key(&amp;node.metadata.source.as_ref().unwrap(), &amp;node.id);
        let source_entry = db.db().get_cf(cf_sources, &amp;source_key).unwrap();
        assert!(source_entry.is_some(), "Source index should exist");
        println!("  sources CF: Source index exists âœ“");

        println!("RESULT: All indexes verified in RocksDB âœ“");
    }
    ```
  </evidence_of_success>
</full_state_verification>

<real_data_testing>
  <!-- NO MOCK DATA - All tests use real MemoryNode instances -->
  <requirement>Tests MUST create valid MemoryNode instances that pass validate()</requirement>
  <requirement>Embeddings MUST be normalized (magnitude ~= 1.0)</requirement>
  <requirement>importance MUST be in [0.0, 1.0]</requirement>
  <requirement>emotional_valence MUST be in [-1.0, 1.0]</requirement>
  <requirement>content MUST be &lt;= MAX_CONTENT_SIZE (1MB)</requirement>

  <helper_function>
    ```rust
    /// Create a valid, normalized embedding
    fn create_normalized_embedding(dim: usize) -> EmbeddingVector {
        let val = 1.0 / (dim as f32).sqrt();
        vec![val; dim]
    }

    /// Create a valid MemoryNode that passes validate()
    fn create_valid_test_node() -> MemoryNode {
        let embedding = create_normalized_embedding(DEFAULT_EMBEDDING_DIM);
        let mut node = MemoryNode::new("Test content".to_string(), embedding);
        node.importance = 0.75;
        node.emotional_valence = 0.5;
        assert!(node.validate().is_ok(), "Test node must be valid");
        node
    }
    ```
  </helper_function>
</real_data_testing>

<files_to_modify>
  <file path="crates/context-graph-storage/src/rocksdb_backend.rs">
    - Add NotFound, Serialization, ValidationFailed variants to StorageError
    - Add From impls for SerializationError and ValidationError
    - Add store_node, get_node, update_node, delete_node methods
    - Add private helper functions: format_temporal_key, format_tag_key, format_source_key
    - Add comprehensive tests with evidence of success
  </file>
</files_to_modify>

<validation_criteria>
  <criterion>store_node() writes to nodes, embeddings, johari, temporal, tags, sources CFs atomically</criterion>
  <criterion>store_node() validates node before storage, returns ValidationFailed if invalid</criterion>
  <criterion>get_node() retrieves and deserializes MemoryNode correctly</criterion>
  <criterion>get_node() returns NotFound for non-existent ID</criterion>
  <criterion>update_node() handles index updates when quadrant/tags change</criterion>
  <criterion>update_node() returns NotFound for non-existent ID (does NOT create)</criterion>
  <criterion>delete_node() removes from all indexes (hard delete)</criterion>
  <criterion>delete_node() marks as deleted but preserves data (soft delete per SEC-06)</criterion>
  <criterion>delete_node() returns NotFound for non-existent ID</criterion>
  <criterion>Latency target: &lt;1ms for store, &lt;500Î¼s for get at p95</criterion>
  <criterion>WriteBatch ensures atomicity across column families</criterion>
  <criterion>All tests use real data, not mocks</criterion>
  <criterion>All tests print before/after state</criterion>
</validation_criteria>

<test_commands>
  <command>cargo build --package context-graph-storage</command>
  <command>cargo test --package context-graph-storage crud -- --nocapture</command>
  <command>cargo test --package context-graph-storage node -- --nocapture</command>
  <command>cargo test --package context-graph-storage evidence -- --nocapture</command>
  <command>cargo clippy --package context-graph-storage -- -D warnings</command>
</test_commands>

<sherlock_verification requirement="MANDATORY">
  After implementation is complete, the implementing agent MUST use the sherlock-holmes
  subagent to verify the entire task. The sherlock-holmes agent will:

  1. Verify all methods are implemented with correct signatures
  2. Verify StorageError has NotFound, Serialization, ValidationFailed variants
  3. Verify From impls exist for error types
  4. Verify all tests pass and show before/after state
  5. Verify tests use real data (no mocks)
  6. Verify Full State Verification is performed in tests
  7. Verify edge cases EC-001, EC-002, EC-003 are tested
  8. Verify atomic WriteBatch usage
  9. Verify SEC-06 soft delete compliance
  10. Verify performance targets in test output

  COMMAND: Use Task tool with subagent_type="sherlock-holmes" for final verification
</sherlock_verification>
</task_spec>
```

## Implementation Notes

### WriteBatch Atomicity
RocksDB WriteBatch ensures all-or-nothing semantics. If any write fails, the entire batch is rolled back.

### Index Key Formats
- **Temporal**: `{timestamp_millis_be:8}:{uuid_bytes:16}` - enables range scans by time
- **Tag**: `{tag_string}:{uuid_bytes:16}` - enables prefix scans by tag
- **Source**: `{source_string}:{uuid_bytes:16}` - enables prefix scans by source

### Soft Delete (Constitution SEC-06)
Per the constitution, soft delete with 30-day recovery is the default. Permanent deletion requires explicit `soft_delete=false`.

### Fail Fast Policy
- No silent fallbacks
- All errors propagate with context
- Validation happens BEFORE storage
- Non-existent entities return NotFound

---

*Task ID: TASK-M02-017*
*Module: 02 - Core Infrastructure*
*Layer: Logic*
*Version: 2.0 - Updated with Full State Verification, Fail Fast, Real Data Testing*
