# TASK-M02-027: Create Module Integration Tests

```xml
<task_spec id="TASK-M02-027" version="2.0">
<metadata>
  <title>Create Module Integration Tests</title>
  <status>complete</status>
  <layer>surface</layer>
  <module>module-02</module>
  <sequence>27</sequence>
  <priority>critical</priority>
  <implements>
    <item>TECH-CORE-002 Section 4: Testing specification</item>
    <item>REQ-CORE-006: CRUD requirements verification</item>
  </implements>
  <depends_on>
    <task_ref status="completed">TASK-M02-017</task_ref>
    <task_ref status="completed">TASK-M02-018</task_ref>
    <task_ref status="completed">TASK-M02-021</task_ref>
    <task_ref status="completed">TASK-M02-022</task_ref>
    <task_ref status="completed">TASK-M02-023</task_ref>
    <task_ref status="completed">TASK-M02-024</task_ref>
    <task_ref status="completed">TASK-M02-025</task_ref>
    <task_ref status="completed">TASK-M02-026</task_ref>
  </depends_on>
</metadata>
</task_spec>
```

---

## CRITICAL: Read This First

**The old task document was WRONG.** Key issues:
1. Used `#[tokio::test]` and `.await` - but the API is **SYNCHRONOUS**, not async
2. Referenced `crates/context-graph-storage/src/rocksdb_backend.rs` - this file does not exist; it's a **directory** with multiple files
3. Referenced non-existent methods like `compute_suggested_action()` - it's actually `suggested_action` field computed via `CognitivePulse::from_values()`
4. Wrong import paths and type signatures

---

## Current Codebase State (Verified 2025-12-31)

### Actual Directory Structure
```
crates/context-graph-storage/
├── Cargo.toml
├── src/
│   ├── lib.rs                      # Re-exports: RocksDbMemex, StorageError, Memex, StorageHealth
│   ├── memex.rs                    # Memex trait + StorageHealth struct
│   ├── serialization.rs            # Bincode serialization functions
│   ├── column_families.rs          # CF names and descriptors
│   ├── indexes.rs                  # Index helpers
│   └── rocksdb_backend/            # <-- DIRECTORY, not single file!
│       ├── mod.rs                  # Re-exports all submodules
│       ├── core.rs                 # RocksDbMemex struct, open(), health_check(), flush_all()
│       ├── config.rs               # RocksDB configuration
│       ├── error.rs                # StorageError enum (9 variants)
│       ├── helpers.rs              # Key formatting helpers
│       ├── node_ops.rs             # store_node, get_node, update_node, delete_node
│       ├── edge_ops.rs             # store_edge, get_edge, update_edge, delete_edge, get_edges_from, get_edges_to
│       ├── index_ops.rs            # get_nodes_by_quadrant, get_nodes_by_tag, get_nodes_by_source, get_nodes_in_time_range
│       ├── embedding_ops.rs        # store_embedding, get_embedding, batch_get_embeddings, delete_embedding, embedding_exists
│       ├── memex_impl.rs           # Memex trait implementation for RocksDbMemex
│       └── tests_*.rs              # 8 test modules (already 215 unit tests passing)
├── tests/                          # <-- DOES NOT EXIST YET - must create
```

### Current Test Status
```bash
cargo test --package context-graph-storage --lib
# Result: 215 passed; 0 failed
```

**No integration tests directory exists.** You must create `crates/context-graph-storage/tests/`.

---

## Verified API Signatures (ACTUAL - Copy These Exactly)

### Storage Operations (ALL SYNCHRONOUS - NO ASYNC!)

```rust
// === RocksDbMemex (core.rs) ===
impl RocksDbMemex {
    pub fn open(path: impl AsRef<Path>) -> Result<Self, StorageError>
    pub fn health_check(&self) -> Result<(), StorageError>
    pub fn flush_all(&self) -> Result<(), StorageError>
}

// === Node Operations (node_ops.rs) ===
impl RocksDbMemex {
    pub fn store_node(&self, node: &MemoryNode) -> Result<(), StorageError>
    pub fn get_node(&self, id: &NodeId) -> Result<MemoryNode, StorageError>
    pub fn update_node(&self, node: &MemoryNode) -> Result<(), StorageError>
    pub fn delete_node(&self, id: &NodeId, soft_delete: bool) -> Result<(), StorageError>
}

// === Edge Operations (edge_ops.rs) ===
impl RocksDbMemex {
    pub fn store_edge(&self, edge: &GraphEdge) -> Result<(), StorageError>
    pub fn get_edge(&self, source_id: &NodeId, target_id: &NodeId, edge_type: EdgeType) -> Result<GraphEdge, StorageError>
    pub fn update_edge(&self, edge: &GraphEdge) -> Result<(), StorageError>
    pub fn delete_edge(&self, source_id: &NodeId, target_id: &NodeId, edge_type: EdgeType) -> Result<(), StorageError>
    pub fn get_edges_from(&self, source_id: &NodeId) -> Result<Vec<GraphEdge>, StorageError>
    pub fn get_edges_to(&self, target_id: &NodeId) -> Result<Vec<GraphEdge>, StorageError>
}

// === Index Operations (index_ops.rs) ===
impl RocksDbMemex {
    pub fn get_nodes_by_quadrant(&self, quadrant: JohariQuadrant, limit: Option<usize>, offset: usize) -> Result<Vec<NodeId>, StorageError>
    pub fn get_nodes_by_tag(&self, tag: &str, limit: Option<usize>, offset: usize) -> Result<Vec<NodeId>, StorageError>
    pub fn get_nodes_by_source(&self, source: &str, limit: Option<usize>, offset: usize) -> Result<Vec<NodeId>, StorageError>
    pub fn get_nodes_in_time_range(&self, start: DateTime<Utc>, end: DateTime<Utc>, limit: Option<usize>, offset: usize) -> Result<Vec<NodeId>, StorageError>
}

// === Embedding Operations (embedding_ops.rs) ===
impl RocksDbMemex {
    pub fn store_embedding(&self, node_id: &NodeId, embedding: &EmbeddingVector) -> Result<(), StorageError>
    pub fn get_embedding(&self, node_id: &NodeId) -> Result<EmbeddingVector, StorageError>
    pub fn batch_get_embeddings(&self, node_ids: &[NodeId]) -> Result<Vec<Option<EmbeddingVector>>, StorageError>
    pub fn delete_embedding(&self, node_id: &NodeId) -> Result<(), StorageError>
    pub fn embedding_exists(&self, node_id: &NodeId) -> Result<bool, StorageError>
}

// === Memex Trait (memex.rs) - SYNCHRONOUS ===
pub trait Memex: Send + Sync {
    fn store_node(&self, node: &MemoryNode) -> Result<(), StorageError>;
    fn get_node(&self, id: &NodeId) -> Result<MemoryNode, StorageError>;
    fn update_node(&self, node: &MemoryNode) -> Result<(), StorageError>;
    fn delete_node(&self, id: &NodeId, soft_delete: bool) -> Result<(), StorageError>;
    fn store_edge(&self, edge: &GraphEdge) -> Result<(), StorageError>;
    fn get_edge(&self, source_id: &NodeId, target_id: &NodeId, edge_type: EdgeType) -> Result<GraphEdge, StorageError>;
    fn get_edges_from(&self, source_id: &NodeId) -> Result<Vec<GraphEdge>, StorageError>;
    fn get_edges_to(&self, target_id: &NodeId) -> Result<Vec<GraphEdge>, StorageError>;
    fn query_by_quadrant(&self, quadrant: JohariQuadrant, limit: Option<usize>) -> Result<Vec<NodeId>, StorageError>;
    fn query_by_tag(&self, tag: &str, limit: Option<usize>) -> Result<Vec<NodeId>, StorageError>;
    fn get_embedding(&self, id: &NodeId) -> Result<EmbeddingVector, StorageError>;
    fn health_check(&self) -> Result<StorageHealth, StorageError>;
}

// === StorageHealth (memex.rs) ===
pub struct StorageHealth {
    pub is_healthy: bool,
    pub node_count: u64,
    pub edge_count: u64,
    pub storage_bytes: u64,
}

// === StorageError (error.rs) - 9 VARIANTS ===
pub enum StorageError {
    OpenFailed(String),
    ColumnFamilyNotFound(String),
    Serialization(SerializationError),
    NotFound { id: String },
    WriteFailed(String),
    ReadFailed(String),
    ValidationFailed(ValidationError),
    IndexCorrupted { cf: String, details: String },
    Internal(String),
}
```

### Core Types (from context-graph-core)

```rust
// Types re-exported from context_graph_core::types
pub type NodeId = Uuid;                    // uuid::Uuid
pub type EmbeddingVector = Vec<f32>;       // 1536 dimensions
pub const DEFAULT_EMBEDDING_DIM: usize = 1536;

// MemoryNode fields
pub struct MemoryNode {
    pub id: NodeId,
    pub content: String,
    pub embedding: EmbeddingVector,
    pub quadrant: JohariQuadrant,
    pub importance: f32,                   // [0.0, 1.0]
    pub access_count: u32,
    pub created_at: DateTime<Utc>,
    pub last_accessed: DateTime<Utc>,
    pub metadata: NodeMetadata,
}

// GraphEdge fields (13 Marblestone fields)
pub struct GraphEdge {
    pub source_id: NodeId,
    pub target_id: NodeId,
    pub edge_type: EdgeType,
    pub weight: f32,
    pub confidence: f32,
    pub created_at: DateTime<Utc>,
    pub last_traversed: Option<DateTime<Utc>>,
    pub traversal_count: u32,
    pub neurotransmitter_weights: NeurotransmitterWeights,
    pub steering_reward: f32,              // [-1.0, 1.0]
    pub is_amortized_shortcut: bool,
    pub shortcut_hops_saved: u8,
    pub domain: Domain,
}

// Marblestone types
pub enum Domain { Code, Legal, Medical, Creative, Research, General }
pub enum EdgeType { Semantic, Temporal, Causal, Hierarchical }
pub struct NeurotransmitterWeights {
    pub excitatory: f32,    // [0.0, 1.0]
    pub inhibitory: f32,    // [0.0, 1.0]
    pub modulatory: f32,    // [0.0, 1.0]
}

// CognitivePulse (7 fields)
pub struct CognitivePulse {
    pub entropy: f32,              // [0.0, 1.0]
    pub coherence: f32,            // [0.0, 1.0]
    pub coherence_delta: f32,      // [-1.0, 1.0]
    pub emotional_weight: f32,     // [0.0, 2.0]
    pub suggested_action: SuggestedAction,  // COMPUTED, not a method!
    pub source_layer: Option<LayerId>,
    pub timestamp: DateTime<Utc>,
}

// Use CognitivePulse::from_values(entropy, coherence) to auto-compute suggested_action
```

---

## Task: Create Integration Tests

### File to Create: `crates/context-graph-storage/tests/integration_tests.rs`

```rust
//! Integration tests for Module 02 - Core Infrastructure.
//!
//! These tests verify end-to-end functionality of the storage layer.
//! All tests use REAL RocksDB instances via tempfile::TempDir.
//!
//! NO MOCK DATA. NO ASYNC. All assertions verify actual database state.

use chrono::{Duration, Utc};
use std::sync::Arc;
use std::thread;
use std::time::Instant;
use tempfile::TempDir;

use context_graph_core::marblestone::{Domain, EdgeType, NeurotransmitterWeights};
use context_graph_core::types::{
    CognitivePulse, EmbeddingVector, GraphEdge, JohariQuadrant, MemoryNode, NodeId,
    SuggestedAction,
};
use context_graph_storage::{Memex, RocksDbMemex, StorageError};

// ============================================================================
// TEST HELPERS
// ============================================================================

/// Create a valid normalized embedding (magnitude ~1.0)
fn create_valid_embedding() -> EmbeddingVector {
    const DIM: usize = 1536;
    let val = 1.0_f32 / (DIM as f32).sqrt();
    vec![val; DIM]
}

fn create_test_node() -> MemoryNode {
    MemoryNode::new("Test content".to_string(), create_valid_embedding())
}

fn create_node_with_content(content: &str) -> MemoryNode {
    MemoryNode::new(content.to_string(), create_valid_embedding())
}

fn create_test_edge(source: NodeId, target: NodeId) -> GraphEdge {
    GraphEdge::new(source, target, EdgeType::Semantic, Domain::General)
}

fn setup_db() -> (RocksDbMemex, TempDir) {
    let tmp = TempDir::new().expect("create temp dir");
    let db = RocksDbMemex::open(tmp.path()).expect("open db");
    (db, tmp)
}

// ============================================================================
// NODE LIFECYCLE TESTS
// ============================================================================

#[test]
fn test_node_lifecycle_create_read_update_delete() {
    let (db, _tmp) = setup_db();
    let node = create_test_node();
    let node_id = node.id;

    println!("=== NODE LIFECYCLE TEST ===");
    println!("TRIGGER: Creating node with ID {}", node_id);

    // CREATE
    db.store_node(&node).expect("store failed");

    // READ - Verify in Source of Truth
    let retrieved = db.get_node(&node_id).expect("get failed");
    println!("VERIFY: Node exists with content '{}'", retrieved.content);
    assert_eq!(retrieved.id, node_id);
    assert_eq!(retrieved.content, node.content);

    // UPDATE
    let mut updated = retrieved.clone();
    updated.importance = 0.9;
    updated.metadata.tags.push("updated-tag".to_string());
    db.update_node(&updated).expect("update failed");

    // VERIFY UPDATE
    let after_update = db.get_node(&node_id).expect("get after update");
    println!("VERIFY: importance={}", after_update.importance);
    assert!((after_update.importance - 0.9).abs() < 0.001);

    // SOFT DELETE
    db.delete_node(&node_id, true).expect("soft delete");
    let soft_deleted = db.get_node(&node_id).expect("get soft deleted");
    println!("VERIFY: deleted flag={}", soft_deleted.metadata.deleted);
    assert!(soft_deleted.metadata.deleted);

    // HARD DELETE
    db.delete_node(&node_id, false).expect("hard delete");
    let result = db.get_node(&node_id);
    println!("VERIFY: NotFound={}", result.is_err());
    assert!(matches!(result, Err(StorageError::NotFound { .. })));

    println!("RESULT: PASSED");
}

#[test]
fn test_node_embedding_roundtrip() {
    let (db, _tmp) = setup_db();
    let node = create_test_node();
    let original = node.embedding.clone();

    db.store_node(&node).expect("store");
    let retrieved = db.get_embedding(&node.id).expect("get embedding");

    println!("=== EMBEDDING ROUNDTRIP TEST ===");
    println!("VERIFY: dim original={}, retrieved={}", original.len(), retrieved.len());
    assert_eq!(original.len(), retrieved.len());

    for (i, (o, r)) in original.iter().zip(retrieved.iter()).enumerate() {
        assert!((o - r).abs() < 1e-7, "Mismatch at {}: {} vs {}", i, o, r);
    }
    println!("RESULT: PASSED");
}

// ============================================================================
// MARBLESTONE EDGE TESTS
// ============================================================================

#[test]
fn test_edge_with_neurotransmitter_weights() {
    let (db, _tmp) = setup_db();
    let node1 = create_test_node();
    let node2 = create_test_node();
    db.store_node(&node1).expect("store node1");
    db.store_node(&node2).expect("store node2");

    let edge = GraphEdge::new(node1.id, node2.id, EdgeType::Causal, Domain::Code);
    println!("=== NT WEIGHTS TEST ===");
    println!("BEFORE: NT={:?}", edge.neurotransmitter_weights);

    db.store_edge(&edge).expect("store edge");
    let retrieved = db.get_edge(&node1.id, &node2.id, EdgeType::Causal).expect("get edge");

    println!("AFTER: NT={:?}", retrieved.neurotransmitter_weights);
    assert_eq!(retrieved.neurotransmitter_weights.excitatory, edge.neurotransmitter_weights.excitatory);
    assert_eq!(retrieved.domain, Domain::Code);
    println!("RESULT: PASSED");
}

#[test]
fn test_edge_steering_reward_persistence() {
    let (db, _tmp) = setup_db();
    let node1 = create_test_node();
    let node2 = create_test_node();
    db.store_node(&node1).expect("store node1");
    db.store_node(&node2).expect("store node2");

    let mut edge = create_test_edge(node1.id, node2.id);
    println!("=== STEERING REWARD TEST ===");
    println!("BEFORE: steering_reward={}", edge.steering_reward);

    edge.apply_steering_reward(0.5);
    println!("AFTER APPLY: steering_reward={}", edge.steering_reward);

    db.store_edge(&edge).expect("store");
    let retrieved = db.get_edge(&node1.id, &node2.id, EdgeType::Semantic).expect("get");

    println!("VERIFY: retrieved steering_reward={}", retrieved.steering_reward);
    assert!((retrieved.steering_reward - edge.steering_reward).abs() < 0.001);
    println!("RESULT: PASSED");
}

#[test]
fn test_amortized_shortcut_edge() {
    let (db, _tmp) = setup_db();
    let node1 = create_test_node();
    let node2 = create_test_node();
    db.store_node(&node1).expect("store node1");
    db.store_node(&node2).expect("store node2");

    let mut edge = create_test_edge(node1.id, node2.id);
    println!("=== AMORTIZED SHORTCUT TEST ===");
    println!("BEFORE: is_amortized_shortcut={}", edge.is_amortized_shortcut);

    edge.mark_as_shortcut(3);
    println!("AFTER: is_amortized_shortcut={}, hops_saved={}", edge.is_amortized_shortcut, edge.shortcut_hops_saved);

    db.store_edge(&edge).expect("store");
    let retrieved = db.get_edge(&node1.id, &node2.id, EdgeType::Semantic).expect("get");

    println!("VERIFY: is_amortized_shortcut={}", retrieved.is_amortized_shortcut);
    assert!(retrieved.is_amortized_shortcut);
    assert_eq!(retrieved.shortcut_hops_saved, 3);
    println!("RESULT: PASSED");
}

// ============================================================================
// JOHARI QUADRANT INDEX TESTS
// ============================================================================

#[test]
fn test_johari_quadrant_index_consistency() {
    let (db, _tmp) = setup_db();

    let mut open_node = create_test_node();
    open_node.quadrant = JohariQuadrant::Open;
    let mut hidden_node = create_test_node();
    hidden_node.quadrant = JohariQuadrant::Hidden;

    println!("=== JOHARI INDEX TEST ===");
    db.store_node(&open_node).expect("store open");
    db.store_node(&hidden_node).expect("store hidden");

    let open_ids = db.get_nodes_by_quadrant(JohariQuadrant::Open, None, 0).expect("query open");
    let hidden_ids = db.get_nodes_by_quadrant(JohariQuadrant::Hidden, None, 0).expect("query hidden");

    println!("VERIFY: Open has {} nodes, Hidden has {} nodes", open_ids.len(), hidden_ids.len());
    assert!(open_ids.contains(&open_node.id));
    assert!(hidden_ids.contains(&hidden_node.id));

    // Transition test
    let mut transitioned = open_node.clone();
    transitioned.quadrant = JohariQuadrant::Hidden;
    db.update_node(&transitioned).expect("update");

    let open_after = db.get_nodes_by_quadrant(JohariQuadrant::Open, None, 0).expect("query");
    let hidden_after = db.get_nodes_by_quadrant(JohariQuadrant::Hidden, None, 0).expect("query");

    println!("AFTER TRANSITION: Open={}, Hidden={}", open_after.len(), hidden_after.len());
    assert!(!open_after.contains(&open_node.id));
    assert!(hidden_after.contains(&open_node.id));
    println!("RESULT: PASSED");
}

// ============================================================================
// COGNITIVE PULSE TESTS
// ============================================================================

#[test]
fn test_cognitive_pulse_action_matrix() {
    println!("=== COGNITIVE PULSE ACTION MATRIX ===");

    // Test decision paths from constitution.yaml
    let stabilize = CognitivePulse::from_values(0.8, 0.3);
    println!("entropy=0.8, coherence=0.3 => {:?}", stabilize.suggested_action);
    assert_eq!(stabilize.suggested_action, SuggestedAction::Stabilize);

    let ready = CognitivePulse::from_values(0.3, 0.8);
    println!("entropy=0.3, coherence=0.8 => {:?}", ready.suggested_action);
    assert_eq!(ready.suggested_action, SuggestedAction::Ready);

    let explore = CognitivePulse::from_values(0.7, 0.6);
    println!("entropy=0.7, coherence=0.6 => {:?}", explore.suggested_action);
    assert_eq!(explore.suggested_action, SuggestedAction::Explore);

    let continue_action = CognitivePulse::from_values(0.5, 0.5);
    println!("entropy=0.5, coherence=0.5 => {:?}", continue_action.suggested_action);
    assert_eq!(continue_action.suggested_action, SuggestedAction::Continue);

    println!("RESULT: PASSED");
}

// ============================================================================
// TAG INDEX TESTS
// ============================================================================

#[test]
fn test_tag_index_consistency() {
    let (db, _tmp) = setup_db();
    let mut node = create_test_node();
    node.metadata.tags = vec!["rust".to_string(), "async".to_string()];

    println!("=== TAG INDEX TEST ===");
    println!("BEFORE: tags={:?}", node.metadata.tags);
    db.store_node(&node).expect("store");

    let rust_nodes = db.get_nodes_by_tag("rust", None, 0).expect("query rust");
    assert!(rust_nodes.contains(&node.id));

    node.metadata.tags = vec!["async".to_string(), "tokio".to_string()];
    println!("TRIGGER: Update tags to {:?}", node.metadata.tags);
    db.update_node(&node).expect("update");

    let rust_after = db.get_nodes_by_tag("rust", None, 0).expect("query rust");
    let tokio_nodes = db.get_nodes_by_tag("tokio", None, 0).expect("query tokio");

    println!("VERIFY: rust={}, tokio={}", rust_after.len(), tokio_nodes.len());
    assert!(!rust_after.contains(&node.id));
    assert!(tokio_nodes.contains(&node.id));
    println!("RESULT: PASSED");
}

// ============================================================================
// TEMPORAL INDEX TESTS
// ============================================================================

#[test]
fn test_temporal_index() {
    let (db, _tmp) = setup_db();
    let node1 = create_test_node();
    let node2 = create_test_node();
    let node3 = create_test_node();

    db.store_node(&node1).expect("store 1");
    std::thread::sleep(std::time::Duration::from_millis(10));
    db.store_node(&node2).expect("store 2");
    std::thread::sleep(std::time::Duration::from_millis(10));
    db.store_node(&node3).expect("store 3");

    println!("=== TEMPORAL INDEX TEST ===");
    let start = node1.created_at - Duration::seconds(1);
    let end = Utc::now() + Duration::seconds(1);

    let nodes = db.get_nodes_in_time_range(start, end, None, 0).expect("query");
    println!("VERIFY: Found {} nodes in range", nodes.len());
    assert!(nodes.len() >= 3);
    println!("RESULT: PASSED");
}

// ============================================================================
// CONCURRENT ACCESS TESTS
// ============================================================================

#[test]
fn test_concurrent_reads() {
    let (db, _tmp) = setup_db();
    let db = Arc::new(db);
    let node = create_test_node();
    let node_id = node.id;
    db.store_node(&node).expect("store");

    println!("=== CONCURRENT READS TEST ===");
    let handles: Vec<_> = (0..100).map(|_| {
        let db = Arc::clone(&db);
        thread::spawn(move || db.get_node(&node_id).is_ok())
    }).collect();

    let success_count: usize = handles.into_iter()
        .map(|h| if h.join().unwrap() { 1 } else { 0 })
        .sum();

    println!("VERIFY: {}/100 succeeded", success_count);
    assert_eq!(success_count, 100);
    println!("RESULT: PASSED");
}

#[test]
fn test_concurrent_writes() {
    let (db, _tmp) = setup_db();
    let db = Arc::new(db);

    println!("=== CONCURRENT WRITES TEST ===");
    let handles: Vec<_> = (0..50).map(|i| {
        let db = Arc::clone(&db);
        thread::spawn(move || {
            let node = create_node_with_content(&format!("Node {}", i));
            let id = node.id;
            (id, db.store_node(&node).is_ok())
        })
    }).collect();

    let mut stored = Vec::new();
    for h in handles {
        let (id, ok) = h.join().unwrap();
        if ok { stored.push(id); }
    }

    println!("VERIFY: {} nodes stored", stored.len());
    assert_eq!(stored.len(), 50);

    for id in &stored {
        assert!(db.get_node(id).is_ok());
    }
    println!("RESULT: PASSED");
}

// ============================================================================
// PERFORMANCE TESTS
// ============================================================================

#[test]
fn test_store_latency() {
    let (db, _tmp) = setup_db();
    let mut latencies = Vec::with_capacity(1000);

    println!("=== STORE LATENCY TEST ===");
    for i in 0..1000 {
        let node = create_node_with_content(&format!("Perf {}", i));
        let start = Instant::now();
        db.store_node(&node).expect("store");
        latencies.push(start.elapsed());
    }

    latencies.sort();
    let p50 = latencies[500];
    let p95 = latencies[950];
    let p99 = latencies[990];

    println!("VERIFY: p50={:?}, p95={:?}, p99={:?}", p50, p95, p99);
    assert!(p99 < std::time::Duration::from_millis(10));
    println!("RESULT: PASSED");
}

#[test]
fn test_get_latency() {
    let (db, _tmp) = setup_db();
    let mut ids = Vec::with_capacity(1000);

    for i in 0..1000 {
        let node = create_node_with_content(&format!("Perf {}", i));
        ids.push(node.id);
        db.store_node(&node).expect("store");
    }
    db.flush_all().expect("flush");

    println!("=== GET LATENCY TEST ===");
    let mut latencies = Vec::with_capacity(1000);
    for id in &ids {
        let start = Instant::now();
        db.get_node(id).expect("get");
        latencies.push(start.elapsed());
    }

    latencies.sort();
    let p50 = latencies[500];
    let p95 = latencies[950];
    let p99 = latencies[990];

    println!("VERIFY: p50={:?}, p95={:?}, p99={:?}", p50, p95, p99);
    assert!(p99 < std::time::Duration::from_millis(5));
    println!("RESULT: PASSED");
}

// ============================================================================
// ERROR HANDLING TESTS
// ============================================================================

#[test]
fn test_not_found_error() {
    let (db, _tmp) = setup_db();
    let fake_id = uuid::Uuid::new_v4();

    println!("=== NOT FOUND ERROR TEST ===");
    println!("TRIGGER: Get non-existent {}", fake_id);

    let result = db.get_node(&fake_id);
    println!("VERIFY: NotFound={}", matches!(&result, Err(StorageError::NotFound { .. })));
    assert!(matches!(result, Err(StorageError::NotFound { .. })));
    println!("RESULT: PASSED");
}

#[test]
fn test_validation_error() {
    let (db, _tmp) = setup_db();
    let mut node = create_test_node();
    node.embedding = vec![0.1; 100]; // Wrong dimension

    println!("=== VALIDATION ERROR TEST ===");
    println!("TRIGGER: Store with embedding dim={}", node.embedding.len());

    let result = db.store_node(&node);
    println!("VERIFY: ValidationFailed={}", matches!(&result, Err(StorageError::ValidationFailed(_))));
    assert!(matches!(result, Err(StorageError::ValidationFailed(_))));
    println!("RESULT: PASSED");
}

// ============================================================================
// MEMEX TRAIT TEST
// ============================================================================

#[test]
fn test_memex_trait() {
    let (db, _tmp) = setup_db();
    let memex: &dyn Memex = &db;

    println!("=== MEMEX TRAIT TEST ===");
    let node = create_test_node();
    memex.store_node(&node).expect("store via trait");
    let retrieved = memex.get_node(&node.id).expect("get via trait");

    println!("VERIFY: roundtrip via trait");
    assert_eq!(retrieved.id, node.id);

    let health = memex.health_check().expect("health check");
    println!("VERIFY: is_healthy={}", health.is_healthy);
    assert!(health.is_healthy);
    println!("RESULT: PASSED");
}

// ============================================================================
// EDGE CASES
// ============================================================================

#[test]
fn edge_case_empty_result() {
    let (db, _tmp) = setup_db();
    println!("=== EDGE CASE: EMPTY RESULT ===");
    let result = db.get_nodes_by_tag("nonexistent-xyz", None, 0).expect("query");
    println!("BEFORE: Empty DB, TRIGGER: Query nonexistent tag, AFTER: len={}", result.len());
    assert!(result.is_empty());
    println!("RESULT: PASSED");
}

#[test]
fn edge_case_limit() {
    let (db, _tmp) = setup_db();
    println!("=== EDGE CASE: LIMIT ===");

    for i in 0..100 {
        let mut node = create_node_with_content(&format!("N{}", i));
        node.metadata.tags.push("test-limit".to_string());
        db.store_node(&node).expect("store");
    }

    let limited = db.get_nodes_by_tag("test-limit", Some(10), 0).expect("query");
    println!("BEFORE: 100 nodes, TRIGGER: limit=10, AFTER: len={}", limited.len());
    assert_eq!(limited.len(), 10);
    println!("RESULT: PASSED");
}

#[test]
fn edge_case_nan_importance() {
    let (db, _tmp) = setup_db();
    println!("=== EDGE CASE: NaN ===");

    let mut node = create_test_node();
    node.importance = f32::NAN;
    println!("BEFORE: importance=NaN, TRIGGER: store_node");

    let result = db.store_node(&node);
    println!("AFTER: error={}", result.is_err());
    assert!(result.is_err());
    println!("RESULT: PASSED");
}
```

---

## Files to Modify

### `crates/context-graph-storage/Cargo.toml`

Add if not present:
```toml
[dev-dependencies]
tempfile = "3.14"
```

---

## Verification Commands (Run in Order)

```bash
# 1. Build storage crate
cargo build --package context-graph-storage

# 2. Run integration tests
cargo test --package context-graph-storage --test integration_tests -- --nocapture

# 3. Run all tests (unit + integration)
cargo test --package context-graph-storage

# 4. Lint check
cargo clippy --package context-graph-storage --tests -- -D warnings

# 5. Performance tests (release mode)
cargo test --package context-graph-storage --test integration_tests --release -- latency --nocapture
```

---

## Full State Verification Protocol (MANDATORY)

### 1. Source of Truth
The Source of Truth is **RocksDB** on disk. All data is stored in column families.

### 2. Execute & Inspect Protocol
After each operation:
1. Execute the storage operation
2. Read back from RocksDB via a separate get operation
3. Compare expected vs actual

### 3. Edge Case Audit (Must log for each test)
```
=== TEST NAME ===
BEFORE: [initial state]
TRIGGER: [action performed]
AFTER: [observed state]
VERIFY: [assertion result]
RESULT: PASSED/FAILED
```

### 4. Evidence of Success
Tests must print actual values from RocksDB.

---

## Final Verification (MANDATORY)

After implementation, invoke the `sherlock-holmes` subagent to verify:
1. All tests pass: `cargo test --package context-graph-storage`
2. No mock data anywhere
3. Every test reads back from RocksDB to verify
4. Edge cases properly handled
5. Performance within tolerances
6. No clippy warnings

The sherlock-holmes agent confirms completeness before marking task done.

---

*Task ID: TASK-M02-027*
*Module: 02 - Core Infrastructure*
*Layer: Surface*
*Updated: 2025-12-31*
*Status: Ready for Implementation*
