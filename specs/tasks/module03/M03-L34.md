# Task Specification: M03-L34

```xml
<task_spec id="M03-L34" version="1.0">
<metadata>
  <title>Inter-Model Scale Normalization (Pre-Concat)</title>
  <status>ready</status>
  <layer>logic</layer>
  <sequence>34</sequence>
  <implements>
    - PRD Section 7.4: FuseMoE input conditioning
    - Constitution: reliability.numerical_stability
    - Technical Engine: Cross-model embedding calibration
  </implements>
  <depends_on>
    - M03-F03 (ModelEmbedding)
    - M03-F02 (ModelDimensions)
    - M03-L17 (BatchProcessor)
  </depends_on>
  <estimated_hours>3</estimated_hours>
</metadata>

<context>
The 12 embedding models produce vectors with different activation ranges:

| Model | Typical Activation Range | Issue |
|-------|-------------------------|-------|
| E1 (E5-Large) | (-0.05, 0.05) | Small magnitude |
| E9 (HDC) | (-1.0, 1.0) | Large magnitude |
| E11 (MiniLM) | (-0.1, 0.1) | Medium magnitude |
| E12 (ColBERT) | (-0.2, 0.2) | Medium magnitude |

When these are concatenated raw into the 8320D input vector for FuseMoE:
- The gating network will disproportionately weight high-magnitude models
- Low-magnitude models (like E5) will be effectively "ignored"
- Expert networks will see skewed gradients during training

SOLUTION: L2-normalize each model's output BEFORE concatenation, ensuring every
sub-vector within the 8320D input has a unit norm of 1.0. This ensures:
1. All models contribute equally to the gating decision
2. Expert networks see balanced inputs
3. No numerical instability from extreme magnitude differences

This is a critical "glue" task that prevents numerical divergence in FuseMoE.
</context>

<definition_of_done>
  <signatures>
```rust
use std::sync::Arc;

/// Normalization strategy for pre-concatenation
#[derive(Debug, Clone, Copy, PartialEq)]
pub enum NormalizationStrategy {
    /// L2 normalize to unit norm (recommended)
    L2Unit,
    /// L2 normalize with model-specific target magnitude
    L2Scaled { target_magnitude: f32 },
    /// Z-score normalization (mean=0, std=1)
    ZScore,
    /// Min-max normalization to [0, 1]
    MinMax,
    /// No normalization (passthrough)
    None,
}

/// Configuration for pre-concat normalization
#[derive(Debug, Clone)]
pub struct PreConcatNormConfig {
    /// Default strategy for all models
    pub default_strategy: NormalizationStrategy,
    /// Per-model overrides (if needed)
    pub model_overrides: std::collections::HashMap<ModelId, NormalizationStrategy>,
    /// Minimum norm threshold (below this, zero vector is returned)
    pub min_norm_threshold: f32,
    /// Enable magnitude tracking for diagnostics
    pub track_magnitudes: bool,
}

impl Default for PreConcatNormConfig {
    fn default() -> Self {
        Self {
            default_strategy: NormalizationStrategy::L2Unit,
            model_overrides: std::collections::HashMap::new(),
            min_norm_threshold: 1e-8,
            track_magnitudes: false,
        }
    }
}

/// Pre-concatenation normalizer
pub struct PreConcatNormalizer {
    config: PreConcatNormConfig,
    magnitude_stats: Option<MagnitudeStats>,
}

/// Statistics for tracking embedding magnitudes
#[derive(Debug, Clone, Default)]
pub struct MagnitudeStats {
    pub samples_per_model: std::collections::HashMap<ModelId, usize>,
    pub mean_magnitude_per_model: std::collections::HashMap<ModelId, f32>,
    pub min_magnitude_per_model: std::collections::HashMap<ModelId, f32>,
    pub max_magnitude_per_model: std::collections::HashMap<ModelId, f32>,
}

impl PreConcatNormalizer {
    /// Create normalizer with configuration
    pub fn new(config: PreConcatNormConfig) -> Self;

    /// Create with default L2 unit normalization
    pub fn with_l2_unit() -> Self;

    /// Normalize a single model embedding in-place
    pub fn normalize(&self, embedding: &mut ModelEmbedding);

    /// Normalize a single model embedding, returning new vector
    pub fn normalize_copy(&self, embedding: &ModelEmbedding) -> ModelEmbedding;

    /// Normalize batch of embeddings from same model
    pub fn normalize_batch(&self, embeddings: &mut [ModelEmbedding]);

    /// Normalize all model outputs before concatenation
    pub fn normalize_for_concat(
        &self,
        embeddings: &mut std::collections::HashMap<ModelId, ModelEmbedding>,
    );

    /// Compute L2 norm of vector
    pub fn l2_norm(vector: &[f32]) -> f32;

    /// Apply L2 normalization to vector
    pub fn apply_l2_norm(vector: &mut [f32], target_magnitude: f32);

    /// Get strategy for specific model
    pub fn get_strategy(&self, model_id: ModelId) -> NormalizationStrategy;

    /// Get magnitude statistics (if tracking enabled)
    pub fn magnitude_stats(&self) -> Option<&MagnitudeStats>;

    /// Reset magnitude statistics
    pub fn reset_stats(&mut self);
}

/// Extension trait for ModelEmbedding
pub trait NormalizableEmbedding {
    /// L2 norm of the embedding
    fn l2_norm(&self) -> f32;

    /// Normalize to unit norm in-place
    fn normalize_l2(&mut self);

    /// Check if embedding is normalized (norm ≈ 1.0)
    fn is_normalized(&self, tolerance: f32) -> bool;
}

impl NormalizableEmbedding for ModelEmbedding {
    fn l2_norm(&self) -> f32 {
        self.vector.iter().map(|x| x * x).sum::<f32>().sqrt()
    }

    fn normalize_l2(&mut self) {
        let norm = self.l2_norm();
        if norm > 1e-8 {
            for v in &mut self.vector {
                *v /= norm;
            }
        }
    }

    fn is_normalized(&self, tolerance: f32) -> bool {
        (self.l2_norm() - 1.0).abs() < tolerance
    }
}
```
  </signatures>

  <constraints>
    - L2 normalization MUST produce unit vectors (norm = 1.0 ± 1e-6)
    - Zero vectors (norm < threshold) are passed through unchanged
    - Normalization MUST be applied before ConcatenatedEmbedding construction
    - Overhead: <10μs per embedding (SIMD-optimized)
    - All 12 model outputs normalized identically by default
    - Model-specific overrides available for special cases
    - Thread-safe for concurrent normalization
    - No allocations for in-place normalization
  </constraints>

  <verification>
    <step>Each normalized ModelEmbedding has l2_norm() = 1.0 ± 1e-6</step>
    <step>ConcatenatedEmbedding has 12 sub-vectors, each with unit norm</step>
    <step>Zero input vectors remain zero (no NaN from 0/0)</step>
    <step>Normalization preserves vector direction (cosine similarity = 1.0)</step>
    <step>Batch normalization matches individual normalization</step>
    <step>normalize_for_concat() handles missing models gracefully</step>
    <step>Performance: <10μs per 1024D embedding on single core</step>
    <step>Magnitude tracking records correct statistics</step>
  </verification>
</definition_of_done>

<files_to_create>
  <file>crates/context-graph-embeddings/src/fusion/normalization.rs</file>
</files_to_create>

<validation_criteria>
  <criterion>cargo check passes</criterion>
  <criterion>cargo test fusion::normalization passes</criterion>
  <criterion>All 12 model embeddings have unit norm after normalization</criterion>
  <criterion>8320D concatenated input has 12 unit-norm sub-vectors</criterion>
  <criterion>FuseMoE gating weights become balanced after enabling normalization</criterion>
  <criterion>No NaN or Inf values in normalized output</criterion>
</validation_criteria>
</task_spec>
```

## Implementation Notes

### SIMD-Optimized L2 Normalization

```rust
use std::simd::{f32x8, SimdFloat};

impl PreConcatNormalizer {
    #[inline]
    pub fn l2_norm_simd(vector: &[f32]) -> f32 {
        let chunks = vector.chunks_exact(8);
        let remainder = chunks.remainder();

        let mut sum_vec = f32x8::splat(0.0);
        for chunk in chunks {
            let v = f32x8::from_slice(chunk);
            sum_vec += v * v;
        }

        let mut sum: f32 = sum_vec.reduce_sum();
        for &v in remainder {
            sum += v * v;
        }

        sum.sqrt()
    }

    #[inline]
    pub fn apply_l2_norm_simd(vector: &mut [f32], target_magnitude: f32) {
        let norm = Self::l2_norm_simd(vector);
        if norm < 1e-8 {
            return; // Zero vector, avoid division by zero
        }

        let scale = target_magnitude / norm;
        let scale_vec = f32x8::splat(scale);

        let chunks = vector.chunks_exact_mut(8);
        let remainder_len = chunks.remainder().len();

        for chunk in chunks {
            let v = f32x8::from_slice(chunk);
            (v * scale_vec).copy_to_slice(chunk);
        }

        // Handle remainder
        let start = vector.len() - remainder_len;
        for v in &mut vector[start..] {
            *v *= scale;
        }
    }
}
```

### Integration with BatchProcessor

```rust
impl BatchProcessor {
    async fn process_batch_internal(&self, batch: &Batch) -> EmbeddingResult<Vec<FusedEmbedding>> {
        // ... existing model execution code ...

        // NEW: Normalize before concatenation
        let normalizer = &self.pre_concat_normalizer;
        for model_results in &mut all_model_results {
            normalizer.normalize_batch(model_results);
        }

        // Concatenate normalized embeddings
        let concatenated = self.concatenate_embeddings(all_model_results);

        // FuseMoE fusion
        let fused = self.fuse_moe.forward(&concatenated).await?;

        Ok(fused)
    }
}
```

### Verification That Gating Is Balanced

```rust
#[test]
fn test_gating_balance_with_normalization() {
    let inputs = generate_test_embeddings_with_varying_magnitudes();

    // Without normalization
    let raw_gating = compute_gating_weights(&inputs);
    assert!(raw_gating[0] > 0.5, "E5 ignored due to low magnitude");

    // With normalization
    let normalizer = PreConcatNormalizer::with_l2_unit();
    let mut normalized = inputs.clone();
    normalizer.normalize_for_concat(&mut normalized);

    let normalized_gating = compute_gating_weights(&normalized);

    // Gating should be more balanced
    let variance = statistical_variance(&normalized_gating);
    assert!(variance < 0.1, "Gating weights should be balanced after normalization");
}
```

### Magnitude Tracking for Diagnostics

```rust
impl PreConcatNormalizer {
    pub fn update_stats(&mut self, model_id: ModelId, embedding: &ModelEmbedding) {
        if let Some(stats) = &mut self.magnitude_stats {
            let norm = embedding.l2_norm();

            let count = stats.samples_per_model.entry(model_id).or_insert(0);
            let mean = stats.mean_magnitude_per_model.entry(model_id).or_insert(0.0);
            let min = stats.min_magnitude_per_model.entry(model_id).or_insert(f32::MAX);
            let max = stats.max_magnitude_per_model.entry(model_id).or_insert(f32::MIN);

            // Online mean update
            *count += 1;
            *mean += (norm - *mean) / (*count as f32);
            *min = min.min(norm);
            *max = max.max(norm);
        }
    }
}
```

---
*Task ID: M03-L34*
*Layer: Logic*
*Module: 03 - 12-Model Embedding Pipeline*
