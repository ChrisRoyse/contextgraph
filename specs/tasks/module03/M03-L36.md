# Task Specification: M03-L36

```xml
<task_spec id="M03-L36" version="1.0">
<metadata>
  <title>MSE-Minimizing Quantization Calibration</title>
  <status>ready</status>
  <layer>logic</layer>
  <sequence>74</sequence>
  <implements>
    - Glue for M03-L28: Compute optimal scale factors for FP8/FP4 quantization
    - RTX 5090 Technical Report: Block-scaled FP4 calibration
    - Constitution: cuda.quantization.calibration
  </implements>
  <depends_on>
    - M03-L28 (Blackwell Quantization Kernels)
    - M03-L31 (FuseMoE Weight Registry)
    - M03-S13 (Model Artifact Manager)
    - M03-S19 (Semantic Alignment Verification)
  </depends_on>
  <estimated_hours>5</estimated_hours>
</metadata>

<context>
Implement a calibration suite that computes optimal block-scale factors for FP8/FP4
quantization. Without proper calibration, converting FP16 weights to FP4 causes
semantic drift that breaks the Semantic Alignment Verification (M03-S19).

**The Problem:**
M03-L28 implements the FP8/FP4 kernels, but weights are stored as FP16. Converting
to FP4 isn't a simple cast - it requires finding per-block scale factors that
minimize Mean Squared Error (MSE) between the quantized and original values.

**Key Insight:**
FP4 has only 16 representable values. To maintain semantic meaning:
1. Analyze weight distribution per block
2. Find optimal scale factor minimizing MSE
3. Store scale factors alongside quantized weights
4. Verify semantic alignment post-quantization

**Calibration Types:**
1. **Static Calibration**: One-time offline on "Golden Checkpoint"
2. **Dynamic Calibration**: Per-batch scale factor adjustment (runtime)
3. **Entropy-Aware Calibration**: Weight blocks by attention entropy

**Performance Target:**
- Calibration overhead: <1 minute for full 12-model pipeline
- MSE degradation: <0.1% vs FP16 baseline
- Semantic similarity preservation: >99% correlation
</context>

<definition_of_done>
  <signatures>
```rust
/// Calibration result for a single weight tensor
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct CalibrationResult {
    pub tensor_name: String,
    pub format: QuantFormat,
    pub block_size: usize,
    pub scale_factors: Vec<f32>,     // One per block
    pub mse_before: f64,             // MSE if naive quantization
    pub mse_after: f64,              // MSE with optimal scales
    pub improvement_ratio: f64,      // mse_before / mse_after
    pub outlier_count: usize,        // Values clipped to format range
}

/// Calibration profile for an entire model
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ModelCalibrationProfile {
    pub model_id: ModelId,
    pub format: QuantFormat,
    pub timestamp: chrono::DateTime<chrono::Utc>,
    pub tensor_calibrations: HashMap<String, CalibrationResult>,
    pub overall_mse: f64,
    pub semantic_correlation: f64,   // vs FP16 baseline
    pub golden_checkpoint_hash: String,
}

/// Calibration suite configuration
#[derive(Debug, Clone)]
pub struct CalibrationConfig {
    pub format: QuantFormat,
    pub block_sizes: Vec<usize>,     // Try multiple block sizes
    pub num_calibration_samples: usize,
    pub semantic_test_queries: Vec<String>,
    pub mse_threshold: f64,          // Fail if exceeded
    pub correlation_threshold: f64,   // Fail if below
}

pub struct QuantizationCalibrator {
    config: CalibrationConfig,
    weight_registry: Arc<WeightRegistry>,
    artifact_manager: Arc<ModelArtifactManager>,
    quantizer: Arc<BlackwellQuantizer>,
}

impl QuantizationCalibrator {
    /// Create calibrator with configuration
    pub fn new(
        config: CalibrationConfig,
        weight_registry: Arc<WeightRegistry>,
        artifact_manager: Arc<ModelArtifactManager>,
        quantizer: Arc<BlackwellQuantizer>,
    ) -> Self;

    /// Calibrate a single tensor with grid search for optimal block size
    pub fn calibrate_tensor(
        &self,
        tensor: &Tensor,
        name: &str,
    ) -> CudaResult<CalibrationResult>;

    /// Calibrate all weights in a model
    pub async fn calibrate_model(
        &self,
        model_id: ModelId,
    ) -> CudaResult<ModelCalibrationProfile>;

    /// Calibrate all 12 models (parallel execution)
    pub async fn calibrate_all_models(&self) -> CudaResult<Vec<ModelCalibrationProfile>>;

    /// Find optimal block size for a tensor
    fn find_optimal_block_size(&self, tensor: &Tensor) -> usize;

    /// Compute MSE between original and quantized tensor
    fn compute_mse(&self, original: &Tensor, quantized: &QuantizedTensor) -> f64;

    /// Validate semantic preservation post-quantization
    pub async fn validate_semantic_alignment(
        &self,
        model_id: ModelId,
        profile: &ModelCalibrationProfile,
        test_queries: &[String],
    ) -> CudaResult<f64>;

    /// Save calibration profile to disk
    pub fn save_profile(&self, profile: &ModelCalibrationProfile) -> EmbeddingResult<PathBuf>;

    /// Load calibration profile from disk
    pub fn load_profile(&self, model_id: ModelId) -> EmbeddingResult<ModelCalibrationProfile>;

    /// Check if calibration profile exists and is valid for current checkpoint
    pub fn profile_valid(&self, model_id: ModelId) -> bool;
}

/// Extension trait for BlackwellQuantizer
impl BlackwellQuantizer {
    /// Apply calibration profile during quantization
    pub fn quantize_with_calibration(
        &self,
        tensor: &Tensor,
        profile: &CalibrationResult,
    ) -> CudaResult<QuantizedTensor>;
}
```
  </signatures>

  <constraints>
    - Block sizes must be power of 2: [32, 64, 128, 256]
    - FP4 requires larger blocks (>=64) for accuracy
    - Grid search parallelized across CUDA streams
    - Calibration profiles cached on disk with checkpoint hash
    - Must re-calibrate if model checkpoint changes
    - Semantic validation uses same test set as M03-S19
    - 64-byte alignment for Blackwell L2 cache efficiency
    - Thread-safe calibration for concurrent model loading
  </constraints>

  <verification>
    <step>calibrate_tensor produces optimal scale factors</step>
    <step>MSE reduction is measurable vs naive quantization</step>
    <step>FP4 calibration passes semantic alignment test (>99% correlation)</step>
    <step>Calibration profiles are deterministic (same input = same output)</step>
    <step>Profile cache invalidation on checkpoint change</step>
    <step>Parallel calibration of all 12 models completes in <1 minute</step>
  </verification>
</definition_of_done>

<files_to_create>
  <file>crates/context-graph-cuda/src/quantization/calibration.rs</file>
  <file>crates/context-graph-cuda/src/quantization/mse.rs</file>
  <file>crates/context-graph-cuda/src/quantization/profiles.rs</file>
</files_to_create>

<validation_criteria>
  <criterion>cargo check passes</criterion>
  <criterion>cargo clippy passes with no warnings</criterion>
  <criterion>Calibration produces measurable MSE improvement</criterion>
  <criterion>FP4 quantized embeddings maintain >99% correlation with FP16</criterion>
  <criterion>Profile serialization/deserialization works correctly</criterion>
  <criterion>Cache invalidation triggers re-calibration</criterion>
</validation_criteria>
</task_spec>
```

## Implementation Notes

### MSE Optimization Algorithm

```rust
impl QuantizationCalibrator {
    fn calibrate_tensor(&self, tensor: &Tensor, name: &str) -> CudaResult<CalibrationResult> {
        let mut best_result = None;
        let mut best_mse = f64::MAX;

        // Grid search over block sizes
        for &block_size in &self.config.block_sizes {
            // Skip incompatible block sizes for format
            if self.config.format == QuantFormat::FP4 && block_size < 64 {
                continue;
            }

            let result = self.calibrate_with_block_size(tensor, name, block_size)?;

            if result.mse_after < best_mse {
                best_mse = result.mse_after;
                best_result = Some(result);
            }
        }

        best_result.ok_or_else(|| CudaError::CalibrationFailed(name.to_string()))
    }

    fn calibrate_with_block_size(
        &self,
        tensor: &Tensor,
        name: &str,
        block_size: usize,
    ) -> CudaResult<CalibrationResult> {
        let data = tensor.to_vec::<f32>()?;
        let mut scale_factors = Vec::new();
        let mut total_mse_before = 0.0;
        let mut total_mse_after = 0.0;
        let mut outlier_count = 0;

        let max_representable = match self.config.format {
            QuantFormat::FP4 => 6.0,
            QuantFormat::FP8E4M3 => 448.0,
            QuantFormat::FP8E5M2 => 57344.0,
            QuantFormat::FP16 => 65504.0,
        };

        for block in data.chunks(block_size) {
            // Find optimal scale via binary search
            let (scale, mse_before, mse_after, outliers) =
                self.optimize_block_scale(block, max_representable);

            scale_factors.push(scale);
            total_mse_before += mse_before;
            total_mse_after += mse_after;
            outlier_count += outliers;
        }

        let num_blocks = scale_factors.len() as f64;
        Ok(CalibrationResult {
            tensor_name: name.to_string(),
            format: self.config.format,
            block_size,
            scale_factors,
            mse_before: total_mse_before / num_blocks,
            mse_after: total_mse_after / num_blocks,
            improvement_ratio: total_mse_before / total_mse_after.max(1e-10),
            outlier_count,
        })
    }

    fn optimize_block_scale(
        &self,
        block: &[f32],
        max_representable: f32,
    ) -> (f32, f64, f64, usize) {
        // Find max absolute value
        let max_abs = block.iter().map(|x| x.abs()).fold(0.0f32, f32::max);

        if max_abs == 0.0 {
            return (1.0, 0.0, 0.0, 0);
        }

        // Naive scale (full range utilization)
        let naive_scale = max_representable / max_abs;
        let mse_naive = self.compute_block_mse(block, naive_scale, max_representable);

        // Binary search for optimal scale
        let mut low = naive_scale * 0.1;
        let mut high = naive_scale * 2.0;
        let mut best_scale = naive_scale;
        let mut best_mse = mse_naive;

        for _ in 0..20 {
            let mid1 = low + (high - low) / 3.0;
            let mid2 = high - (high - low) / 3.0;

            let mse1 = self.compute_block_mse(block, mid1, max_representable);
            let mse2 = self.compute_block_mse(block, mid2, max_representable);

            if mse1 < mse2 {
                high = mid2;
                if mse1 < best_mse {
                    best_mse = mse1;
                    best_scale = mid1;
                }
            } else {
                low = mid1;
                if mse2 < best_mse {
                    best_mse = mse2;
                    best_scale = mid2;
                }
            }
        }

        // Count outliers
        let outliers = block.iter()
            .filter(|&&v| (v * best_scale).abs() > max_representable)
            .count();

        (best_scale, mse_naive, best_mse, outliers)
    }
}
```

### Semantic Alignment Validation

```rust
impl QuantizationCalibrator {
    async fn validate_semantic_alignment(
        &self,
        model_id: ModelId,
        profile: &ModelCalibrationProfile,
        test_queries: &[String],
    ) -> CudaResult<f64> {
        let fp16_embeddings = self.embed_queries_fp16(model_id, test_queries).await?;
        let quantized_embeddings = self.embed_queries_quantized(
            model_id,
            profile,
            test_queries,
        ).await?;

        // Compute Pearson correlation between embedding vectors
        let mut correlations = Vec::new();
        for (fp16, quant) in fp16_embeddings.iter().zip(&quantized_embeddings) {
            let correlation = pearson_correlation(&fp16.vector, &quant.vector);
            correlations.push(correlation);
        }

        let mean_correlation = correlations.iter().sum::<f64>() / correlations.len() as f64;

        if mean_correlation < self.config.correlation_threshold {
            return Err(CudaError::SemanticDrift {
                model_id,
                correlation: mean_correlation,
                threshold: self.config.correlation_threshold,
            });
        }

        Ok(mean_correlation)
    }
}
```

### Profile Cache Management

```rust
impl QuantizationCalibrator {
    fn profile_path(&self, model_id: ModelId) -> PathBuf {
        self.artifact_manager
            .cache_dir()
            .join("calibration")
            .join(format!("{:?}_{}.profile", model_id, self.config.format))
    }

    pub fn profile_valid(&self, model_id: ModelId) -> bool {
        let path = self.profile_path(model_id);
        if !path.exists() {
            return false;
        }

        // Check if golden checkpoint hash matches
        match self.load_profile(model_id) {
            Ok(profile) => {
                let current_hash = self.weight_registry.checkpoint_hash(model_id);
                profile.golden_checkpoint_hash == current_hash
            }
            Err(_) => false,
        }
    }
}
```

---
*Task ID: M03-L36*
*Layer: Logic*
*Module: 03 - 12-Model Embedding Pipeline*
