# Task Specification: M03-S22

```xml
<task_spec id="M03-S22" version="1.0">
<metadata>
  <title>Tokenizer JSON Artifact Lifecycle</title>
  <status>ready</status>
  <layer>surface</layer>
  <sequence>73</sequence>
  <implements>
    - Extension to M03-S13: Ensure tokenizer configuration files are managed alongside model weights
    - PRD: 12-model tokenizer configuration integrity
    - Constitution: reliability.tokenizer_verification
  </implements>
  <depends_on>
    - M03-S13 (ModelArtifactManager)
    - M03-L29 (TokenizationManager)
    - M03-F01 (ModelId)
  </depends_on>
  <estimated_hours>3</estimated_hours>
</metadata>

<context>
Extend the Model Artifact Manager to handle tokenizer configuration files (tokenizer.json,
tokenizer_config.json, special_tokens_map.json) for all 8 pretrained model families.

**The Problem:**
M03-S13 focuses on .safetensors model weight files, but the TokenizationManager (M03-L29)
requires JSON configuration files to instantiate HuggingFace tokenizers. Without explicit
management, the system may:
- Hit HuggingFace API at runtime to download missing JSONs (403 errors in air-gapped environments)
- Experience massive latency spikes on first tokenization
- Fail silently when tokenizer configs are corrupted

**Key Responsibilities:**
- Download and verify all tokenizer JSON files alongside model weights
- Support 8 unique tokenizer families (not 12 - some models share tokenizers)
- Verify file integrity via checksums
- Handle offline-first deployment scenarios
- Integrate with TokenizationManager initialization

**Tokenizer Family Mapping:**
| Family | Models Sharing | Files Required |
|--------|---------------|----------------|
| E5 | Semantic (E1) | tokenizer.json, tokenizer_config.json, special_tokens_map.json |
| Longformer | Causal (E5) | tokenizer.json, tokenizer_config.json, special_tokens_map.json |
| SPLADE | Sparse (E6) | tokenizer.json, tokenizer_config.json, special_tokens_map.json |
| CodeBERT | Code (E7) | tokenizer.json, tokenizer_config.json, special_tokens_map.json, vocab.txt |
| MiniLM | Graph (E8), Entity (E11) | tokenizer.json, tokenizer_config.json, special_tokens_map.json |
| CLIP | Multimodal (E10) | tokenizer.json, tokenizer_config.json, special_tokens_map.json, merges.txt |
| ColBERT | Late-Interaction (E12) | tokenizer.json, tokenizer_config.json, special_tokens_map.json |
| Custom | Temporal (E2-E4), HDC (E9) | N/A (no pretrained tokenizer) |
</context>

<definition_of_done>
  <signatures>
```rust
/// Tokenizer artifact metadata
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct TokenizerArtifact {
    pub family: TokenizerFamily,
    pub repo_id: String,           // e.g., "intfloat/e5-large-v2"
    pub files: Vec<TokenizerFile>, // All required files
    pub revision: Option<String>,
}

/// Individual tokenizer file
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct TokenizerFile {
    pub filename: String,          // e.g., "tokenizer.json"
    pub sha256: String,            // expected hash
    pub size_bytes: u64,
    pub required: bool,            // true for tokenizer.json, false for optional files
}

/// Tokenizer family enum
#[derive(Debug, Clone, Copy, PartialEq, Eq, Hash, Serialize, Deserialize)]
pub enum TokenizerFamily {
    E5,
    Longformer,
    Splade,
    CodeBert,
    MiniLM,
    Clip,
    ColBert,
    Custom, // No pretrained tokenizer
}

/// Tokenizer readiness status
#[derive(Debug, Clone)]
pub struct TokenizerReadinessReport {
    pub ready_families: Vec<TokenizerFamily>,
    pub missing_families: Vec<TokenizerFamily>,
    pub missing_files: Vec<(TokenizerFamily, String)>,
    pub corrupted_files: Vec<(TokenizerFamily, String, String)>, // family, file, error
}

impl ModelArtifactManager {
    /// Load tokenizer manifests in addition to model manifests
    pub fn load_tokenizer_manifests(&mut self) -> EmbeddingResult<()>;

    /// Check if tokenizer files are available for a family
    pub fn is_tokenizer_available(&self, family: TokenizerFamily) -> bool;

    /// Get local directory path for tokenizer files
    pub fn get_tokenizer_dir(&self, family: TokenizerFamily) -> Option<PathBuf>;

    /// Verify all tokenizer files for a family
    pub async fn verify_tokenizer(&self, family: TokenizerFamily) -> EmbeddingResult<bool>;

    /// Download all tokenizer files for a family
    pub async fn download_tokenizer(&self, family: TokenizerFamily) -> EmbeddingResult<PathBuf>;

    /// Ensure all required tokenizers are present
    pub async fn ensure_all_tokenizers(&self) -> EmbeddingResult<()>;

    /// Get tokenizer readiness report
    pub fn tokenizer_readiness(&self) -> TokenizerReadinessReport;

    /// Map ModelId to its TokenizerFamily
    pub fn model_to_tokenizer_family(model_id: ModelId) -> TokenizerFamily;
}

/// Extension trait for TokenizationManager integration
pub trait TokenizerArtifactProvider {
    /// Get tokenizer path for model, downloading if necessary
    fn get_or_download_tokenizer(&self, model_id: ModelId) -> impl Future<Output = EmbeddingResult<PathBuf>>;
}
```
  </signatures>

  <constraints>
    - Must download all 3-4 files per tokenizer family atomically
    - Files must be verified before TokenizationManager initialization
    - Support for air-gapped/offline deployments (no runtime downloads)
    - Atomic file writes (download to temp, then rename)
    - Handle HuggingFace rate limiting with retry logic
    - Verify MIME types for JSON files
    - UTF-8 validation for all tokenizer files
  </constraints>

  <verification>
    <step>All 8 tokenizer family manifests loaded correctly</step>
    <step>is_tokenizer_available returns true for complete families</step>
    <step>verify_tokenizer detects corrupted JSON files</step>
    <step>download_tokenizer fetches all required files</step>
    <step>ensure_all_tokenizers completes before pipeline init</step>
    <step>TokenizationManager initializes without API calls when files present</step>
    <step>Graceful error when running offline without cached tokenizers</step>
  </verification>
</definition_of_done>

<files_to_create>
  <file>crates/context-graph-embeddings/src/artifacts/tokenizers.rs</file>
  <file>crates/context-graph-embeddings/src/artifacts/tokenizer_manifests.json</file>
</files_to_create>

<validation_criteria>
  <criterion>cargo check passes</criterion>
  <criterion>cargo test artifacts::tokenizers passes</criterion>
  <criterion>Offline initialization works with cached files</criterion>
  <criterion>TokenizationManager loads without network when files present</criterion>
  <criterion>Corrupted file detection works</criterion>
</validation_criteria>
</task_spec>
```

## Implementation Notes

### Tokenizer Manifest (tokenizer_manifests.json)

```json
[
  {
    "family": "E5",
    "repo_id": "intfloat/e5-large-v2",
    "files": [
      {"filename": "tokenizer.json", "sha256": "abc123...", "size_bytes": 711000, "required": true},
      {"filename": "tokenizer_config.json", "sha256": "def456...", "size_bytes": 1200, "required": true},
      {"filename": "special_tokens_map.json", "sha256": "ghi789...", "size_bytes": 800, "required": true}
    ],
    "revision": "main"
  },
  {
    "family": "CodeBert",
    "repo_id": "microsoft/codebert-base",
    "files": [
      {"filename": "tokenizer.json", "sha256": "...", "size_bytes": 520000, "required": true},
      {"filename": "tokenizer_config.json", "sha256": "...", "size_bytes": 1100, "required": true},
      {"filename": "special_tokens_map.json", "sha256": "...", "size_bytes": 600, "required": true},
      {"filename": "vocab.txt", "sha256": "...", "size_bytes": 231000, "required": true}
    ],
    "revision": "main"
  }
]
```

### Integration with Pipeline Initialization

```rust
impl EmbeddingPipeline {
    pub async fn new(config: EmbeddingConfig) -> EmbeddingResult<Self> {
        let artifact_manager = ModelArtifactManager::new(config.artifacts.clone())?;

        // Step 0a: Ensure all model weights are available
        if config.artifacts.auto_download {
            artifact_manager.ensure_all_models().await?;
        }

        // Step 0b: Ensure all tokenizer files are available (NEW)
        if config.artifacts.auto_download {
            artifact_manager.ensure_all_tokenizers().await?;
        } else {
            let tok_report = artifact_manager.tokenizer_readiness();
            if !tok_report.missing_families.is_empty() {
                return Err(EmbeddingError::MissingTokenizers(tok_report.missing_families));
            }
        }

        // Step 1: Initialize TokenizationManager (tokenizers now guaranteed)
        let tokenizer_manager = TokenizationManager::new(&artifact_manager)?;

        // Step 2: Initialize ModelRegistry
        let registry = ModelRegistry::new(config.registry.clone()).await?;

        // ... rest of initialization
    }
}
```

### Model to Tokenizer Family Mapping

```rust
impl ModelArtifactManager {
    pub fn model_to_tokenizer_family(model_id: ModelId) -> TokenizerFamily {
        match model_id {
            ModelId::Semantic => TokenizerFamily::E5,
            ModelId::TemporalRecent |
            ModelId::TemporalPeriodic |
            ModelId::TemporalPositional => TokenizerFamily::Custom,
            ModelId::Causal => TokenizerFamily::Longformer,
            ModelId::Sparse => TokenizerFamily::Splade,
            ModelId::Code => TokenizerFamily::CodeBert,
            ModelId::Graph | ModelId::Entity => TokenizerFamily::MiniLM,
            ModelId::HDC => TokenizerFamily::Custom,
            ModelId::Multimodal => TokenizerFamily::Clip,
            ModelId::LateInteraction => TokenizerFamily::ColBert,
        }
    }
}
```

---
*Task ID: M03-S22*
*Layer: Surface*
*Module: 03 - 12-Model Embedding Pipeline*
