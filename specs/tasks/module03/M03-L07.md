# M03-L07: Causal Model (E5 - allenai/longformer-base-4096)

## Task Metadata
| Field | Value |
|-------|-------|
| **ID** | M03-L07 |
| **Layer** | Logic |
| **Status** | complete |
| **Depends On** | M03-F09 (EmbeddingModel trait - COMPLETE), M03-L01 (ModelRegistry - COMPLETE), M03-L03 (SemanticModel pattern - COMPLETE) |
| **Implements** | PRD E5: Causal embedding, 768D output, Longformer architecture |
| **Constitution Ref** | embeddings.models.E5_Causal |
| **Estimated Hours** | 4 |

---

## Critical Context for AI Agent

### What This Task Is
Implement the **CausalModel** - a pretrained embedding model (E5) that produces 768-dimensional vectors using the **allenai/longformer-base-4096** architecture. This model specializes in processing long documents (up to 4096 tokens) using a combination of sliding window attention and global attention for efficient long-range dependency capture needed in causal reasoning tasks.

### Key Differences from Other Pretrained Models
| Aspect | SemanticModel (E1) | CausalModel (E5) |
|--------|-------------------|------------------|
| Model | e5-large-v2 | longformer-base-4096 |
| Dimension | 1024D | 768D |
| Max Tokens | 512 | 4096 |
| Attention | Standard self-attention | Sliding window + global |
| Use Case | Semantic similarity | Causal reasoning, long docs |
| Latency Budget | <5ms | <8ms |
| Memory | ~1.3GB | ~750MB (base vs large) |

### CODEBASE CURRENT STATE (Audited 2026-01-01)

#### Dependencies VERIFIED COMPLETE:
- **M03-F09** (EmbeddingModel trait): `crates/context-graph-embeddings/src/traits/embedding_model.rs`
  - 29 tests passing
  - Required methods: `model_id()`, `supported_input_types()`, `embed()`, `is_initialized()`
  - Default implementations: `dimension()`, `latency_budget_ms()`, `max_tokens()`, `is_pretrained()`, `validate_input()`

- **M03-L01** (ModelRegistry): `crates/context-graph-embeddings/src/models/registry.rs`
  - 68 tests passing
  - Provides model lifecycle management

- **M03-L03** (SemanticModel): `crates/context-graph-embeddings/src/models/pretrained/semantic.rs`
  - 21 tests passing - **USE THIS AS REFERENCE PATTERN**
  - Stub implementation with xxhash64-seeded deterministic vectors

#### Current Crate Structure:
```
crates/context-graph-embeddings/
├── Cargo.toml              # stub feature default, candle feature optional
├── src/
│   ├── lib.rs              # Root exports
│   ├── traits/
│   │   ├── embedding_model.rs  # EmbeddingModel trait
│   │   └── model_factory.rs    # ModelFactory, SingleModelConfig
│   ├── models/
│   │   ├── mod.rs              # Exports all models
│   │   ├── registry.rs         # ModelRegistry
│   │   ├── memory_tracker.rs   # MemoryTracker
│   │   ├── pretrained/
│   │   │   ├── mod.rs          # UPDATE: Add CausalModel export
│   │   │   └── semantic.rs     # SemanticModel - REFERENCE PATTERN
│   │   └── custom/             # Custom models (temporal, etc.)
│   ├── types/
│   │   ├── model_id.rs         # ModelId::Causal = 4
│   │   ├── embedding.rs        # ModelEmbedding
│   │   └── input.rs            # ModelInput, InputType
│   └── error.rs                # EmbeddingError
```

### Key Facts from model_id.rs
```rust
// Already exists in crates/context-graph-embeddings/src/types/model_id.rs
ModelId::Causal = 4,                           // Enum variant
ModelId::Causal.dimension() => 768             // Native dimension
ModelId::Causal.max_tokens() => 4096           // Extended context
ModelId::Causal.latency_budget_ms() => 8       // Latency target
ModelId::Causal.is_pretrained() => true        // Uses HuggingFace weights
ModelId::Causal.model_repo() => "allenai/longformer-base-4096"
ModelId::Causal.directory_name() => "causal"   // Model files location
ModelId::Causal.tokenizer_family() => TokenizerFamily::RobertaBpe
```

---

## Exact Implementation Requirements

### Files to Create

#### 1. CREATE: `crates/context-graph-embeddings/src/models/pretrained/causal.rs`

```rust
//! Causal embedding model using allenai/longformer-base-4096.
//!
//! This model (E5) produces 768D vectors optimized for causal reasoning tasks.
//! Uses sliding window attention + global attention for 4096 token context.
//!
//! # Thread Safety
//! - `AtomicBool` for `loaded` state (lock-free reads)
//! - Inner model/tokenizer require explicit synchronization if mutable
//!
//! # Memory Layout
//! - Total estimated: ~750MB for FP32 weights (base model)
//! - With FP16 quantization: ~375MB

use std::path::{Path, PathBuf};
use std::sync::atomic::{AtomicBool, Ordering};

use async_trait::async_trait;
use rand::{Rng, SeedableRng};
use rand::rngs::StdRng;

use crate::error::{EmbeddingError, EmbeddingResult};
use crate::traits::{EmbeddingModel, SingleModelConfig};
use crate::types::{InputType, ModelEmbedding, ModelId, ModelInput};

/// Native dimension for Longformer-base model.
pub const CAUSAL_DIMENSION: usize = 768;

/// Maximum tokens for Longformer (extended context).
pub const CAUSAL_MAX_TOKENS: usize = 4096;

/// Latency budget in milliseconds (P95 target).
pub const CAUSAL_LATENCY_BUDGET_MS: u32 = 8;

/// Default attention window size for sliding window attention.
pub const DEFAULT_ATTENTION_WINDOW: usize = 512;

/// Internal state that varies based on feature flags.
#[allow(dead_code)]
enum ModelState {
    /// Unloaded - no weights in memory.
    Unloaded,

    /// Loaded with candle model and tokenizer (candle feature).
    #[cfg(feature = "candle")]
    Loaded {
        // TODO: candle_transformers::models::longformer::LongformerModel,
        // tokenizer: tokenizers::Tokenizer,
        // device: candle::Device,
    },

    /// Stub for testing without real weights.
    #[cfg(not(feature = "candle"))]
    Stub,
}

/// Causal embedding model using allenai/longformer-base-4096.
///
/// This model produces 768D vectors optimized for causal reasoning.
/// Uses sliding window attention + global attention for efficient
/// processing of documents up to 4096 tokens.
///
/// # Attention Mechanism
///
/// Longformer uses a combination of:
/// - **Sliding window attention**: Each token attends to `window_size` neighbors
/// - **Global attention**: Selected tokens (e.g., [CLS]) attend to all tokens
///
/// This allows O(n × w) complexity instead of O(n²) for long sequences.
///
/// # Construction
///
/// ```rust,ignore
/// use context_graph_embeddings::models::CausalModel;
/// use context_graph_embeddings::traits::SingleModelConfig;
/// use std::path::Path;
///
/// let model = CausalModel::new(
///     Path::new("models/causal"),
///     SingleModelConfig::default(),
/// )?;
/// model.load().await?;  // Must load before embed
/// ```
pub struct CausalModel {
    /// Model weights and inference engine.
    #[allow(dead_code)]
    model_state: std::sync::RwLock<ModelState>,

    /// Path to model weights directory.
    #[allow(dead_code)]
    model_path: PathBuf,

    /// Configuration for this model instance.
    #[allow(dead_code)]
    config: SingleModelConfig,

    /// Whether model weights are loaded and ready.
    loaded: AtomicBool,

    /// Memory used by model weights (bytes).
    #[allow(dead_code)]
    memory_size: usize,

    /// Attention window size for sliding window attention.
    attention_window: usize,

    /// Token indices that receive global attention (e.g., [CLS] at 0).
    global_attention_tokens: Vec<usize>,
}

impl CausalModel {
    /// Create a new CausalModel instance.
    ///
    /// Model is NOT loaded after construction. Call `load()` before `embed()`.
    ///
    /// # Arguments
    /// * `model_path` - Path to directory containing model weights
    /// * `config` - Device placement and quantization settings
    ///
    /// # Errors
    /// - `EmbeddingError::ConfigError` if config validation fails
    pub fn new(model_path: &Path, config: SingleModelConfig) -> EmbeddingResult<Self> {
        if config.max_batch_size == 0 {
            return Err(EmbeddingError::ConfigError {
                message: "max_batch_size cannot be zero".to_string(),
            });
        }

        Ok(Self {
            model_state: std::sync::RwLock::new(ModelState::Unloaded),
            model_path: model_path.to_path_buf(),
            config,
            loaded: AtomicBool::new(false),
            memory_size: 0,
            attention_window: DEFAULT_ATTENTION_WINDOW,
            global_attention_tokens: vec![0], // [CLS] token by default
        })
    }

    /// Configure which token indices receive global attention.
    ///
    /// By default, only the [CLS] token (index 0) has global attention.
    /// Additional tokens can be added for task-specific needs.
    ///
    /// # Arguments
    /// * `tokens` - Indices of tokens that should attend globally
    pub fn set_global_attention_tokens(&mut self, tokens: &[usize]) {
        self.global_attention_tokens = tokens.to_vec();
    }

    /// Get the attention window size for sliding attention.
    #[inline]
    #[must_use]
    pub fn attention_window(&self) -> usize {
        self.attention_window
    }

    /// Get the current global attention token indices.
    #[inline]
    #[must_use]
    pub fn global_attention_tokens(&self) -> &[usize] {
        &self.global_attention_tokens
    }

    /// Load model weights into memory.
    #[cfg(not(feature = "candle"))]
    pub async fn load(&self) -> EmbeddingResult<()> {
        let mut state = self.model_state.write().map_err(|e| {
            EmbeddingError::InternalError {
                message: format!("Failed to acquire write lock: {}", e),
            }
        })?;

        *state = ModelState::Stub;
        self.loaded.store(true, Ordering::SeqCst);
        tracing::info!("CausalModel stub loaded (no real weights)");
        Ok(())
    }

    /// Unload model weights from memory.
    pub async fn unload(&self) -> EmbeddingResult<()> {
        if !self.is_initialized() {
            return Err(EmbeddingError::NotInitialized {
                model_id: self.model_id(),
            });
        }

        let mut state = self.model_state.write().map_err(|e| {
            EmbeddingError::InternalError {
                message: format!("Failed to acquire write lock: {}", e),
            }
        })?;

        *state = ModelState::Unloaded;
        self.loaded.store(false, Ordering::SeqCst);
        tracing::info!("CausalModel unloaded");
        Ok(())
    }

    /// Embed a batch of inputs (more efficient than single embed).
    pub async fn embed_batch(&self, inputs: &[ModelInput]) -> EmbeddingResult<Vec<ModelEmbedding>> {
        if !self.is_initialized() {
            return Err(EmbeddingError::NotInitialized {
                model_id: self.model_id(),
            });
        }

        let mut results = Vec::with_capacity(inputs.len());
        for input in inputs {
            results.push(self.embed(input).await?);
        }
        Ok(results)
    }
}

#[async_trait]
impl EmbeddingModel for CausalModel {
    fn model_id(&self) -> ModelId {
        ModelId::Causal
    }

    fn supported_input_types(&self) -> &[InputType] {
        &[InputType::Text]
    }

    fn is_initialized(&self) -> bool {
        self.loaded.load(Ordering::SeqCst)
    }

    #[cfg(not(feature = "candle"))]
    async fn embed(&self, input: &ModelInput) -> EmbeddingResult<ModelEmbedding> {
        if !self.is_initialized() {
            return Err(EmbeddingError::NotInitialized {
                model_id: self.model_id(),
            });
        }

        self.validate_input(input)?;

        let start = std::time::Instant::now();

        // Extract text content for hashing
        let text_content = match input {
            ModelInput::Text { content, instruction } => {
                let mut full = content.clone();
                if let Some(inst) = instruction {
                    full = format!("{} {}", inst, full);
                }
                full
            }
            _ => {
                return Err(EmbeddingError::UnsupportedModality {
                    model_id: ModelId::Causal,
                    input_type: InputType::Code, // Will be matched to actual type
                });
            }
        };

        // Deterministic embedding based on input hash (xxhash64)
        let hash = xxhash_rust::xxh64::xxh64(text_content.as_bytes(), 0);
        let mut rng = StdRng::seed_from_u64(hash);

        let mut vector: Vec<f32> = (0..CAUSAL_DIMENSION)
            .map(|_| rng.gen_range(-1.0..1.0))
            .collect();

        // L2 normalize to unit vector
        let norm: f32 = vector.iter().map(|x| x * x).sum::<f32>().sqrt();
        if norm > f32::EPSILON {
            for v in &mut vector {
                *v /= norm;
            }
        }

        let latency_us = start.elapsed().as_micros() as u64;
        Ok(ModelEmbedding::new(ModelId::Causal, vector, latency_us))
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use std::path::Path;

    fn create_test_model() -> CausalModel {
        CausalModel::new(
            Path::new("models/causal"),
            SingleModelConfig::default(),
        ).expect("Failed to create CausalModel")
    }

    async fn create_and_load_model() -> CausalModel {
        let model = create_test_model();
        model.load().await.expect("Failed to load model");
        model
    }

    // ==================== Construction Tests ====================

    #[test]
    fn test_new_creates_unloaded_model() {
        let model = create_test_model();
        assert!(!model.is_initialized());
    }

    #[test]
    fn test_new_with_zero_batch_size_fails() {
        let config = SingleModelConfig {
            max_batch_size: 0,
            ..Default::default()
        };
        let result = CausalModel::new(Path::new("models/causal"), config);
        assert!(matches!(result, Err(EmbeddingError::ConfigError { .. })));
    }

    #[test]
    fn test_default_global_attention_on_cls() {
        let model = create_test_model();
        assert_eq!(model.global_attention_tokens(), &[0]);
    }

    #[test]
    fn test_default_attention_window() {
        let model = create_test_model();
        assert_eq!(model.attention_window(), DEFAULT_ATTENTION_WINDOW);
    }

    #[test]
    fn test_set_global_attention_tokens() {
        let mut model = create_test_model();
        model.set_global_attention_tokens(&[0, 10, 20]);
        assert_eq!(model.global_attention_tokens(), &[0, 10, 20]);
    }

    // ==================== Trait Implementation Tests ====================

    #[test]
    fn test_model_id() {
        let model = create_test_model();
        assert_eq!(model.model_id(), ModelId::Causal);
    }

    #[test]
    fn test_dimension() {
        let model = create_test_model();
        assert_eq!(model.dimension(), 768);
    }

    #[test]
    fn test_max_tokens() {
        let model = create_test_model();
        assert_eq!(model.max_tokens(), 4096);
    }

    #[test]
    fn test_latency_budget_ms() {
        let model = create_test_model();
        assert_eq!(model.latency_budget_ms(), 8);
    }

    #[test]
    fn test_is_pretrained() {
        let model = create_test_model();
        assert!(model.is_pretrained());
    }

    #[test]
    fn test_supported_input_types() {
        let model = create_test_model();
        assert_eq!(model.supported_input_types(), &[InputType::Text]);
    }

    // ==================== State Transition Tests ====================

    #[tokio::test]
    async fn test_load_sets_initialized() {
        let model = create_test_model();
        assert!(!model.is_initialized());
        model.load().await.expect("Load should succeed");
        assert!(model.is_initialized());
    }

    #[tokio::test]
    async fn test_unload_clears_initialized() {
        let model = create_and_load_model().await;
        assert!(model.is_initialized());
        model.unload().await.expect("Unload should succeed");
        assert!(!model.is_initialized());
    }

    #[tokio::test]
    async fn test_unload_when_not_loaded_fails() {
        let model = create_test_model();
        let result = model.unload().await;
        assert!(matches!(result, Err(EmbeddingError::NotInitialized { .. })));
    }

    #[tokio::test]
    async fn test_state_transitions_full_cycle() {
        let model = create_test_model();

        // new -> unloaded
        assert!(!model.is_initialized());

        // load -> loaded
        model.load().await.unwrap();
        assert!(model.is_initialized());

        // unload -> unloaded
        model.unload().await.unwrap();
        assert!(!model.is_initialized());

        // reload -> loaded again
        model.load().await.unwrap();
        assert!(model.is_initialized());
    }

    // ==================== Embedding Tests ====================

    #[tokio::test]
    async fn test_embed_before_load_fails() {
        let model = create_test_model();
        let input = ModelInput::text("test").expect("Failed to create input");
        let result = model.embed(&input).await;
        assert!(matches!(result, Err(EmbeddingError::NotInitialized { .. })));
    }

    #[tokio::test]
    async fn test_embed_returns_768d_vector() {
        let model = create_and_load_model().await;
        let input = ModelInput::text("Test for causal embedding").expect("Input");

        let embedding = model.embed(&input).await.expect("Embed should succeed");

        assert_eq!(embedding.vector.len(), CAUSAL_DIMENSION);
        assert_eq!(embedding.vector.len(), 768);
    }

    #[tokio::test]
    async fn test_embed_returns_l2_normalized_vector() {
        let model = create_and_load_model().await;
        let input = ModelInput::text("Test normalization").expect("Input");

        let embedding = model.embed(&input).await.expect("Embed");

        let norm: f32 = embedding.vector.iter().map(|x| x * x).sum::<f32>().sqrt();
        assert!((norm - 1.0).abs() < 0.001, "L2 norm should be ~1.0, got {}", norm);
    }

    #[tokio::test]
    async fn test_embed_no_nan_values() {
        let model = create_and_load_model().await;
        let input = ModelInput::text("Check for NaN").expect("Input");

        let embedding = model.embed(&input).await.expect("Embed");

        let has_nan = embedding.vector.iter().any(|x| x.is_nan());
        assert!(!has_nan, "Vector must not contain NaN values");
    }

    #[tokio::test]
    async fn test_embed_no_inf_values() {
        let model = create_and_load_model().await;
        let input = ModelInput::text("Check for Inf").expect("Input");

        let embedding = model.embed(&input).await.expect("Embed");

        let has_inf = embedding.vector.iter().any(|x| x.is_infinite());
        assert!(!has_inf, "Vector must not contain Inf values");
    }

    #[tokio::test]
    async fn test_embed_deterministic() {
        let model = create_and_load_model().await;
        let input = ModelInput::text("Determinism check").expect("Input");

        let emb1 = model.embed(&input).await.expect("Embed 1");
        let emb2 = model.embed(&input).await.expect("Embed 2");

        assert_eq!(emb1.vector, emb2.vector, "Same input must produce identical embeddings");
    }

    #[tokio::test]
    async fn test_embed_different_inputs_differ() {
        let model = create_and_load_model().await;
        let input1 = ModelInput::text("Input A").expect("Input");
        let input2 = ModelInput::text("Input B").expect("Input");

        let emb1 = model.embed(&input1).await.expect("Embed 1");
        let emb2 = model.embed(&input2).await.expect("Embed 2");

        assert_ne!(emb1.vector, emb2.vector, "Different inputs must produce different embeddings");
    }

    #[tokio::test]
    async fn test_embed_model_id_is_causal() {
        let model = create_and_load_model().await;
        let input = ModelInput::text("test").expect("Input");

        let embedding = model.embed(&input).await.expect("Embed");

        assert_eq!(embedding.model_id, ModelId::Causal);
    }

    #[tokio::test]
    async fn test_embed_latency_under_budget() {
        let model = create_and_load_model().await;
        let input = ModelInput::text("Short text for latency test").expect("Input");

        let start = std::time::Instant::now();
        let _embedding = model.embed(&input).await.expect("Embed");
        let elapsed = start.elapsed();

        // Stub should be well under 8ms budget
        assert!(elapsed.as_millis() < 8, "Latency {} ms exceeds 8ms budget", elapsed.as_millis());
    }

    // ==================== Batch Tests ====================

    #[tokio::test]
    async fn test_embed_batch_multiple_inputs() {
        let model = create_and_load_model().await;
        let inputs = vec![
            ModelInput::text("Input 1").expect("Input"),
            ModelInput::text("Input 2").expect("Input"),
            ModelInput::text("Input 3").expect("Input"),
        ];

        let embeddings = model.embed_batch(&inputs).await.expect("Batch embed");

        assert_eq!(embeddings.len(), 3);
        for emb in &embeddings {
            assert_eq!(emb.vector.len(), 768);
            assert_eq!(emb.model_id, ModelId::Causal);
        }
    }

    #[tokio::test]
    async fn test_embed_batch_before_load_fails() {
        let model = create_test_model();
        let inputs = vec![ModelInput::text("test").expect("Input")];

        let result = model.embed_batch(&inputs).await;
        assert!(matches!(result, Err(EmbeddingError::NotInitialized { .. })));
    }

    // ==================== EDGE CASE TESTS (MANDATORY) ====================

    #[tokio::test]
    async fn test_edge_case_1_empty_input() {
        let model = create_and_load_model().await;

        println!("=== EDGE CASE 1: Empty Input ===");
        println!("BEFORE: model initialized = {}", model.is_initialized());

        let input = ModelInput::text("").expect("Input");
        let result = model.embed(&input).await;

        println!("AFTER: result = {:?}", result.is_ok());

        // Empty input MUST still produce valid 768D embedding
        assert!(result.is_ok(), "Empty input should not error");
        let embedding = result.unwrap();
        assert_eq!(embedding.vector.len(), 768);
        println!("AFTER: vector.len() = {}", embedding.vector.len());
    }

    #[tokio::test]
    async fn test_edge_case_2_long_document_4096_tokens() {
        let model = create_and_load_model().await;

        println!("=== EDGE CASE 2: Long Document (simulating 4096 tokens) ===");

        // Simulate a long document (~20K chars, roughly 4000+ tokens)
        let long_text = "This is a test sentence. ".repeat(800);

        println!("BEFORE: input length = {} chars", long_text.len());

        let input = ModelInput::text(&long_text).expect("Input");
        let result = model.embed(&input).await;

        println!("AFTER: result = {:?}", result.is_ok());

        // Must handle long input (real model would truncate at 4096)
        assert!(result.is_ok());
        let embedding = result.unwrap();
        assert_eq!(embedding.vector.len(), 768);
        println!("AFTER: vector.len() = {}", embedding.vector.len());
    }

    #[tokio::test]
    async fn test_edge_case_3_unsupported_modality() {
        let model = create_and_load_model().await;

        println!("=== EDGE CASE 3: Unsupported Modality (Code) ===");
        println!("BEFORE: model supports only Text");

        let input = ModelInput::Code {
            content: "fn main() {}".to_string(),
            language: "rust".to_string(),
        };

        let result = model.embed(&input).await;

        println!("AFTER: result = {:?}", result);

        // MUST return UnsupportedModality error
        assert!(matches!(result, Err(EmbeddingError::UnsupportedModality { .. })));
    }

    // ==================== SOURCE OF TRUTH VERIFICATION ====================

    #[tokio::test]
    async fn test_source_of_truth_verification() {
        let model = create_and_load_model().await;
        let input = ModelInput::text_with_instruction(
            "Cause A leads to Effect B through mechanism C",
            "causal reasoning"
        ).expect("Input");

        // Execute
        let embedding = model.embed(&input).await.expect("Embed should succeed");

        // INSPECT SOURCE OF TRUTH
        println!("=== SOURCE OF TRUTH VERIFICATION ===");
        println!("model_id: {:?}", embedding.model_id);
        println!("vector.len(): {}", embedding.vector.len());
        println!("vector[0..10]: {:?}", &embedding.vector[0..10]);
        println!("latency_us: {}", embedding.latency_us);

        let norm: f32 = embedding.vector.iter().map(|x| x * x).sum::<f32>().sqrt();
        println!("L2 norm: {}", norm);

        let has_nan = embedding.vector.iter().any(|x| x.is_nan());
        let has_inf = embedding.vector.iter().any(|x| x.is_infinite());
        println!("has_nan: {}, has_inf: {}", has_nan, has_inf);

        let all_finite = embedding.vector.iter().all(|x| x.is_finite());
        println!("all values finite: {}", all_finite);

        // VERIFY
        assert_eq!(embedding.model_id, ModelId::Causal);
        assert_eq!(embedding.vector.len(), 768);
        assert!((norm - 1.0).abs() < 0.001);
        assert!(!has_nan && !has_inf);
    }

    // ==================== EVIDENCE OF SUCCESS ====================

    #[tokio::test]
    async fn test_evidence_of_success() {
        println!("\n========================================");
        println!("M03-L07 EVIDENCE OF SUCCESS");
        println!("========================================\n");

        let model = create_and_load_model().await;

        // Test 1: Model metadata
        println!("1. MODEL METADATA:");
        println!("   model_id = {:?}", model.model_id());
        println!("   dimension = {}", model.dimension());
        println!("   max_tokens = {}", model.max_tokens());
        println!("   is_initialized = {}", model.is_initialized());
        println!("   is_pretrained = {}", model.is_pretrained());
        println!("   latency_budget_ms = {}", model.latency_budget_ms());
        println!("   attention_window = {}", model.attention_window());
        println!("   global_attention_tokens = {:?}", model.global_attention_tokens());

        // Test 2: Embed and verify output
        let input = ModelInput::text("Causal reasoning test").expect("Input");

        let start = std::time::Instant::now();
        let embedding = model.embed(&input).await.expect("Embed");
        let elapsed = start.elapsed();

        println!("\n2. EMBEDDING OUTPUT:");
        println!("   vector length = {}", embedding.vector.len());
        println!("   latency = {:?}", elapsed);
        println!("   first 10 values = {:?}", &embedding.vector[0..10]);

        let norm: f32 = embedding.vector.iter().map(|x| x * x).sum::<f32>().sqrt();
        println!("   L2 norm = {}", norm);

        // Test 3: Determinism verification
        println!("\n3. DETERMINISM CHECK:");
        let emb2 = model.embed(&input).await.unwrap();
        let is_deterministic = embedding.vector == emb2.vector;
        println!("   same input same output = {}", is_deterministic);

        // Test 4: Different inputs differ
        println!("\n4. UNIQUENESS CHECK:");
        let input2 = ModelInput::text("Different text").expect("Input");
        let emb3 = model.embed(&input2).await.unwrap();
        let vectors_differ = embedding.vector != emb3.vector;
        println!("   different inputs differ = {}", vectors_differ);

        println!("\n========================================");
        println!("ALL CHECKS PASSED");
        println!("========================================\n");

        assert!(elapsed.as_millis() < 8, "Latency exceeded 8ms budget");
        assert_eq!(embedding.vector.len(), 768);
        assert!((norm - 1.0).abs() < 0.001);
        assert!(is_deterministic, "Same input must be deterministic");
        assert!(vectors_differ, "Different inputs must differ");
    }
}
```

### Files to Modify

#### 2. UPDATE: `crates/context-graph-embeddings/src/models/pretrained/mod.rs`

Add causal module after semantic:
```rust
//! Pretrained embedding models for the 12-model ensemble.
//!
//! This module contains implementations for models that require
//! loading pretrained weights from HuggingFace repositories.
//!
//! # Feature Flags
//!
//! Models require the `candle` feature for actual inference:
//! ```toml
//! context-graph-embeddings = { version = "0.1", features = ["candle"] }
//! ```
//!
//! Without the feature, models use stub implementations for testing.

mod causal;
mod semantic;

pub use causal::{
    CausalModel,
    CAUSAL_DIMENSION,
    CAUSAL_LATENCY_BUDGET_MS,
    CAUSAL_MAX_TOKENS,
    DEFAULT_ATTENTION_WINDOW,
};
pub use semantic::{
    SemanticModel,
    PASSAGE_PREFIX,
    QUERY_PREFIX,
    SEMANTIC_DIMENSION,
    SEMANTIC_LATENCY_BUDGET_MS,
    SEMANTIC_MAX_TOKENS,
};
```

#### 3. UPDATE: `crates/context-graph-embeddings/src/models/mod.rs`

Add CausalModel to exports:
```rust
pub use pretrained::{
    CausalModel, SemanticModel,
    CAUSAL_DIMENSION, CAUSAL_LATENCY_BUDGET_MS, CAUSAL_MAX_TOKENS,
    DEFAULT_ATTENTION_WINDOW, PASSAGE_PREFIX, QUERY_PREFIX,
};
```

---

## Verification Requirements

### Definition of Done Checklist

- [ ] `cargo check -p context-graph-embeddings` passes
- [ ] `cargo test -p context-graph-embeddings causal` passes with 25+ tests
- [ ] `cargo clippy -p context-graph-embeddings -- -D warnings` passes
- [ ] CausalModel implements EmbeddingModel trait
- [ ] `model.model_id()` returns `ModelId::Causal`
- [ ] `model.dimension()` returns 768 (via default impl)
- [ ] `model.max_tokens()` returns 4096 (via default impl)
- [ ] `model.latency_budget_ms()` returns 8 (via default impl)
- [ ] `model.is_initialized()` returns false after construction, true after load
- [ ] `model.is_pretrained()` returns true (via default impl)
- [ ] embed() returns exactly 768D vector
- [ ] Vector is L2 normalized (norm within 0.001 of 1.0)
- [ ] No NaN or Inf values in output
- [ ] Different inputs produce different embeddings
- [ ] Same input produces identical embedding (deterministic)
- [ ] Latency < 8ms (stub implementation)
- [ ] Global attention token configuration works
- [ ] Attention window accessor works

---

## Full State Verification Protocol

### 1. Source of Truth
The embedding output vector stored in `ModelEmbedding.vector`.

### 2. Execute & Inspect Test
Run `cargo test -p context-graph-embeddings test_source_of_truth_verification -- --nocapture`

### 3. Boundary & Edge Case Audit (MANDATORY)
All 3 edge cases must pass:
- Edge Case 1: Empty input → produces valid 768D embedding
- Edge Case 2: Long document (4096 tokens) → handles without error
- Edge Case 3: Unsupported modality (Code) → returns UnsupportedModality error

### 4. Evidence of Success Log
Run `cargo test -p context-graph-embeddings test_evidence_of_success -- --nocapture`

---

## Manual Output Verification (MANDATORY)

After implementing, you MUST manually verify:

```bash
# 1. Check file creation
ls -la crates/context-graph-embeddings/src/models/pretrained/
# Expected: mod.rs, semantic.rs, causal.rs

# 2. Check module exports
grep -n "causal" crates/context-graph-embeddings/src/models/pretrained/mod.rs
# Expected: mod causal; and pub use causal::...

# 3. Check parent exports
grep -n "CausalModel" crates/context-graph-embeddings/src/models/mod.rs
# Expected: pub use pretrained::{CausalModel, ...}

# 4. Run all causal tests
cargo test -p context-graph-embeddings causal -- --nocapture 2>&1 | tail -20
# Expected: test result: ok. 25+ passed; 0 failed

# 5. Verify evidence of success output
cargo test -p context-graph-embeddings test_evidence_of_success -- --nocapture 2>&1 | grep -A 50 "M03-L07 EVIDENCE"
# Expected: "ALL CHECKS PASSED"

# 6. Verify no regressions
cargo test -p context-graph-embeddings 2>&1 | tail -5
# Expected: "test result: ok. N passed; 0 failed"
```

---

## Anti-Patterns to Avoid (from constitution.yaml)

- **AP-001**: Never use `unwrap()` in prod - use `expect()` with context or `?`
- **AP-003**: No magic numbers - constants defined for dimension, max_tokens, etc.
- **AP-007**: Stub data in prod - this is a legitimate stub with `#[cfg(not(feature = "candle"))]`
- **AP-009**: NaN/Infinity in output - L2 normalize safely with epsilon check
- **AP-015**: GPU alloc without pool - this task is CPU stub only

---

## Final Verification: Sherlock Investigation (MANDATORY)

After completing ALL implementation and tests, you MUST spawn the `sherlock-holmes` subagent to perform forensic verification.

Command:
```
Use Task tool with subagent_type="sherlock-holmes" to investigate M03-L07 completion
```

### Sherlock Verification Checklist

| # | Check | How to Verify | Expected Result |
|---|-------|---------------|-----------------|
| 1 | File Existence | `ls .../pretrained/causal.rs` | File exists |
| 2 | Module Export | `grep "mod causal" .../pretrained/mod.rs` | Line found |
| 3 | Trait Implementation | `grep "impl EmbeddingModel for CausalModel"` | Line found |
| 4 | Dimension Accuracy | `cargo test test_embed_returns_768d` | Test passes |
| 5 | Max Tokens | `cargo test test_max_tokens` | Returns 4096 |
| 6 | Normalization | `cargo test test_embed_returns_l2_normalized` | norm = 1.0 |
| 7 | Determinism | `cargo test test_embed_deterministic` | Pass |
| 8 | Different Inputs | `cargo test test_embed_different_inputs_differ` | Pass |
| 9 | Edge Case Empty | `cargo test test_edge_case_1` | Pass |
| 10 | Edge Case Long | `cargo test test_edge_case_2` | Pass |
| 11 | Edge Case Modality | `cargo test test_edge_case_3` | Pass |
| 12 | Latency | `cargo test test_embed_latency_under_budget` | < 8ms |
| 13 | No NaN/Inf | `cargo test test_embed_no_nan test_embed_no_inf` | Pass |
| 14 | No Regressions | `cargo test -p context-graph-embeddings` | All pass |
| 15 | Code Quality | `cargo clippy -- -D warnings` | 0 warnings |

**The task is NOT complete until sherlock-holmes verifies all 15 checks pass.**

---

## Testing Commands

```bash
# Build check
cargo check -p context-graph-embeddings

# Run all CausalModel tests
cargo test -p context-graph-embeddings causal

# Run with output for verification
cargo test -p context-graph-embeddings test_evidence_of_success -- --nocapture

# Run edge case tests with output
cargo test -p context-graph-embeddings test_edge_case -- --nocapture

# Lint check
cargo clippy -p context-graph-embeddings -- -D warnings

# Full test suite (no regressions)
cargo test -p context-graph-embeddings

# Specific verification tests
cargo test -p context-graph-embeddings test_source_of_truth -- --nocapture
```

---

*Task Updated: 2026-01-01*
*Version: 2.0*
*Status: complete*

---

## Completion Record

**Completed**: 2026-01-01
**Verified By**: sherlock-holmes subagent

### Sherlock Verification Summary

All 15 verification checks PASSED:

| # | Check | Result |
|---|-------|--------|
| 1 | File Existence | PASS - causal.rs exists (26531 bytes) |
| 2 | Module Export | PASS - Line 15: mod causal; |
| 3 | Trait Implementation | PASS - Line 253 |
| 4 | Dimension Accuracy | PASS - 768 |
| 5 | Max Tokens | PASS - 4096 |
| 6 | Normalization | PASS - L2 norm = 1.0 |
| 7 | Determinism | PASS |
| 8 | Different Inputs | PASS |
| 9 | Edge Case Empty | PASS |
| 10 | Edge Case Long | PASS |
| 11 | Edge Case Modality | PASS |
| 12 | Latency | PASS - 134μs (< 8ms) |
| 13 | No NaN/Inf | PASS |
| 14 | No Regressions | PASS - 731 tests, 0 failed |
| 15 | Code Quality | PASS - 0 clippy warnings |

### Files Created/Modified

1. **CREATED**: `crates/context-graph-embeddings/src/models/pretrained/causal.rs`
   - 794 lines, 32 dedicated tests

2. **MODIFIED**: `crates/context-graph-embeddings/src/models/pretrained/mod.rs`
   - Added: `mod causal;`
   - Added: `pub use causal::{ CausalModel, CAUSAL_DIMENSION, ... }`

3. **MODIFIED**: `crates/context-graph-embeddings/src/models/mod.rs`
   - Added: `CausalModel` to pretrained exports
