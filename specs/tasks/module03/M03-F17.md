# Task Specification: M03-F17

```xml
<task_spec id="M03-F17" version="2.1">
<metadata>
  <title>Optimized Binary Protocol for Graph Storage (Zero-Copy GDS-Compatible)</title>
  <status>completed</status>
  <completed_date>2026-01-01</completed_date>
  <layer>foundation</layer>
  <sequence>76</sequence>
  <implements>
    - Module 4 Glue: Compact binary serialization for disk-based Knowledge Graph
    - Constitution: storage optimization for 10TB+ deployment
    - PRD: O(1) loading via GDS (GPU Direct Storage)
  </implements>
  <depends_on>
    - M03-F05 (FusedEmbedding) - COMPLETED: crates/context-graph-embeddings/src/types/fused.rs
    - M03-F03 (ModelEmbedding) - COMPLETED: crates/context-graph-embeddings/src/types/embedding.rs
    - M03-S15 (GPU Direct Storage) - PENDING: GDS layer
  </depends_on>
  <estimated_hours>4</estimated_hours>
</metadata>

<current_codebase_state>
## CRITICAL: Existing Implementation Analysis

**Source of Truth**: `crates/context-graph-embeddings/src/types/fused.rs`

### FusedEmbedding Struct (lines 47-61)
```rust
pub struct FusedEmbedding {
    pub vector: Vec<f32>,              // 1536D (FUSED_OUTPUT)
    pub expert_weights: [f32; 8],      // NUM_EXPERTS
    pub selected_experts: [u8; 2],     // TOP_K_EXPERTS
    pub pipeline_latency_us: u64,
    pub content_hash: u64,
    pub aux_data: Option<AuxiliaryEmbeddingData>,
}
```

### Existing Serialization (lines 247-280)
- Current `to_bytes()`: **6198 bytes** (no header)
- Uses **little-endian** byte order
- NO zero-copy support (allocates on read)
- NO GDS alignment (not 64-byte aligned)
- Binary layout:
  | Offset | Size | Field |
  |--------|------|-------|
  | 0 | 6144 | vector (1536 × f32 LE) |
  | 6144 | 32 | expert_weights (8 × f32 LE) |
  | 6176 | 2 | selected_experts (2 × u8) |
  | 6178 | 8 | pipeline_latency_us (u64 LE) |
  | 6186 | 8 | content_hash (u64 LE) |
  | 6194 | 4 | aux_data_len (u32 LE) |
  | 6198+ | var | aux_data blob |

### Dimension Constants (dimensions.rs, lines 66-76)
```rust
pub const NUM_EXPERTS: usize = 8;
pub const TOP_K_EXPERTS: usize = 2;
pub const FUSED_OUTPUT: usize = 1536;
pub const COLBERT_V3_DIM: usize = 128;
```

### Dependencies Already Available (Cargo.toml)
- `serde` + `serde_json` (JSON serialization exists)
- `xxhash-rust` (content hashing)
- `thiserror` (error types)

### Dependencies To Add
- `bytemuck = { version = "1.14", features = ["derive"] }` (zero-copy)
</current_codebase_state>

<context>
## Problem Statement

The current `FusedEmbedding::to_bytes()` implementation (fused.rs:247-280) is NOT GDS-compatible:
1. NO 64-byte header (can't identify/version files)
2. NO 64-byte alignment (GDS requires aligned reads)
3. Uses Vec<u8> allocation on read (not zero-copy)
4. Little-endian only (platform-dependent)

## Why This Matters

For 10TB+ deployment with RTX 5090 / CUDA 13.1:
- GDS enables NVMe→GPU direct transfer (bypasses CPU)
- Zero-copy means O(1) loading latency
- 64-byte alignment matches cache-line and GDS requirements
- Versioned format allows schema evolution without migration tools

## Design Decision: New Storage Module

DO NOT modify existing `fused.rs::to_bytes()` - it's used by existing code.
CREATE NEW storage module with GDS-compatible format:
- `crates/context-graph-embeddings/src/storage/mod.rs`
- `crates/context-graph-embeddings/src/storage/binary.rs`
- `crates/context-graph-embeddings/src/storage/batch.rs`
- `crates/context-graph-embeddings/src/storage/gds.rs`

## NO BACKWARDS COMPATIBILITY

This is a NEW storage format for Module 4 Knowledge Graph integration.
The system must work correctly or fail fast with clear error messages.
DO NOT create workarounds, fallbacks, or graceful degradation.
If something fails, error immediately with full context.
</context>

<definition_of_done>
  <signatures>
```rust
// =============================================================================
// FILE: crates/context-graph-embeddings/src/storage/mod.rs
// =============================================================================

//! GDS-compatible binary storage for FusedEmbedding.
//!
//! This module provides zero-copy serialization with 64-byte alignment
//! for GPU Direct Storage (GDS) integration in the Knowledge Graph.

pub mod binary;
pub mod batch;
pub mod gds;

pub use binary::{
    EmbeddingBinaryCodec,
    EmbeddingHeader,
    FusedEmbeddingRef,
    CompressionType,
    EMBEDDING_BINARY_VERSION,
    EMBEDDING_MAGIC,
};
pub use batch::{BatchBinaryEncoder, EmbeddingIndexHeader};
pub use gds::GdsFile;

// Re-export error types
pub use binary::{EncodeError, DecodeError};

// =============================================================================
// FILE: crates/context-graph-embeddings/src/storage/binary.rs
// =============================================================================

use crate::error::{EmbeddingError, EmbeddingResult};
use crate::types::{FusedEmbedding, AuxiliaryEmbeddingData};
use crate::types::dimensions::{FUSED_OUTPUT, NUM_EXPERTS, TOP_K_EXPERTS};
use bytemuck::{Pod, Zeroable, bytes_of, from_bytes, try_from_bytes};
use std::io::{self, Read, Write};
use std::fs::File;
use std::path::Path;

/// Binary format version. Increment when format changes.
/// Version 1: Initial GDS-compatible format with 64-byte header.
pub const EMBEDDING_BINARY_VERSION: u16 = 1;

/// Magic bytes for file identification: "CGEB" = Context Graph Embedding Binary
pub const EMBEDDING_MAGIC: [u8; 4] = [0x43, 0x47, 0x45, 0x42];

/// Fixed-size binary header (64 bytes, cache-line aligned).
/// MUST remain exactly 64 bytes for GDS compatibility.
#[repr(C, align(64))]
#[derive(Debug, Clone, Copy, Pod, Zeroable)]
pub struct EmbeddingHeader {
    /// Magic bytes: "CGEB" (0x43 0x47 0x45 0x42)
    pub magic: [u8; 4],
    /// Format version (big-endian)
    pub version: u16,
    /// Flags: bit 0 = has_aux_data, bit 1 = compressed_aux, bits 2-15 reserved
    pub flags: u16,
    /// Vector dimension (1536 for FusedEmbedding)
    pub dimension: u32,
    /// Number of experts (8)
    pub num_experts: u8,
    /// Top-K experts selected (2)
    pub top_k: u8,
    /// Reserved for future use
    pub _reserved: [u8; 2],
    /// Content hash (xxHash64) for integrity verification
    pub content_hash: u64,
    /// Pipeline latency in microseconds
    pub pipeline_latency_us: u64,
    /// Auxiliary data offset from start of record (0 if none)
    pub aux_data_offset: u64,
    /// Auxiliary data length in bytes (0 if none)
    pub aux_data_length: u64,
    /// Padding to reach exactly 64 bytes
    pub _padding: [u8; 16],
}

// Compile-time assertion: header must be exactly 64 bytes
const _HEADER_SIZE_CHECK: () = assert!(
    std::mem::size_of::<EmbeddingHeader>() == 64,
    "EmbeddingHeader must be exactly 64 bytes"
);

/// Compression type for auxiliary data.
#[derive(Debug, Clone, Copy, PartialEq, Eq)]
pub enum CompressionType {
    /// No compression (raw bytes)
    None,
    /// LZ4 fast compression (not implemented in v1)
    Lz4,
    /// Zstd compression (not implemented in v1)
    Zstd,
}

/// Binary encoder/decoder for FusedEmbedding.
///
/// Produces GDS-compatible format:
/// - 64-byte aligned header
/// - Big-endian floats for cross-platform compatibility
/// - Zero-copy decode via memory mapping
pub struct EmbeddingBinaryCodec {
    /// Include auxiliary ColBERT data in output
    include_aux_data: bool,
    /// Compression for aux_data (v1 only supports None)
    aux_compression: CompressionType,
}

impl EmbeddingBinaryCodec {
    /// Minimum buffer size without aux_data.
    /// Header(64) + Vector(6144) + Weights(32) + Selected(2) = 6242 bytes
    pub const MIN_BUFFER_SIZE: usize = 64 + (FUSED_OUTPUT * 4) + (NUM_EXPERTS * 4) + TOP_K_EXPERTS;

    /// Create codec with default settings (no aux_data).
    #[must_use]
    pub fn new() -> Self {
        Self {
            include_aux_data: false,
            aux_compression: CompressionType::None,
        }
    }

    /// Create codec with auxiliary data support.
    #[must_use]
    pub fn with_aux_data(compression: CompressionType) -> Self {
        Self {
            include_aux_data: true,
            aux_compression: compression,
        }
    }

    /// Encode FusedEmbedding to GDS-compatible bytes.
    ///
    /// # Binary Layout
    /// | Offset | Size | Field |
    /// |--------|------|-------|
    /// | 0 | 64 | EmbeddingHeader (cache-line aligned) |
    /// | 64 | 6144 | Vector: [f32; 1536] big-endian |
    /// | 6208 | 32 | ExpertWeights: [f32; 8] big-endian |
    /// | 6240 | 2 | SelectedExperts: [u8; 2] |
    /// | 6242 | var | AuxData (if present) |
    ///
    /// # Errors
    /// - `EncodeError::InvalidDimension` if vector dimension != 1536
    pub fn encode(&self, embedding: &FusedEmbedding) -> Result<Vec<u8>, EncodeError> {
        // Validate dimension - FAIL FAST
        if embedding.vector.len() != FUSED_OUTPUT {
            return Err(EncodeError::InvalidDimension {
                expected: FUSED_OUTPUT,
                actual: embedding.vector.len(),
            });
        }

        // Prepare aux_data if requested
        let aux_blob = if self.include_aux_data {
            embedding.aux_data.as_ref().map(|a| a.to_blob())
        } else {
            None
        };

        let aux_offset = if aux_blob.is_some() { Self::MIN_BUFFER_SIZE as u64 } else { 0 };
        let aux_length = aux_blob.as_ref().map(|b| b.len() as u64).unwrap_or(0);

        // Build header
        let mut flags: u16 = 0;
        if aux_blob.is_some() {
            flags |= 0x01; // bit 0: has_aux_data
        }

        let header = EmbeddingHeader {
            magic: EMBEDDING_MAGIC,
            version: EMBEDDING_BINARY_VERSION.to_be(),
            flags: flags.to_be(),
            dimension: (FUSED_OUTPUT as u32).to_be(),
            num_experts: NUM_EXPERTS as u8,
            top_k: TOP_K_EXPERTS as u8,
            _reserved: [0; 2],
            content_hash: embedding.content_hash.to_be(),
            pipeline_latency_us: embedding.pipeline_latency_us.to_be(),
            aux_data_offset: aux_offset.to_be(),
            aux_data_length: aux_length.to_be(),
            _padding: [0; 16],
        };

        // Allocate buffer
        let total_size = Self::MIN_BUFFER_SIZE + aux_length as usize;
        let mut buffer = Vec::with_capacity(total_size);

        // Header (64 bytes)
        buffer.extend_from_slice(bytes_of(&header));

        // Vector (6144 bytes) - big-endian
        for &val in &embedding.vector {
            buffer.extend_from_slice(&val.to_be_bytes());
        }

        // Expert weights (32 bytes) - big-endian
        for &weight in &embedding.expert_weights {
            buffer.extend_from_slice(&weight.to_be_bytes());
        }

        // Selected experts (2 bytes)
        buffer.extend_from_slice(&embedding.selected_experts);

        // Auxiliary data (if present)
        if let Some(aux) = aux_blob {
            buffer.extend_from_slice(&aux);
        }

        debug_assert_eq!(buffer.len(), total_size, "Buffer size mismatch");
        Ok(buffer)
    }

    /// Encode directly to pre-allocated buffer (zero-copy write).
    ///
    /// # Errors
    /// - `EncodeError::BufferTooSmall` if buffer is too small
    /// - `EncodeError::InvalidDimension` if vector dimension != 1536
    pub fn encode_to_buffer(
        &self,
        embedding: &FusedEmbedding,
        buffer: &mut [u8],
    ) -> Result<usize, EncodeError> {
        let encoded = self.encode(embedding)?;
        if buffer.len() < encoded.len() {
            return Err(EncodeError::BufferTooSmall {
                needed: encoded.len(),
                available: buffer.len(),
            });
        }
        buffer[..encoded.len()].copy_from_slice(&encoded);
        Ok(encoded.len())
    }

    /// Encode to file.
    ///
    /// # Errors
    /// - `EncodeError::Io` on file write failure
    pub fn encode_to_file(
        &self,
        embedding: &FusedEmbedding,
        file: &mut File,
    ) -> Result<u64, EncodeError> {
        let encoded = self.encode(embedding)?;
        file.write_all(&encoded)?;
        Ok(encoded.len() as u64)
    }

    /// Decode FusedEmbedding from bytes.
    ///
    /// # Errors
    /// - `DecodeError::BufferTooShort` if bytes < MIN_BUFFER_SIZE
    /// - `DecodeError::InvalidMagic` if magic bytes don't match
    /// - `DecodeError::UnsupportedVersion` if version > current
    /// - `DecodeError::HashMismatch` if verify_hash=true and hash doesn't match
    pub fn decode(&self, bytes: &[u8]) -> Result<FusedEmbedding, DecodeError> {
        // Validate minimum size - FAIL FAST
        if bytes.len() < Self::MIN_BUFFER_SIZE {
            return Err(DecodeError::BufferTooShort {
                needed: Self::MIN_BUFFER_SIZE,
                available: bytes.len(),
            });
        }

        // Parse header
        let header = self.decode_header(bytes)?;

        // Parse vector (big-endian)
        let mut vector = Vec::with_capacity(FUSED_OUTPUT);
        for i in 0..FUSED_OUTPUT {
            let offset = 64 + i * 4;
            let val = f32::from_be_bytes([
                bytes[offset],
                bytes[offset + 1],
                bytes[offset + 2],
                bytes[offset + 3],
            ]);
            vector.push(val);
        }

        // Parse expert weights (big-endian)
        let mut expert_weights = [0.0f32; NUM_EXPERTS];
        for i in 0..NUM_EXPERTS {
            let offset = 64 + (FUSED_OUTPUT * 4) + i * 4;
            expert_weights[i] = f32::from_be_bytes([
                bytes[offset],
                bytes[offset + 1],
                bytes[offset + 2],
                bytes[offset + 3],
            ]);
        }

        // Parse selected experts
        let selected_offset = 64 + (FUSED_OUTPUT * 4) + (NUM_EXPERTS * 4);
        let selected_experts = [bytes[selected_offset], bytes[selected_offset + 1]];

        // Parse aux_data if present
        let aux_data_offset = u64::from_be(header.aux_data_offset) as usize;
        let aux_data_length = u64::from_be(header.aux_data_length) as usize;

        let aux_data = if aux_data_offset > 0 && aux_data_length > 0 {
            let end = aux_data_offset + aux_data_length;
            if bytes.len() < end {
                return Err(DecodeError::BufferTooShort {
                    needed: end,
                    available: bytes.len(),
                });
            }
            Some(AuxiliaryEmbeddingData::from_blob(&bytes[aux_data_offset..end])
                .map_err(|e| DecodeError::AuxDataCorrupted(e.to_string()))?)
        } else {
            None
        };

        Ok(FusedEmbedding {
            vector,
            expert_weights,
            selected_experts,
            pipeline_latency_us: u64::from_be(header.pipeline_latency_us),
            content_hash: u64::from_be(header.content_hash),
            aux_data,
        })
    }

    /// Decode header only (for seeking/filtering).
    ///
    /// # Errors
    /// - `DecodeError::BufferTooShort` if bytes < 64
    /// - `DecodeError::InvalidMagic` if magic bytes don't match
    /// - `DecodeError::UnsupportedVersion` if version > current
    pub fn decode_header(&self, bytes: &[u8]) -> Result<EmbeddingHeader, DecodeError> {
        if bytes.len() < 64 {
            return Err(DecodeError::BufferTooShort {
                needed: 64,
                available: bytes.len(),
            });
        }

        let header: &EmbeddingHeader = try_from_bytes(&bytes[0..64])
            .map_err(|_| DecodeError::InvalidMagic)?;

        // Validate magic - FAIL FAST
        if header.magic != EMBEDDING_MAGIC {
            return Err(DecodeError::InvalidMagic);
        }

        // Validate version - FAIL FAST
        let version = u16::from_be(header.version);
        if version > EMBEDDING_BINARY_VERSION {
            return Err(DecodeError::UnsupportedVersion(version));
        }

        Ok(*header)
    }

    /// Decode with zero-copy reference to memory-mapped buffer.
    ///
    /// Returns a borrowed view into the buffer - no heap allocation.
    ///
    /// # Errors
    /// - Same as `decode_header`
    /// - `DecodeError::AlignmentError` if buffer not 64-byte aligned
    pub fn decode_zero_copy<'a>(&self, bytes: &'a [u8]) -> Result<FusedEmbeddingRef<'a>, DecodeError> {
        // Validate alignment for zero-copy
        if bytes.as_ptr() as usize % 64 != 0 {
            return Err(DecodeError::AlignmentError {
                expected: 64,
                actual: bytes.as_ptr() as usize % 64,
            });
        }

        let header = self.decode_header(bytes)?;

        // Validate buffer size
        if bytes.len() < Self::MIN_BUFFER_SIZE {
            return Err(DecodeError::BufferTooShort {
                needed: Self::MIN_BUFFER_SIZE,
                available: bytes.len(),
            });
        }

        // Create references into buffer (zero-copy)
        let header_ref: &EmbeddingHeader = try_from_bytes(&bytes[0..64])
            .map_err(|_| DecodeError::InvalidMagic)?;

        // Vector slice: bytes 64..6208
        let vector_bytes = &bytes[64..64 + FUSED_OUTPUT * 4];
        let vector: &[f32; FUSED_OUTPUT] = try_from_bytes(vector_bytes)
            .map_err(|_| DecodeError::AlignmentError { expected: 4, actual: 0 })?;

        // Expert weights slice: bytes 6208..6240
        let weights_bytes = &bytes[64 + FUSED_OUTPUT * 4..64 + FUSED_OUTPUT * 4 + NUM_EXPERTS * 4];
        let expert_weights: &[f32; NUM_EXPERTS] = try_from_bytes(weights_bytes)
            .map_err(|_| DecodeError::AlignmentError { expected: 4, actual: 0 })?;

        // Selected experts: bytes 6240..6242
        let selected_offset = 64 + FUSED_OUTPUT * 4 + NUM_EXPERTS * 4;
        let selected_experts: &[u8; TOP_K_EXPERTS] = try_from_bytes(&bytes[selected_offset..selected_offset + 2])
            .map_err(|_| DecodeError::BufferTooShort { needed: 2, available: 0 })?;

        // Aux data reference (if present)
        let aux_data_offset = u64::from_be(header_ref.aux_data_offset) as usize;
        let aux_data_length = u64::from_be(header_ref.aux_data_length) as usize;
        let aux_data = if aux_data_offset > 0 && aux_data_length > 0 {
            if bytes.len() < aux_data_offset + aux_data_length {
                return Err(DecodeError::BufferTooShort {
                    needed: aux_data_offset + aux_data_length,
                    available: bytes.len(),
                });
            }
            Some(&bytes[aux_data_offset..aux_data_offset + aux_data_length])
        } else {
            None
        };

        Ok(FusedEmbeddingRef {
            header: header_ref,
            vector,
            expert_weights,
            selected_experts,
            aux_data,
        })
    }

    /// Compute serialized size for an embedding.
    #[must_use]
    pub fn serialized_size(&self, embedding: &FusedEmbedding) -> usize {
        let aux_size = if self.include_aux_data {
            embedding.aux_data.as_ref().map(|a| a.to_blob().len()).unwrap_or(0)
        } else {
            0
        };
        Self::MIN_BUFFER_SIZE + aux_size
    }
}

impl Default for EmbeddingBinaryCodec {
    fn default() -> Self {
        Self::new()
    }
}

/// Zero-copy reference to FusedEmbedding in memory-mapped buffer.
///
/// All data is borrowed from the underlying buffer - no heap allocation.
/// NOTE: Vector values are big-endian and need byte-swapping on read.
pub struct FusedEmbeddingRef<'a> {
    header: &'a EmbeddingHeader,
    /// NOTE: Values are big-endian, use `vector()` method for conversion
    vector: &'a [f32; FUSED_OUTPUT],
    /// NOTE: Values are big-endian, use `expert_weights()` method for conversion
    expert_weights: &'a [f32; NUM_EXPERTS],
    selected_experts: &'a [u8; TOP_K_EXPERTS],
    aux_data: Option<&'a [u8]>,
}

impl<'a> FusedEmbeddingRef<'a> {
    /// Get vector with byte-swapping from big-endian.
    pub fn vector(&self) -> [f32; FUSED_OUTPUT] {
        let mut result = [0.0f32; FUSED_OUTPUT];
        for (i, &be_val) in self.vector.iter().enumerate() {
            result[i] = f32::from_be_bytes(be_val.to_ne_bytes());
        }
        result
    }

    /// Get expert weights with byte-swapping from big-endian.
    pub fn expert_weights(&self) -> [f32; NUM_EXPERTS] {
        let mut result = [0.0f32; NUM_EXPERTS];
        for (i, &be_val) in self.expert_weights.iter().enumerate() {
            result[i] = f32::from_be_bytes(be_val.to_ne_bytes());
        }
        result
    }

    /// Get selected experts (no byte-swapping needed for u8).
    #[inline]
    pub fn selected_experts(&self) -> &[u8; TOP_K_EXPERTS] {
        self.selected_experts
    }

    /// Convert to owned FusedEmbedding (allocates).
    pub fn to_owned(&self) -> FusedEmbedding {
        let vector = self.vector().to_vec();
        let aux_data = self.aux_data.and_then(|blob| {
            AuxiliaryEmbeddingData::from_blob(blob).ok()
        });

        FusedEmbedding {
            vector,
            expert_weights: self.expert_weights(),
            selected_experts: *self.selected_experts,
            pipeline_latency_us: u64::from_be(self.header.pipeline_latency_us),
            content_hash: u64::from_be(self.header.content_hash),
            aux_data,
        }
    }

    /// Get content hash.
    #[inline]
    pub fn content_hash(&self) -> u64 {
        u64::from_be(self.header.content_hash)
    }

    /// Check if embedding has auxiliary data.
    #[inline]
    pub fn has_aux_data(&self) -> bool {
        self.aux_data.is_some()
    }

    /// Get raw aux_data bytes (for deferred parsing).
    #[inline]
    pub fn aux_data_bytes(&self) -> Option<&'a [u8]> {
        self.aux_data
    }
}

/// Errors during encoding.
#[derive(Debug, thiserror::Error)]
pub enum EncodeError {
    #[error("Buffer too small: need {needed} bytes, have {available}")]
    BufferTooSmall { needed: usize, available: usize },

    #[error("Invalid embedding dimension: expected {expected}, got {actual}")]
    InvalidDimension { expected: usize, actual: usize },

    #[error("IO error: {0}")]
    Io(#[from] io::Error),
}

/// Errors during decoding.
#[derive(Debug, thiserror::Error)]
pub enum DecodeError {
    #[error("Invalid magic bytes: expected 'CGEB' (0x43474542)")]
    InvalidMagic,

    #[error("Unsupported format version: {0} (max supported: {EMBEDDING_BINARY_VERSION})")]
    UnsupportedVersion(u16),

    #[error("Buffer too short: need {needed} bytes, have {available}")]
    BufferTooShort { needed: usize, available: usize },

    #[error("Content hash mismatch: file may be corrupted")]
    HashMismatch,

    #[error("Alignment error: expected {expected}-byte alignment, got offset {actual}")]
    AlignmentError { expected: usize, actual: usize },

    #[error("Auxiliary data corrupted: {0}")]
    AuxDataCorrupted(String),

    #[error("IO error: {0}")]
    Io(#[from] io::Error),
}

// =============================================================================
// FILE: crates/context-graph-embeddings/src/storage/batch.rs
// =============================================================================

use super::binary::{EmbeddingBinaryCodec, EncodeError, EMBEDDING_MAGIC};
use crate::types::FusedEmbedding;
use bytemuck::{Pod, Zeroable, bytes_of};
use std::io::{self, Write};
use std::path::Path;
use std::fs::OpenOptions;

/// Index file header for batch embeddings.
#[repr(C)]
#[derive(Debug, Clone, Copy, Pod, Zeroable)]
pub struct EmbeddingIndexHeader {
    /// Magic bytes: "CGEI" = Context Graph Embedding Index
    pub magic: [u8; 4],
    /// Format version
    pub version: u16,
    /// Reserved
    pub _reserved: u16,
    /// Number of entries in the index
    pub entry_count: u64,
    /// Hash of associated data file (for integrity)
    pub data_file_hash: u64,
}

/// Index file magic bytes.
pub const INDEX_MAGIC: [u8; 4] = [0x43, 0x47, 0x45, 0x49]; // "CGEI"

/// Batch encoder for efficient multi-embedding serialization.
pub struct BatchBinaryEncoder {
    codec: EmbeddingBinaryCodec,
    buffer: Vec<u8>,
    offsets: Vec<u64>,
}

impl BatchBinaryEncoder {
    /// GDS page alignment (4KB).
    pub const PAGE_SIZE: usize = 4096;

    /// Create batch encoder with capacity.
    #[must_use]
    pub fn with_capacity(count: usize) -> Self {
        Self {
            codec: EmbeddingBinaryCodec::new(),
            buffer: Vec::with_capacity(count * EmbeddingBinaryCodec::MIN_BUFFER_SIZE),
            offsets: Vec::with_capacity(count),
        }
    }

    /// Create batch encoder with aux_data support.
    #[must_use]
    pub fn with_aux_data(count: usize) -> Self {
        Self {
            codec: EmbeddingBinaryCodec::with_aux_data(super::binary::CompressionType::None),
            buffer: Vec::with_capacity(count * (EmbeddingBinaryCodec::MIN_BUFFER_SIZE + 1024)),
            offsets: Vec::with_capacity(count),
        }
    }

    /// Add embedding to batch.
    ///
    /// # Errors
    /// - `EncodeError` if encoding fails
    pub fn push(&mut self, embedding: &FusedEmbedding) -> Result<(), EncodeError> {
        let offset = self.buffer.len() as u64;
        let encoded = self.codec.encode(embedding)?;
        self.offsets.push(offset);
        self.buffer.extend_from_slice(&encoded);
        Ok(())
    }

    /// Get number of embeddings in batch.
    #[inline]
    pub fn len(&self) -> usize {
        self.offsets.len()
    }

    /// Check if batch is empty.
    #[inline]
    pub fn is_empty(&self) -> bool {
        self.offsets.is_empty()
    }

    /// Finalize and get bytes with offset table.
    pub fn finalize(self) -> (Vec<u8>, Vec<u64>) {
        (self.buffer, self.offsets)
    }

    /// Write GDS-compatible files: .cgeb (data) and .cgei (index).
    ///
    /// Data file is 4KB-page aligned for optimal GDS performance.
    ///
    /// # Errors
    /// - `io::Error` on file write failure
    pub fn write_gds_file(&self, path: &Path) -> io::Result<()> {
        // Data file
        let data_path = path.with_extension("cgeb");
        let mut data_file = OpenOptions::new()
            .create(true)
            .write(true)
            .truncate(true)
            .open(&data_path)?;

        // Recompute offsets with 4KB page alignment
        let mut aligned_offsets = Vec::with_capacity(self.offsets.len());
        let mut current_offset = 0u64;

        // Write each embedding with page alignment
        for (i, original_offset) in self.offsets.iter().enumerate() {
            // Calculate embedding size
            let end_offset = if i + 1 < self.offsets.len() {
                self.offsets[i + 1]
            } else {
                self.buffer.len() as u64
            };
            let embedding_size = (end_offset - original_offset) as usize;

            // Pad to page boundary
            let padding = (Self::PAGE_SIZE - (current_offset as usize % Self::PAGE_SIZE)) % Self::PAGE_SIZE;
            if padding > 0 {
                data_file.write_all(&vec![0u8; padding])?;
                current_offset += padding as u64;
            }

            aligned_offsets.push(current_offset);

            // Write embedding data
            let start = *original_offset as usize;
            let end = start + embedding_size;
            data_file.write_all(&self.buffer[start..end])?;
            current_offset += embedding_size as u64;
        }

        // Compute data file hash
        let data_hash = xxhash_rust::xxh64::xxh64(&self.buffer, 0);

        // Index file
        let index_path = path.with_extension("cgei");
        let mut index_file = OpenOptions::new()
            .create(true)
            .write(true)
            .truncate(true)
            .open(&index_path)?;

        let index_header = EmbeddingIndexHeader {
            magic: INDEX_MAGIC,
            version: 1u16.to_be(),
            _reserved: 0,
            entry_count: (aligned_offsets.len() as u64).to_be(),
            data_file_hash: data_hash.to_be(),
        };

        index_file.write_all(bytes_of(&index_header))?;

        // Write offset table (big-endian)
        for &offset in &aligned_offsets {
            index_file.write_all(&offset.to_be_bytes())?;
        }

        Ok(())
    }
}

// =============================================================================
// FILE: crates/context-graph-embeddings/src/storage/gds.rs
// =============================================================================

use super::batch::EmbeddingIndexHeader;
use super::binary::{DecodeError, EmbeddingBinaryCodec, FusedEmbeddingRef};
use std::fs::File;
use std::io::{self, Read, Seek, SeekFrom};
use std::path::Path;

/// GDS file reader for batch embeddings.
///
/// Supports O(1) seeking to any embedding by index.
pub struct GdsFile {
    data_file: File,
    offsets: Vec<u64>,
    codec: EmbeddingBinaryCodec,
}

impl GdsFile {
    /// Open GDS file pair (.cgeb data + .cgei index).
    ///
    /// # Errors
    /// - `io::Error` on file open failure
    /// - `DecodeError::InvalidMagic` if index magic doesn't match
    pub fn open(path: &Path) -> Result<Self, GdsFileError> {
        let data_path = path.with_extension("cgeb");
        let index_path = path.with_extension("cgei");

        // Read index file
        let mut index_file = File::open(&index_path)?;
        let mut header_bytes = [0u8; 24];
        index_file.read_exact(&mut header_bytes)?;

        let header: &EmbeddingIndexHeader = bytemuck::from_bytes(&header_bytes);

        // Validate magic
        if header.magic != super::batch::INDEX_MAGIC {
            return Err(GdsFileError::InvalidIndexMagic);
        }

        let entry_count = u64::from_be(header.entry_count) as usize;

        // Read offset table
        let mut offsets = Vec::with_capacity(entry_count);
        for _ in 0..entry_count {
            let mut offset_bytes = [0u8; 8];
            index_file.read_exact(&mut offset_bytes)?;
            offsets.push(u64::from_be_bytes(offset_bytes));
        }

        let data_file = File::open(&data_path)?;

        Ok(Self {
            data_file,
            offsets,
            codec: EmbeddingBinaryCodec::new(),
        })
    }

    /// Number of embeddings in file.
    #[inline]
    pub fn len(&self) -> usize {
        self.offsets.len()
    }

    /// Check if file has no embeddings.
    #[inline]
    pub fn is_empty(&self) -> bool {
        self.offsets.is_empty()
    }

    /// Read embedding at index (O(1) seek + read).
    ///
    /// # Errors
    /// - `GdsFileError::IndexOutOfBounds` if index >= len
    /// - `GdsFileError::Decode` on decode failure
    pub fn read(&mut self, index: usize) -> Result<crate::types::FusedEmbedding, GdsFileError> {
        if index >= self.offsets.len() {
            return Err(GdsFileError::IndexOutOfBounds {
                index,
                len: self.offsets.len(),
            });
        }

        let offset = self.offsets[index];

        // Calculate size
        let size = if index + 1 < self.offsets.len() {
            (self.offsets[index + 1] - offset) as usize
        } else {
            // Last entry - read MIN_BUFFER_SIZE (no aux_data support for now)
            EmbeddingBinaryCodec::MIN_BUFFER_SIZE
        };

        // Seek and read
        self.data_file.seek(SeekFrom::Start(offset))?;
        let mut buffer = vec![0u8; size];
        self.data_file.read_exact(&mut buffer)?;

        self.codec.decode(&buffer).map_err(GdsFileError::Decode)
    }
}

/// Errors for GDS file operations.
#[derive(Debug, thiserror::Error)]
pub enum GdsFileError {
    #[error("Invalid index file magic bytes: expected 'CGEI'")]
    InvalidIndexMagic,

    #[error("Index out of bounds: {index} >= {len}")]
    IndexOutOfBounds { index: usize, len: usize },

    #[error("Decode error: {0}")]
    Decode(#[from] DecodeError),

    #[error("IO error: {0}")]
    Io(#[from] io::Error),
}
```
  </signatures>

  <constraints>
    - Header MUST be exactly 64 bytes (cache-line aligned for GDS)
    - All multi-byte values use big-endian for cross-platform compatibility
    - bytemuck crate for safe zero-copy transmutation (NO raw unsafe)
    - NO backwards compatibility with existing fused.rs::to_bytes() format
    - FAIL FAST on any validation error - no silent fallbacks
    - 4KB page alignment in batch files for GDS optimal I/O
    - Content hash preserved but NOT re-verified by default (opt-in)
  </constraints>
</definition_of_done>

<files_to_create>
  <file>crates/context-graph-embeddings/src/storage/mod.rs</file>
  <file>crates/context-graph-embeddings/src/storage/binary.rs</file>
  <file>crates/context-graph-embeddings/src/storage/batch.rs</file>
  <file>crates/context-graph-embeddings/src/storage/gds.rs</file>
</files_to_create>

<files_to_modify>
  <file>crates/context-graph-embeddings/src/lib.rs - Add: pub mod storage;</file>
  <file>crates/context-graph-embeddings/Cargo.toml - Add: bytemuck = { version = "1.14", features = ["derive"] }</file>
</files_to_modify>

<validation_criteria>
  <criterion>cargo check passes with no warnings</criterion>
  <criterion>cargo test storage passes all tests</criterion>
  <criterion>Round-trip encode/decode preserves exact binary values (bitwise identical)</criterion>
  <criterion>EmbeddingHeader is exactly 64 bytes (compile-time assertion)</criterion>
  <criterion>Serialized size without aux_data = 6242 bytes exactly</criterion>
  <criterion>Decode latency &lt;10μs for single embedding (benchmark)</criterion>
  <criterion>Invalid magic bytes return DecodeError::InvalidMagic immediately</criterion>
  <criterion>Truncated buffers return DecodeError::BufferTooShort immediately</criterion>
  <criterion>Zero-copy decode does NOT allocate heap memory</criterion>
</validation_criteria>
</task_spec>
```

## Implementation Steps

### Step 1: Add Dependency

Edit `crates/context-graph-embeddings/Cargo.toml`:
```toml
[dependencies]
# ... existing ...
bytemuck = { version = "1.14", features = ["derive"] }
```

### Step 2: Create Storage Module

Create directory: `crates/context-graph-embeddings/src/storage/`

### Step 3: Implement in Order

1. `storage/binary.rs` - Core types and codec
2. `storage/batch.rs` - Batch encoder for GDS files
3. `storage/gds.rs` - GDS file reader
4. `storage/mod.rs` - Re-exports

### Step 4: Update lib.rs

Add to `crates/context-graph-embeddings/src/lib.rs`:
```rust
pub mod storage;
```

## Full State Verification Requirements

### Source of Truth

| Data Element | Source Location | Verification Method |
|-------------|-----------------|---------------------|
| FusedEmbedding struct | `types/fused.rs:47-61` | Direct field access after encode/decode |
| Dimension constants | `types/dimensions.rs:66-142` | Compile-time assertions |
| EmbeddingHeader size | `storage/binary.rs` | `const _HEADER_SIZE_CHECK` |
| Binary buffer size | `storage/binary.rs::MIN_BUFFER_SIZE` | Assert equals 6242 |
| Encoded bytes | `EmbeddingBinaryCodec::encode()` return | Compare len to expected |
| Decoded embedding | `EmbeddingBinaryCodec::decode()` return | Compare all fields to original |

### Execute & Inspect Pattern

```rust
// WRONG: Just call encode
let bytes = codec.encode(&embedding)?;

// CORRECT: Call encode, then separately verify
let bytes = codec.encode(&embedding)?;
assert_eq!(bytes.len(), EmbeddingBinaryCodec::MIN_BUFFER_SIZE);
let header = codec.decode_header(&bytes)?;
assert_eq!(header.magic, EMBEDDING_MAGIC);
assert_eq!(u16::from_be(header.version), EMBEDDING_BINARY_VERSION);
```

### Boundary & Edge Case Audit

**Edge Case 1: Minimum Valid Embedding**
- Before: FusedEmbedding with vector = [0.0; 1536], all weights = 0 except two = 0.5
- Execute: `codec.encode(&embedding)`
- After: Verify bytes.len() == 6242, decode produces identical embedding

**Edge Case 2: Maximum Content Hash**
- Before: FusedEmbedding with content_hash = u64::MAX
- Execute: Encode, then decode
- After: `decoded.content_hash == u64::MAX` (big-endian conversion correct)

**Edge Case 3: Invalid Magic Bytes**
- Before: Valid buffer with bytes[0..4] = [0xFF, 0xFF, 0xFF, 0xFF]
- Execute: `codec.decode(&corrupted_buffer)`
- After: Returns `Err(DecodeError::InvalidMagic)`

### Evidence of Success

Each test MUST print before/after state:
```rust
#[test]
fn test_encode_decode_round_trip() {
    let embedding = make_test_embedding();
    println!("BEFORE: vector[0..3] = {:?}", &embedding.vector[0..3]);
    println!("BEFORE: content_hash = {}", embedding.content_hash);

    let bytes = codec.encode(&embedding).expect("encode failed");
    println!("ENCODED: {} bytes", bytes.len());
    println!("ENCODED: magic = {:02x?}", &bytes[0..4]);

    let decoded = codec.decode(&bytes).expect("decode failed");
    println!("AFTER: vector[0..3] = {:?}", &decoded.vector[0..3]);
    println!("AFTER: content_hash = {}", decoded.content_hash);

    assert_eq!(embedding.vector, decoded.vector);
    assert_eq!(embedding.content_hash, decoded.content_hash);
}
```

## Required Tests (storage/binary.rs)

```rust
#[cfg(test)]
mod tests {
    use super::*;
    use crate::types::FusedEmbedding;
    use crate::types::dimensions::{FUSED_OUTPUT, NUM_EXPERTS, TOP_K_EXPERTS};

    fn make_test_embedding() -> FusedEmbedding {
        FusedEmbedding::new(
            vec![0.1; FUSED_OUTPUT],
            [0.125; NUM_EXPERTS],  // sum = 1.0
            [0, 1],
            1000,
            0xDEADBEEF,
        ).expect("test embedding creation")
    }

    // ========== Header Tests ==========

    #[test]
    fn test_header_is_exactly_64_bytes() {
        assert_eq!(std::mem::size_of::<EmbeddingHeader>(), 64);
        println!("PASSED: EmbeddingHeader is exactly 64 bytes");
    }

    #[test]
    fn test_header_is_64_byte_aligned() {
        assert_eq!(std::mem::align_of::<EmbeddingHeader>(), 64);
        println!("PASSED: EmbeddingHeader has 64-byte alignment");
    }

    // ========== Encode Tests ==========

    #[test]
    fn test_encode_produces_6242_bytes_no_aux() {
        let codec = EmbeddingBinaryCodec::new();
        let embedding = make_test_embedding();

        println!("BEFORE: embedding.vector.len() = {}", embedding.vector.len());

        let bytes = codec.encode(&embedding).expect("encode should succeed");

        println!("AFTER: bytes.len() = {}", bytes.len());
        assert_eq!(bytes.len(), EmbeddingBinaryCodec::MIN_BUFFER_SIZE);
        assert_eq!(bytes.len(), 6242);
        println!("PASSED: encode produces exactly 6242 bytes (no aux_data)");
    }

    #[test]
    fn test_encode_writes_correct_magic() {
        let codec = EmbeddingBinaryCodec::new();
        let embedding = make_test_embedding();

        let bytes = codec.encode(&embedding).expect("encode");

        println!("MAGIC: {:02x} {:02x} {:02x} {:02x}", bytes[0], bytes[1], bytes[2], bytes[3]);
        assert_eq!(&bytes[0..4], &EMBEDDING_MAGIC);
        println!("PASSED: magic bytes = 'CGEB' (0x43474542)");
    }

    #[test]
    fn test_encode_fails_fast_on_wrong_dimension() {
        let codec = EmbeddingBinaryCodec::new();
        let bad_embedding = FusedEmbedding {
            vector: vec![0.0; 512],  // WRONG dimension
            expert_weights: [0.125; 8],
            selected_experts: [0, 1],
            pipeline_latency_us: 0,
            content_hash: 0,
            aux_data: None,
        };

        let result = codec.encode(&bad_embedding);

        println!("Result: {:?}", result);
        assert!(result.is_err());
        match result.unwrap_err() {
            EncodeError::InvalidDimension { expected, actual } => {
                assert_eq!(expected, 1536);
                assert_eq!(actual, 512);
            }
            e => panic!("Expected InvalidDimension, got {:?}", e),
        }
        println!("PASSED: encode fails fast on wrong dimension");
    }

    // ========== Decode Tests ==========

    #[test]
    fn test_decode_round_trip_preserves_all_fields() {
        let codec = EmbeddingBinaryCodec::new();
        let original = make_test_embedding();

        println!("BEFORE: vector[0..3] = {:?}", &original.vector[0..3]);
        println!("BEFORE: expert_weights = {:?}", original.expert_weights);
        println!("BEFORE: selected_experts = {:?}", original.selected_experts);
        println!("BEFORE: content_hash = {:#x}", original.content_hash);

        let bytes = codec.encode(&original).expect("encode");
        let decoded = codec.decode(&bytes).expect("decode");

        println!("AFTER: vector[0..3] = {:?}", &decoded.vector[0..3]);
        println!("AFTER: expert_weights = {:?}", decoded.expert_weights);
        println!("AFTER: selected_experts = {:?}", decoded.selected_experts);
        println!("AFTER: content_hash = {:#x}", decoded.content_hash);

        assert_eq!(original.vector, decoded.vector);
        assert_eq!(original.expert_weights, decoded.expert_weights);
        assert_eq!(original.selected_experts, decoded.selected_experts);
        assert_eq!(original.pipeline_latency_us, decoded.pipeline_latency_us);
        assert_eq!(original.content_hash, decoded.content_hash);
        println!("PASSED: round-trip preserves all fields");
    }

    #[test]
    fn test_decode_fails_fast_on_invalid_magic() {
        let codec = EmbeddingBinaryCodec::new();
        let mut buffer = vec![0u8; EmbeddingBinaryCodec::MIN_BUFFER_SIZE];
        buffer[0..4].copy_from_slice(&[0xFF, 0xFF, 0xFF, 0xFF]);  // Bad magic

        println!("BEFORE: buffer[0..4] = {:02x?}", &buffer[0..4]);

        let result = codec.decode(&buffer);

        println!("AFTER: result = {:?}", result);
        assert!(matches!(result, Err(DecodeError::InvalidMagic)));
        println!("PASSED: decode fails fast on invalid magic");
    }

    #[test]
    fn test_decode_fails_fast_on_truncated_buffer() {
        let codec = EmbeddingBinaryCodec::new();
        let buffer = vec![0u8; 100];  // Way too short

        println!("BEFORE: buffer.len() = {}", buffer.len());

        let result = codec.decode(&buffer);

        println!("AFTER: result = {:?}", result);
        match result {
            Err(DecodeError::BufferTooShort { needed, available }) => {
                assert_eq!(needed, 6242);
                assert_eq!(available, 100);
            }
            _ => panic!("Expected BufferTooShort"),
        }
        println!("PASSED: decode fails fast on truncated buffer");
    }

    #[test]
    fn test_decode_fails_fast_on_unsupported_version() {
        let codec = EmbeddingBinaryCodec::new();
        let embedding = make_test_embedding();
        let mut bytes = codec.encode(&embedding).expect("encode");

        // Corrupt version to 99 (big-endian)
        bytes[4] = 0x00;
        bytes[5] = 0x63;  // 99 in big-endian

        println!("BEFORE: version bytes = {:02x?}", &bytes[4..6]);

        let result = codec.decode(&bytes);

        println!("AFTER: result = {:?}", result);
        assert!(matches!(result, Err(DecodeError::UnsupportedVersion(99))));
        println!("PASSED: decode fails fast on unsupported version");
    }

    // ========== Big-Endian Tests ==========

    #[test]
    fn test_encode_uses_big_endian_floats() {
        let codec = EmbeddingBinaryCodec::new();
        let mut embedding = make_test_embedding();
        embedding.vector[0] = 1.0f32;  // Known value

        let bytes = codec.encode(&embedding).expect("encode");

        // 1.0f32 in big-endian = 0x3F800000
        let expected_be = 1.0f32.to_be_bytes();
        println!("Expected BE bytes for 1.0: {:02x?}", expected_be);
        println!("Actual bytes at offset 64: {:02x?}", &bytes[64..68]);

        assert_eq!(&bytes[64..68], &expected_be);
        println!("PASSED: encode uses big-endian for floats");
    }

    #[test]
    fn test_decode_converts_big_endian_correctly() {
        let codec = EmbeddingBinaryCodec::new();
        let mut embedding = make_test_embedding();
        embedding.vector[0] = std::f32::consts::PI;

        println!("BEFORE: original.vector[0] = {}", embedding.vector[0]);

        let bytes = codec.encode(&embedding).expect("encode");
        let decoded = codec.decode(&bytes).expect("decode");

        println!("AFTER: decoded.vector[0] = {}", decoded.vector[0]);
        assert!((embedding.vector[0] - decoded.vector[0]).abs() < 1e-7);
        println!("PASSED: decode converts big-endian correctly (PI preserved)");
    }

    // ========== Edge Case Tests ==========

    #[test]
    fn test_edge_case_max_content_hash() {
        let codec = EmbeddingBinaryCodec::new();
        let mut embedding = make_test_embedding();
        embedding.content_hash = u64::MAX;

        println!("BEFORE: content_hash = {:#x}", embedding.content_hash);

        let bytes = codec.encode(&embedding).expect("encode");
        let decoded = codec.decode(&bytes).expect("decode");

        println!("AFTER: content_hash = {:#x}", decoded.content_hash);
        assert_eq!(decoded.content_hash, u64::MAX);
        println!("Edge Case PASSED: u64::MAX content_hash preserved");
    }

    #[test]
    fn test_edge_case_zero_vector() {
        let codec = EmbeddingBinaryCodec::new();
        let mut embedding = make_test_embedding();
        for v in &mut embedding.vector {
            *v = 0.0;
        }

        println!("BEFORE: all vector elements = 0.0");

        let bytes = codec.encode(&embedding).expect("encode");
        let decoded = codec.decode(&bytes).expect("decode");

        println!("AFTER: decoded.vector[0..5] = {:?}", &decoded.vector[0..5]);
        assert!(decoded.vector.iter().all(|&v| v == 0.0));
        println!("Edge Case PASSED: zero vector preserved");
    }

    #[test]
    fn test_edge_case_negative_floats() {
        let codec = EmbeddingBinaryCodec::new();
        let mut embedding = make_test_embedding();
        embedding.vector[0] = -1.5;
        embedding.vector[1] = f32::MIN;

        println!("BEFORE: vector[0] = {}, vector[1] = {}", embedding.vector[0], embedding.vector[1]);

        let bytes = codec.encode(&embedding).expect("encode");
        let decoded = codec.decode(&bytes).expect("decode");

        println!("AFTER: vector[0] = {}, vector[1] = {}", decoded.vector[0], decoded.vector[1]);
        assert_eq!(decoded.vector[0], -1.5);
        assert_eq!(decoded.vector[1], f32::MIN);
        println!("Edge Case PASSED: negative floats preserved");
    }
}
```

## Manual Verification Checklist

After implementation, manually verify:

- [ ] `cargo check -p context-graph-embeddings` passes with no warnings
- [ ] `cargo test -p context-graph-embeddings storage` passes all tests
- [ ] `EmbeddingHeader` size is 64 bytes (check compile output)
- [ ] Encoded bytes start with `0x43 0x47 0x45 0x42` ("CGEB")
- [ ] Test output shows before/after state for each test
- [ ] Error messages include context (expected vs actual values)

## Sherlock-Holmes Verification

After completing implementation, use the `sherlock-holmes` subagent to perform forensic verification:

```
Task: Verify M03-F17 storage module implementation

Investigation checklist:
1. Verify EmbeddingHeader is exactly 64 bytes (check binary output of std::mem::size_of)
2. Verify MIN_BUFFER_SIZE = 6242 bytes (64 + 6144 + 32 + 2)
3. Verify magic bytes in encoded output match "CGEB"
4. Verify round-trip preserves ALL fields including content_hash
5. Verify big-endian encoding by inspecting raw bytes
6. Verify error handling for: invalid magic, truncated buffer, unsupported version
7. Run `cargo test storage` and verify all tests pass with PASSED messages
8. Check that NO heap allocation occurs in decode_zero_copy
```

---

## Completion Verification (2026-01-01)

### Sherlock-Holmes Forensic Audit Results

**CASE STATUS: VERIFIED COMPLETE**

#### Files Implemented
| File | Status | Path |
|------|--------|------|
| mod.rs | COMPLETE | `crates/context-graph-embeddings/src/storage/mod.rs` |
| binary.rs | COMPLETE | `crates/context-graph-embeddings/src/storage/binary.rs` |
| batch.rs | COMPLETE | `crates/context-graph-embeddings/src/storage/batch.rs` |
| gds.rs | COMPLETE | `crates/context-graph-embeddings/src/storage/gds.rs` |

#### Build Verification
```bash
$ cargo check -p context-graph-embeddings
# Result: PASS - Zero warnings, zero errors
```

#### Test Verification
```bash
$ cargo test -p context-graph-embeddings storage
# Result: 45/45 tests PASS
```

#### Binary Format Verification
| Requirement | Expected | Actual | Status |
|-------------|----------|--------|--------|
| EmbeddingHeader size | 64 bytes | 64 bytes | PASS |
| MIN_BUFFER_SIZE | 6242 bytes | 6242 bytes | PASS |
| Magic bytes | "CGEB" (0x43474542) | [43, 47, 45, 42] | PASS |
| Float encoding | Big-endian | Big-endian | PASS |
| Page alignment | 4KB | 4096 bytes | PASS |

#### Edge Cases Tested
- [x] Invalid magic bytes → `DecodeError::InvalidMagic`
- [x] Truncated buffer → `DecodeError::BufferTooShort`
- [x] Unsupported version → `DecodeError::UnsupportedVersion(99)`
- [x] Max content_hash (u64::MAX) → Preserved correctly
- [x] Zero vector → All zeros preserved
- [x] Special floats (INF, NEG_INF, ±0) → All preserved
- [x] Negative floats → Preserved correctly
- [x] Round-trip encode/decode → All fields match

#### Re-exports Verified
All specified types re-exported in `storage/mod.rs`:
- `EmbeddingBinaryCodec`, `EmbeddingHeader`, `FusedEmbeddingRef`
- `CompressionType`, `EncodeError`, `DecodeError`
- `BatchBinaryEncoder`, `EmbeddingIndexHeader`
- `GdsFile`, `GdsFileError`, `GdsFileIter`
- `EMBEDDING_BINARY_VERSION`, `EMBEDDING_MAGIC`, `INDEX_MAGIC`, `INDEX_VERSION`

#### Dependencies Verified
```toml
# Cargo.toml line 38
bytemuck = { version = "1.14", features = ["derive"] }
```

---
*Task ID: M03-F17*
*Layer: Foundation*
*Module: 03 - 12-Model Embedding Pipeline*
*Version: 2.1 - COMPLETED with forensic verification*
