# Task Specification: M03-S21

```xml
<task_spec id="M03-S21" version="1.0">
<metadata>
  <title>Memory-Mapped Weights (Zero-Copy Fallback)</title>
  <status>ready</status>
  <layer>surface</layer>
  <sequence>21</sequence>
  <implements>
    - PRD Section 9.3: Model weight management
    - Constitution: efficiency.memory_optimization
    - Technical Engine: GDS fallback mechanism
  </implements>
  <depends_on>
    - M03-S13 (Model Artifact Manager)
    - M03-S15 (GDS Integration)
    - M03-F01 (ModelId)
  </depends_on>
  <estimated_hours>4</estimated_hours>
</metadata>

<context>
GPU Direct Storage (GDS - M03-S15) is the PRIMARY path for loading model weights,
providing 25+ GB/s NVMe→GPU transfer. However, GDS requires:
- Linux only (no Windows/macOS)
- cuFile driver installed
- Compatible NVMe SSD
- Blackwell GPU (for optimal performance)

WITHOUT a robust fallback:
- Systems without GDS crash on model loading
- Using `std::fs::read()` loads entire weights into RAM first
- 12 models × ~1GB each = 12GB RAM spike during loading
- OOM on memory-constrained systems

SOLUTION: Memory-mapped file I/O (mmap) provides:
1. Zero-copy access - weights loaded on-demand via page faults
2. OS-managed paging - unused weights don't consume physical RAM
3. Shared mapping - multiple model instances share same physical pages
4. Lazy loading - only pages actually accessed are loaded from disk

This is the fallback path when GDS is unavailable, ensuring the pipeline
works on ALL systems while maintaining efficient memory usage.
</context>

<definition_of_done>
  <signatures>
```rust
use std::path::{Path, PathBuf};
use std::sync::Arc;

/// Memory-mapped file handle for model weights
#[derive(Debug)]
pub struct MappedWeights {
    /// File path for debugging/logging
    path: PathBuf,
    /// Memory-mapped region
    mmap: memmap2::Mmap,
    /// File size in bytes
    size: usize,
    /// Model identifier
    model_id: ModelId,
    /// Lock status (prevent unmapping while in use)
    locked: std::sync::atomic::AtomicBool,
}

impl MappedWeights {
    /// Map a safetensors file
    pub fn open(path: impl AsRef<Path>, model_id: ModelId) -> EmbeddingResult<Self>;

    /// Map with specific options (read-ahead, populate, etc.)
    pub fn open_with_options(
        path: impl AsRef<Path>,
        model_id: ModelId,
        options: MmapOptions,
    ) -> EmbeddingResult<Self>;

    /// Get byte slice of entire mapped region
    pub fn as_bytes(&self) -> &[u8];

    /// Get byte slice for specific offset and length
    pub fn slice(&self, offset: usize, len: usize) -> EmbeddingResult<&[u8]>;

    /// Advise the OS about access pattern
    pub fn advise(&self, advice: MmapAdvice) -> EmbeddingResult<()>;

    /// Pre-fault pages (populate memory before use)
    pub fn populate(&self) -> EmbeddingResult<()>;

    /// Lock pages in memory (prevent swapping)
    pub fn lock_in_memory(&self) -> EmbeddingResult<()>;

    /// Unlock pages (allow swapping)
    pub fn unlock(&self) -> EmbeddingResult<()>;

    /// Get file size
    pub fn size(&self) -> usize;

    /// Get model ID
    pub fn model_id(&self) -> ModelId;

    /// Check if currently locked
    pub fn is_locked(&self) -> bool;
}

/// Options for memory mapping
#[derive(Debug, Clone, Default)]
pub struct MmapOptions {
    /// Read-ahead hint (pages)
    pub read_ahead: Option<usize>,
    /// Pre-populate pages on map
    pub populate: bool,
    /// Lock pages in memory
    pub lock: bool,
    /// Huge pages (2MB instead of 4KB)
    pub huge_pages: bool,
}

/// Access pattern hints for the OS
#[derive(Debug, Clone, Copy)]
pub enum MmapAdvice {
    /// Normal access pattern
    Normal,
    /// Sequential access (enables read-ahead)
    Sequential,
    /// Random access
    Random,
    /// Will need this data soon
    WillNeed,
    /// Won't need this data for a while
    DontNeed,
}

/// Unified weight source (GDS or mmap)
#[derive(Debug)]
pub enum WeightSource {
    /// GPU Direct Storage (primary path)
    Gds(GdsHandle),
    /// Memory-mapped file (fallback)
    Mmap(Arc<MappedWeights>),
    /// In-memory buffer (for small models or testing)
    Buffer(Vec<u8>),
}

impl WeightSource {
    /// Open weights with automatic method selection
    pub async fn open(
        path: impl AsRef<Path>,
        model_id: ModelId,
        prefer_gds: bool,
    ) -> EmbeddingResult<Self>;

    /// Get bytes for weight tensor at offset
    pub fn get_tensor_bytes(&self, offset: usize, len: usize) -> EmbeddingResult<&[u8]>;

    /// Prepare for tensor load (pre-fetch for mmap)
    pub async fn prepare_tensor(&self, offset: usize, len: usize) -> EmbeddingResult<()>;

    /// Check if GDS is being used
    pub fn is_gds(&self) -> bool;

    /// Get source type for logging
    pub fn source_type(&self) -> &'static str;
}

/// Cache for mapped weight files
pub struct WeightMmapCache {
    /// Cached mappings by path
    cache: dashmap::DashMap<PathBuf, Arc<MappedWeights>>,
    /// Maximum cached size in bytes
    max_size: usize,
    /// Current cached size
    current_size: std::sync::atomic::AtomicUsize,
}

impl WeightMmapCache {
    /// Create cache with size limit
    pub fn new(max_size_bytes: usize) -> Self;

    /// Get or create mapping for file
    pub fn get_or_open(
        &self,
        path: impl AsRef<Path>,
        model_id: ModelId,
    ) -> EmbeddingResult<Arc<MappedWeights>>;

    /// Evict least-recently-used mappings
    pub fn evict_lru(&self, target_free: usize);

    /// Clear all mappings (for shutdown)
    pub fn clear(&self);

    /// Get cache statistics
    pub fn stats(&self) -> MmapCacheStats;
}

#[derive(Debug, Clone, Default)]
pub struct MmapCacheStats {
    pub entries: usize,
    pub total_size: usize,
    pub hits: u64,
    pub misses: u64,
    pub evictions: u64,
}

/// Integration with safetensors
pub trait SafetensorsMmap {
    /// Load safetensors from memory-mapped file
    fn from_mmap(mmap: &MappedWeights) -> EmbeddingResult<safetensors::SafeTensors>;
}
```
  </signatures>

  <constraints>
    - Mapping MUST be read-only (prevent accidental modification)
    - Handle files >4GB correctly on 64-bit systems
    - Graceful handling of mapping failures (disk full, permissions)
    - Thread-safe: multiple threads can read same mapping
    - No memory leaks on drop
    - Pre-population optional (for critical models)
    - Huge pages optional (when available)
    - Cache respects max size limit
    - Compatible with safetensors file format
    - Works on Linux, macOS, Windows
  </constraints>

  <verification>
    <step>MappedWeights::open() successfully maps .safetensors file</step>
    <step>as_bytes() returns valid slice of file contents</step>
    <step>slice() with valid offset/len returns correct bytes</step>
    <step>slice() with invalid offset returns error (not panic)</step>
    <step>Multiple threads can read same MappedWeights concurrently</step>
    <step>Drop unmaps memory correctly (no leaks)</step>
    <step>WeightSource::open() uses GDS when available, mmap otherwise</step>
    <step>Cache evicts old entries when max size exceeded</step>
    <step>safetensors can load tensors from mmap source</step>
    <step>Memory usage stays bounded during model loading</step>
  </verification>
</definition_of_done>

<files_to_create>
  <file>crates/context-graph-embeddings/src/weights/mmap.rs</file>
  <file>crates/context-graph-embeddings/src/weights/source.rs</file>
  <file>crates/context-graph-embeddings/src/weights/cache.rs</file>
  <file>crates/context-graph-embeddings/src/weights/mod.rs</file>
</files_to_create>

<validation_criteria>
  <criterion>cargo check passes</criterion>
  <criterion>cargo test weights::mmap passes</criterion>
  <criterion>Model loads successfully with GDS disabled</criterion>
  <criterion>RAM usage stays bounded during loading</criterion>
  <criterion>Cache correctly evicts old entries</criterion>
  <criterion>Works on Linux, macOS, Windows</criterion>
</validation_criteria>
</task_spec>
```

## Implementation Notes

### Basic Memory Mapping

```rust
use memmap2::{Mmap, MmapOptions as Mm2Options};
use std::fs::File;

impl MappedWeights {
    pub fn open(path: impl AsRef<Path>, model_id: ModelId) -> EmbeddingResult<Self> {
        Self::open_with_options(path, model_id, MmapOptions::default())
    }

    pub fn open_with_options(
        path: impl AsRef<Path>,
        model_id: ModelId,
        options: MmapOptions,
    ) -> EmbeddingResult<Self> {
        let path = path.as_ref().to_path_buf();
        let file = File::open(&path).map_err(|e| EmbeddingError::WeightLoadError {
            model: model_id,
            source: format!("Failed to open {}: {}", path.display(), e),
        })?;

        let metadata = file.metadata()?;
        let size = metadata.len() as usize;

        // Build mmap with options
        let mut builder = Mm2Options::new();

        if options.populate {
            builder.populate();
        }

        #[cfg(target_os = "linux")]
        if options.huge_pages {
            // Use huge pages hint
            builder.huge(Some(2 * 1024 * 1024)); // 2MB pages
        }

        // Create read-only mapping
        let mmap = unsafe { builder.map(&file) }.map_err(|e| {
            EmbeddingError::WeightLoadError {
                model: model_id,
                source: format!("Failed to mmap {}: {}", path.display(), e),
            }
        })?;

        let mapped = Self {
            path,
            mmap,
            size,
            model_id,
            locked: std::sync::atomic::AtomicBool::new(false),
        };

        // Apply read-ahead if specified
        if let Some(pages) = options.read_ahead {
            mapped.advise_readahead(pages)?;
        }

        // Lock if requested
        if options.lock {
            mapped.lock_in_memory()?;
        }

        Ok(mapped)
    }

    pub fn as_bytes(&self) -> &[u8] {
        &self.mmap[..]
    }

    pub fn slice(&self, offset: usize, len: usize) -> EmbeddingResult<&[u8]> {
        if offset + len > self.size {
            return Err(EmbeddingError::InvalidWeightOffset {
                model: self.model_id,
                offset,
                len,
                file_size: self.size,
            });
        }
        Ok(&self.mmap[offset..offset + len])
    }
}
```

### Access Pattern Hints

```rust
impl MappedWeights {
    pub fn advise(&self, advice: MmapAdvice) -> EmbeddingResult<()> {
        #[cfg(unix)]
        {
            use std::os::unix::io::AsRawFd;
            let advice_flag = match advice {
                MmapAdvice::Normal => libc::MADV_NORMAL,
                MmapAdvice::Sequential => libc::MADV_SEQUENTIAL,
                MmapAdvice::Random => libc::MADV_RANDOM,
                MmapAdvice::WillNeed => libc::MADV_WILLNEED,
                MmapAdvice::DontNeed => libc::MADV_DONTNEED,
            };

            let result = unsafe {
                libc::madvise(
                    self.mmap.as_ptr() as *mut libc::c_void,
                    self.size,
                    advice_flag,
                )
            };

            if result != 0 {
                return Err(EmbeddingError::MmapAdviseError {
                    advice: format!("{:?}", advice),
                    errno: std::io::Error::last_os_error(),
                });
            }
        }

        Ok(())
    }

    pub fn populate(&self) -> EmbeddingResult<()> {
        // Force all pages to be loaded by touching them
        let page_size = 4096;
        let mut sum: u8 = 0;

        for offset in (0..self.size).step_by(page_size) {
            sum = sum.wrapping_add(self.mmap[offset]);
        }

        // Prevent optimization from removing the loop
        std::hint::black_box(sum);

        Ok(())
    }

    pub fn lock_in_memory(&self) -> EmbeddingResult<()> {
        #[cfg(unix)]
        {
            let result = unsafe {
                libc::mlock(self.mmap.as_ptr() as *const libc::c_void, self.size)
            };

            if result != 0 {
                return Err(EmbeddingError::MlockError {
                    errno: std::io::Error::last_os_error(),
                });
            }

            self.locked.store(true, std::sync::atomic::Ordering::SeqCst);
        }

        Ok(())
    }
}
```

### Unified Weight Source

```rust
impl WeightSource {
    pub async fn open(
        path: impl AsRef<Path>,
        model_id: ModelId,
        prefer_gds: bool,
    ) -> EmbeddingResult<Self> {
        let path = path.as_ref();

        // Try GDS first if preferred and available
        if prefer_gds && GdsHandle::is_available() {
            match GdsHandle::open(path, model_id).await {
                Ok(handle) => {
                    tracing::debug!(
                        "Using GDS for model {:?}: {}",
                        model_id,
                        path.display()
                    );
                    return Ok(WeightSource::Gds(handle));
                }
                Err(e) => {
                    tracing::warn!(
                        "GDS unavailable for {:?}, falling back to mmap: {}",
                        model_id,
                        e
                    );
                }
            }
        }

        // Fall back to mmap
        let mmap = MappedWeights::open_with_options(
            path,
            model_id,
            MmapOptions {
                read_ahead: Some(256), // 1MB read-ahead for sequential tensor access
                populate: false,        // Lazy loading
                lock: false,            // Allow paging
                huge_pages: true,       // Try huge pages
            },
        )?;

        tracing::debug!(
            "Using mmap for model {:?}: {} ({} MB)",
            model_id,
            path.display(),
            mmap.size() / (1024 * 1024)
        );

        Ok(WeightSource::Mmap(Arc::new(mmap)))
    }

    pub fn get_tensor_bytes(&self, offset: usize, len: usize) -> EmbeddingResult<&[u8]> {
        match self {
            WeightSource::Gds(handle) => handle.get_bytes(offset, len),
            WeightSource::Mmap(mmap) => mmap.slice(offset, len),
            WeightSource::Buffer(buf) => {
                if offset + len > buf.len() {
                    return Err(EmbeddingError::InvalidWeightOffset {
                        model: ModelId::Semantic, // placeholder
                        offset,
                        len,
                        file_size: buf.len(),
                    });
                }
                Ok(&buf[offset..offset + len])
            }
        }
    }

    pub async fn prepare_tensor(&self, offset: usize, len: usize) -> EmbeddingResult<()> {
        match self {
            WeightSource::Gds(handle) => {
                // GDS pre-fetches automatically
                Ok(())
            }
            WeightSource::Mmap(mmap) => {
                // Advise OS to load these pages
                #[cfg(unix)]
                {
                    let ptr = mmap.as_bytes().as_ptr().wrapping_add(offset);
                    unsafe {
                        libc::madvise(
                            ptr as *mut libc::c_void,
                            len,
                            libc::MADV_WILLNEED,
                        );
                    }
                }
                Ok(())
            }
            WeightSource::Buffer(_) => Ok(()), // Already in memory
        }
    }

    pub fn is_gds(&self) -> bool {
        matches!(self, WeightSource::Gds(_))
    }

    pub fn source_type(&self) -> &'static str {
        match self {
            WeightSource::Gds(_) => "GDS",
            WeightSource::Mmap(_) => "mmap",
            WeightSource::Buffer(_) => "buffer",
        }
    }
}
```

### Safetensors Integration

```rust
use safetensors::SafeTensors;

impl SafetensorsMmap for SafeTensors<'_> {
    fn from_mmap(mmap: &MappedWeights) -> EmbeddingResult<SafeTensors> {
        SafeTensors::deserialize(mmap.as_bytes()).map_err(|e| {
            EmbeddingError::SafetensorsError {
                model: mmap.model_id(),
                source: e.to_string(),
            }
        })
    }
}

// Usage in model loading
impl ModelFactory {
    async fn load_model_weights(
        &self,
        model_id: ModelId,
        path: &Path,
    ) -> EmbeddingResult<LoadedWeights> {
        // Open with automatic GDS/mmap selection
        let source = WeightSource::open(path, model_id, self.prefer_gds).await?;

        tracing::info!(
            "Loading {:?} weights via {} ({:.2} GB)",
            model_id,
            source.source_type(),
            source.size() as f64 / (1024.0 * 1024.0 * 1024.0)
        );

        // Parse safetensors header
        let tensors = match &source {
            WeightSource::Mmap(mmap) => SafeTensors::from_mmap(mmap)?,
            _ => SafeTensors::deserialize(source.get_all_bytes()?)?,
        };

        Ok(LoadedWeights { source, tensors })
    }
}
```

### Cache Implementation

```rust
use dashmap::DashMap;
use std::sync::atomic::{AtomicUsize, AtomicU64, Ordering};

impl WeightMmapCache {
    pub fn new(max_size_bytes: usize) -> Self {
        Self {
            cache: DashMap::new(),
            max_size: max_size_bytes,
            current_size: AtomicUsize::new(0),
            hits: AtomicU64::new(0),
            misses: AtomicU64::new(0),
        }
    }

    pub fn get_or_open(
        &self,
        path: impl AsRef<Path>,
        model_id: ModelId,
    ) -> EmbeddingResult<Arc<MappedWeights>> {
        let path = path.as_ref().to_path_buf();

        // Check cache first
        if let Some(entry) = self.cache.get(&path) {
            self.hits.fetch_add(1, Ordering::Relaxed);
            return Ok(Arc::clone(entry.value()));
        }

        self.misses.fetch_add(1, Ordering::Relaxed);

        // Open new mapping
        let mmap = Arc::new(MappedWeights::open(&path, model_id)?);
        let size = mmap.size();

        // Evict if needed
        let current = self.current_size.load(Ordering::Relaxed);
        if current + size > self.max_size {
            self.evict_lru(size);
        }

        // Insert into cache
        self.cache.insert(path, Arc::clone(&mmap));
        self.current_size.fetch_add(size, Ordering::Relaxed);

        Ok(mmap)
    }

    pub fn evict_lru(&self, target_free: usize) {
        // Simple eviction: remove oldest entries until we have space
        let mut freed = 0;
        let mut to_remove = Vec::new();

        for entry in self.cache.iter() {
            if freed >= target_free {
                break;
            }
            // Don't evict locked entries
            if !entry.value().is_locked() {
                freed += entry.value().size();
                to_remove.push(entry.key().clone());
            }
        }

        for key in to_remove {
            if let Some((_, mmap)) = self.cache.remove(&key) {
                self.current_size.fetch_sub(mmap.size(), Ordering::Relaxed);
                self.evictions.fetch_add(1, Ordering::Relaxed);
            }
        }
    }
}
```

---
*Task ID: M03-S21*
*Layer: Surface*
*Module: 03 - 12-Model Embedding Pipeline*
