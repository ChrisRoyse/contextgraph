# Task Specification: M03-L35

```xml
<task_spec id="M03-L35" version="1.0">
<metadata>
  <title>Token-to-Vector Attribution Map (for ColBERT)</title>
  <status>ready</status>
  <layer>logic</layer>
  <sequence>35</sequence>
  <implements>
    - PRD Section 7.3: E12 ColBERT late-interaction storage
    - Constitution: data.provenance_tracking
    - Technical Engine: Module 4 graph node integration
  </implements>
  <depends_on>
    - M03-F05 (FusedEmbedding with aux_data)
    - M03-L14 (ColBERT E12 implementation)
    - M03-L29 (TokenizationManager)
  </depends_on>
  <estimated_hours>3</estimated_hours>
</metadata>

<context>
ColBERT (E12) produces "Late-Interaction" embeddings: one 128D vector PER TOKEN,
not a single pooled vector. This is stored in `FusedEmbedding.aux_data`.

Module 4 (Knowledge Graph) needs to store graph nodes with:
1. The token string (e.g., "machine", "learning")
2. Its specific 128D ColBERT vector
3. The position in the original text
4. Relationship to the parent FusedEmbedding

WITHOUT this mapping:
- We have 128D vectors in `aux_data` but no way to know which token they represent
- Module 4 cannot create meaningful graph node labels
- Late-interaction retrieval cannot highlight which tokens matched

This task creates a bidirectional mapping between:
- Token strings (from TokenizationManager)
- Token vectors (from ColBERT output)
- Token positions (for context reconstruction)

This is a critical "glue" task for Module 4 Knowledge Graph integration.
</context>

<definition_of_done>
  <signatures>
```rust
use std::sync::Arc;

/// A single token with its vector embedding and metadata
#[derive(Debug, Clone)]
pub struct TokenVector {
    /// The actual token string (e.g., "learning", "##ing" for subwords)
    pub token: String,
    /// Token ID from the tokenizer vocabulary
    pub token_id: u32,
    /// The 128D ColBERT vector for this token
    pub vector: Vec<f32>,
    /// Position in the original token sequence (0-indexed)
    pub position: usize,
    /// Character span in original text (start, end)
    pub char_span: Option<(usize, usize)>,
    /// Is this a special token ([CLS], [SEP], [PAD])?
    pub is_special: bool,
}

/// Complete token-vector attribution for a text
#[derive(Debug, Clone)]
pub struct TokenVectorAttribution {
    /// Original input text
    pub original_text: String,
    /// Ordered list of token-vector pairs
    pub tokens: Vec<TokenVector>,
    /// Model that produced the vectors
    pub model_id: ModelId,
    /// Dimension of each vector
    pub vector_dim: usize,
}

impl TokenVectorAttribution {
    /// Create from ColBERT output and tokenization
    pub fn from_colbert_output(
        text: &str,
        tokenization: &TokenizationResult,
        colbert_output: &ColbertOutput,
    ) -> EmbeddingResult<Self>;

    /// Get token at index with its vector
    pub fn get_token(&self, index: usize) -> Option<&TokenVector>;

    /// Get token by position in original text (character offset)
    pub fn get_token_at_position(&self, char_offset: usize) -> Option<&TokenVector>;

    /// Find tokens matching a substring (returns all matches)
    pub fn find_tokens(&self, substring: &str) -> Vec<&TokenVector>;

    /// Get all non-special tokens
    pub fn content_tokens(&self) -> impl Iterator<Item = &TokenVector>;

    /// Get vectors only (for batch operations)
    pub fn vectors(&self) -> Vec<&[f32]>;

    /// Get token strings only
    pub fn token_strings(&self) -> Vec<&str>;

    /// Number of tokens (including special tokens)
    pub fn len(&self) -> usize;

    /// Number of content tokens (excluding special tokens)
    pub fn content_len(&self) -> usize;

    /// Reconstruct text from tokens (for verification)
    pub fn reconstruct_text(&self) -> String;

    /// Serialize for storage in Module 4
    pub fn to_graph_nodes(&self) -> Vec<GraphNodeData>;
}

/// Graph node data for Module 4 integration
#[derive(Debug, Clone)]
pub struct GraphNodeData {
    /// Node label (the token string)
    pub label: String,
    /// 128D embedding vector
    pub embedding: Vec<f32>,
    /// Position in source text
    pub position: usize,
    /// Character span for highlighting
    pub char_span: Option<(usize, usize)>,
    /// Reference to parent FusedEmbedding
    pub parent_embedding_id: Option<u64>,
}

/// Builder for creating TokenVectorAttribution
pub struct TokenVectorAttributionBuilder {
    text: String,
    model_id: ModelId,
    tokens: Vec<TokenVector>,
}

impl TokenVectorAttributionBuilder {
    pub fn new(text: String, model_id: ModelId) -> Self;

    /// Add a token-vector pair
    pub fn add_token(
        &mut self,
        token: String,
        token_id: u32,
        vector: Vec<f32>,
        position: usize,
        char_span: Option<(usize, usize)>,
        is_special: bool,
    ) -> &mut Self;

    /// Add from tokenizer encoding and vector batch
    pub fn add_from_encoding(
        &mut self,
        encoding: &tokenizers::Encoding,
        vectors: &[Vec<f32>],
    ) -> EmbeddingResult<&mut Self>;

    /// Build the attribution
    pub fn build(self) -> TokenVectorAttribution;
}

/// Extension for FusedEmbedding to access token attribution
impl FusedEmbedding {
    /// Get token-vector attribution from aux_data (if ColBERT was used)
    pub fn token_attribution(&self) -> Option<&TokenVectorAttribution>;

    /// Get a specific token with its vector
    pub fn get_token_with_vector(&self, index: usize) -> Option<&TokenVector>;
}

/// Integration with TokenizationManager
impl TokenizationManager {
    /// Get tokenization result with character offsets
    pub fn tokenize_with_offsets(
        &self,
        text: &str,
        model_id: ModelId,
    ) -> EmbeddingResult<TokenizationWithOffsets>;
}

/// Tokenization result with character offset mapping
#[derive(Debug, Clone)]
pub struct TokenizationWithOffsets {
    pub token_ids: Vec<u32>,
    pub tokens: Vec<String>,
    pub offsets: Vec<(usize, usize)>,  // Character spans
    pub special_tokens_mask: Vec<bool>,
}
```
  </signatures>

  <constraints>
    - Token-vector alignment MUST be exact (same count, same order)
    - Character offsets MUST be accurate for text highlighting
    - Special tokens ([CLS], [SEP], [PAD]) clearly marked
    - Support for subword tokenization (##prefixes, etc.)
    - Serialization format compatible with Module 4 graph storage
    - Memory efficient: share vector data with aux_data where possible
    - Thread-safe for concurrent access
    - Round-trip: reconstruct_text() â‰ˆ original_text (modulo whitespace)
  </constraints>

  <verification>
    <step>TokenVectorAttribution.len() == colbert_output.vectors.len()</step>
    <step>Each token.vector matches corresponding ColBERT output vector</step>
    <step>Character offsets enable correct text highlighting</step>
    <step>get_token_with_vector(0) returns [CLS] token for BERT-style models</step>
    <step>find_tokens("learning") returns correct TokenVector entries</step>
    <step>content_tokens() excludes [CLS], [SEP], [PAD]</step>
    <step>to_graph_nodes() produces valid GraphNodeData for each token</step>
    <step>Subword tokens (##ing) correctly mapped to char spans</step>
  </verification>
</definition_of_done>

<files_to_create>
  <file>crates/context-graph-embeddings/src/models/colbert/attribution.rs</file>
  <file>crates/context-graph-embeddings/src/models/colbert/graph_node.rs</file>
</files_to_create>

<validation_criteria>
  <criterion>cargo check passes</criterion>
  <criterion>cargo test models::colbert::attribution passes</criterion>
  <criterion>Token count matches ColBERT vector count</criterion>
  <criterion>Character offsets enable correct highlighting</criterion>
  <criterion>Module 4 can consume GraphNodeData correctly</criterion>
  <criterion>Round-trip text reconstruction is accurate</criterion>
</validation_criteria>
</task_spec>
```

## Implementation Notes

### Creating Attribution from ColBERT Output

```rust
impl TokenVectorAttribution {
    pub fn from_colbert_output(
        text: &str,
        tokenization: &TokenizationWithOffsets,
        colbert_output: &ColbertOutput,
    ) -> EmbeddingResult<Self> {
        // Verify alignment
        if tokenization.token_ids.len() != colbert_output.vectors.len() {
            return Err(EmbeddingError::TokenVectorMismatch {
                token_count: tokenization.token_ids.len(),
                vector_count: colbert_output.vectors.len(),
            });
        }

        let tokens: Vec<TokenVector> = tokenization
            .token_ids
            .iter()
            .enumerate()
            .zip(&tokenization.tokens)
            .zip(&tokenization.offsets)
            .zip(&tokenization.special_tokens_mask)
            .zip(&colbert_output.vectors)
            .map(|((((position, &token_id), token), &offset), &is_special), vector)| {
                TokenVector {
                    token: token.clone(),
                    token_id,
                    vector: vector.clone(),
                    position,
                    char_span: Some(offset),
                    is_special,
                }
            })
            .collect();

        Ok(Self {
            original_text: text.to_string(),
            tokens,
            model_id: ModelId::LateInteraction,
            vector_dim: 128,
        })
    }
}
```

### Text Highlighting with Character Spans

```rust
impl TokenVectorAttribution {
    /// Highlight tokens in original text based on similarity scores
    pub fn highlight_tokens(
        &self,
        scores: &[f32],
        threshold: f32,
    ) -> Vec<(usize, usize, f32)> {
        self.tokens
            .iter()
            .zip(scores)
            .filter(|(token, &score)| !token.is_special && score >= threshold)
            .filter_map(|(token, &score)| {
                token.char_span.map(|(start, end)| (start, end, score))
            })
            .collect()
    }

    /// Get token at character position for click-to-explain
    pub fn get_token_at_position(&self, char_offset: usize) -> Option<&TokenVector> {
        self.tokens.iter().find(|token| {
            if let Some((start, end)) = token.char_span {
                char_offset >= start && char_offset < end
            } else {
                false
            }
        })
    }
}
```

### Integration with FusedEmbedding

```rust
/// Extended AuxiliaryEmbeddingData to include attribution
#[derive(Debug, Clone)]
pub enum AuxiliaryEmbeddingData {
    /// ColBERT per-token embeddings with attribution
    ColbertTokens {
        vectors: Vec<Vec<f32>>,
        attribution: Option<TokenVectorAttribution>,
    },
    /// SPLADE sparse representation
    SpladeActivations {
        indices: Vec<u32>,
        values: Vec<f32>,
    },
    // ... other variants
}

impl FusedEmbedding {
    pub fn get_token_with_vector(&self, index: usize) -> Option<&TokenVector> {
        match &self.aux_data {
            Some(AuxiliaryEmbeddingData::ColbertTokens { attribution, .. }) => {
                attribution.as_ref()?.get_token(index)
            }
            _ => None,
        }
    }

    pub fn token_attribution(&self) -> Option<&TokenVectorAttribution> {
        match &self.aux_data {
            Some(AuxiliaryEmbeddingData::ColbertTokens { attribution, .. }) => {
                attribution.as_ref()
            }
            _ => None,
        }
    }
}
```

### Module 4 Integration: Graph Node Export

```rust
impl TokenVectorAttribution {
    pub fn to_graph_nodes(&self, parent_id: Option<u64>) -> Vec<GraphNodeData> {
        self.content_tokens()
            .map(|token| GraphNodeData {
                label: token.token.clone(),
                embedding: token.vector.clone(),
                position: token.position,
                char_span: token.char_span,
                parent_embedding_id: parent_id,
            })
            .collect()
    }
}

// Example usage in Module 4
async fn store_embedding_with_tokens(
    graph: &mut KnowledgeGraph,
    fused: &FusedEmbedding,
) -> Result<()> {
    // Store the main fused embedding as a node
    let parent_node = graph.add_node(NodeType::Embedding, &fused.vector).await?;

    // Store individual token nodes with links to parent
    if let Some(attribution) = fused.token_attribution() {
        for graph_node in attribution.to_graph_nodes(Some(parent_node.id)) {
            let token_node = graph.add_node(
                NodeType::Token,
                &graph_node.embedding,
            ).await?;

            // Link token to parent embedding
            graph.add_edge(
                token_node.id,
                parent_node.id,
                EdgeType::PartOf,
                graph_node.position as f32,
            ).await?;
        }
    }

    Ok(())
}
```

### Handling Subword Tokenization

```rust
impl TokenVectorAttribution {
    /// Merge subword tokens for display (e.g., "learn" + "##ing" -> "learning")
    pub fn merge_subwords(&self) -> Vec<MergedToken> {
        let mut merged = Vec::new();
        let mut current_word = String::new();
        let mut current_vectors = Vec::new();
        let mut word_start = 0;

        for token in self.content_tokens() {
            if token.token.starts_with("##") {
                // Continuation of previous word
                current_word.push_str(&token.token[2..]);
                current_vectors.push(&token.vector);
            } else {
                // New word - save previous if exists
                if !current_word.is_empty() {
                    merged.push(MergedToken {
                        word: std::mem::take(&mut current_word),
                        vectors: std::mem::take(&mut current_vectors),
                        char_span: (word_start, token.char_span.map(|(s, _)| s).unwrap_or(0)),
                    });
                }
                current_word = token.token.clone();
                current_vectors = vec![&token.vector];
                word_start = token.char_span.map(|(s, _)| s).unwrap_or(0);
            }
        }

        // Don't forget the last word
        if !current_word.is_empty() {
            merged.push(MergedToken {
                word: current_word,
                vectors: current_vectors,
                char_span: (word_start, self.original_text.len()),
            });
        }

        merged
    }
}

#[derive(Debug)]
pub struct MergedToken<'a> {
    pub word: String,
    pub vectors: Vec<&'a Vec<f32>>,
    pub char_span: (usize, usize),
}
```

---
*Task ID: M03-L35*
*Layer: Logic*
*Module: 03 - 12-Model Embedding Pipeline*
