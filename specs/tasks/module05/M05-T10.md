# M05-T10: Implement Surprise Computation Methods

| Field | Value |
|-------|-------|
| **Task ID** | M05-T10 |
| **Module** | context-graph-utl |
| **Status** | Pending |
| **Priority** | P0 (Critical Path) |
| **Depends On** | M05-T09 (KL Divergence Computation) |
| **Estimated Hours** | 3 |
| **Constitution Refs** | TECH-UTL-005 Section 5, SPEC-UTL-005 Section 3.2 |

---

## Executive Summary

Implement the core surprise computation methods that combine KL divergence with centroid computation and distance-based fallbacks. These methods compute delta_s (surprise signal) for the UTL learning equation by comparing observed embeddings against context embeddings.

**Key Functions:**
- `compute_surprise_kl()` - KL divergence-based surprise from context centroid
- `compute_surprise_distance()` - Cosine distance-based alternative
- Context centroid computation with weighted averaging

---

## Implementation Requirements

### File Location

**Source File:** `crates/context-graph-utl/src/surprise/kl_divergence.rs`

### Dependencies from M05-T09

The following must be available from M05-T09:

```rust
// From M05-T09: kl_divergence.rs
pub fn kl_divergence(p: &[f32], q: &[f32], epsilon: f32) -> f32;
pub fn softmax_normalize(values: &[f32], temperature: f32) -> Vec<f32>;
pub fn cosine_similarity(a: &[f32], b: &[f32]) -> f32;
```

### Functions to Implement

#### 1. `compute_centroid`

```rust
/// Compute the centroid (mean) of context embeddings.
///
/// # Arguments
///
/// * `embeddings` - Slice of embedding vectors (each Vec<f32> has dimension D)
///
/// # Returns
///
/// Centroid vector of dimension D, or None if embeddings is empty
///
/// # Performance
///
/// O(n * d) where n = embeddings.len(), d = dimension
pub fn compute_centroid(embeddings: &[Vec<f32>]) -> Option<Vec<f32>> {
    if embeddings.is_empty() {
        return None;
    }

    let dim = embeddings[0].len();
    let mut centroid = vec![0.0f32; dim];
    let n = embeddings.len() as f32;

    for embedding in embeddings {
        for (i, &val) in embedding.iter().enumerate() {
            centroid[i] += val;
        }
    }

    for val in &mut centroid {
        *val /= n;
    }

    Some(centroid)
}
```

#### 2. `compute_surprise_kl`

```rust
use super::kl_divergence::{kl_divergence, softmax_normalize, cosine_similarity};
use crate::config::SurpriseConfig;

/// Compute surprise using KL divergence from context centroid.
///
/// Algorithm:
/// 1. Compute centroid of context embeddings
/// 2. Softmax normalize observed and centroid vectors
/// 3. Compute KL divergence between distributions
/// 4. Normalize to [0, 1] using max_kl_value
///
/// # Arguments
///
/// * `observed` - The observed embedding vector (dimension D)
/// * `context_embeddings` - Slice of recent context embeddings
/// * `config` - Surprise computation configuration
///
/// # Returns
///
/// Surprise value delta_s in range [0, 1]
///
/// # Fallback Behavior
///
/// - Returns `max_surprise_no_context` when context is empty
/// - Falls back to distance method when context < min_context_for_kl
///
/// # Performance Target
///
/// <5ms including centroid computation for 50 context vectors
pub fn compute_surprise_kl(
    observed: &[f32],
    context_embeddings: &[Vec<f32>],
    config: &SurpriseConfig,
) -> f32 {
    // Handle empty context
    if context_embeddings.is_empty() {
        return config.max_surprise_no_context;
    }

    // Fall back to distance method if insufficient context for reliable KL
    if context_embeddings.len() < config.min_context_for_kl {
        return compute_surprise_distance(observed, context_embeddings, config);
    }

    // Compute context centroid
    let centroid = match compute_centroid(context_embeddings) {
        Some(c) => c,
        None => return config.max_surprise_no_context,
    };

    // Softmax normalize both vectors to create probability distributions
    let p = softmax_normalize(observed, config.kl.temperature);
    let q = softmax_normalize(&centroid, config.kl.temperature);

    // Compute KL divergence
    let kl_raw = kl_divergence(&p, &q, config.kl.epsilon);

    // Normalize to [0, 1] range
    let surprise = (kl_raw / config.kl.max_kl_value).clamp(0.0, 1.0);

    surprise
}
```

#### 3. `compute_surprise_distance`

```rust
/// Compute surprise using cosine distance from context centroid.
///
/// Alternative to KL divergence when context is small or for ensemble use.
///
/// # Formula
///
/// surprise = (1 - cosine_similarity(observed, centroid)) / 2
///
/// This maps cosine similarity [-1, 1] to surprise [0, 1]:
/// - Perfect match (sim=1) -> surprise=0
/// - Orthogonal (sim=0) -> surprise=0.5
/// - Opposite (sim=-1) -> surprise=1
///
/// # Arguments
///
/// * `observed` - The observed embedding vector
/// * `context_embeddings` - Slice of context embeddings
/// * `config` - Surprise configuration
///
/// # Returns
///
/// Surprise value in range [0, 1]
pub fn compute_surprise_distance(
    observed: &[f32],
    context_embeddings: &[Vec<f32>],
    config: &SurpriseConfig,
) -> f32 {
    if context_embeddings.is_empty() {
        return config.max_surprise_no_context;
    }

    let centroid = match compute_centroid(context_embeddings) {
        Some(c) => c,
        None => return config.max_surprise_no_context,
    };

    let similarity = cosine_similarity(observed, &centroid);

    // Map similarity [-1, 1] to surprise [0, 1]
    let surprise = (1.0 - similarity) / 2.0;

    surprise.clamp(0.0, 1.0)
}
```

#### 4. `compute_weighted_centroid` (Optional Enhancement)

```rust
/// Compute weighted centroid with recency decay.
///
/// More recent embeddings contribute more to the centroid.
///
/// # Arguments
///
/// * `embeddings` - Slice of embeddings (most recent last)
/// * `decay` - Decay factor (0.95 means each older entry weighted 0.95x previous)
///
/// # Returns
///
/// Weighted centroid vector
pub fn compute_weighted_centroid(
    embeddings: &[Vec<f32>],
    decay: f32,
) -> Option<Vec<f32>> {
    if embeddings.is_empty() {
        return None;
    }

    let dim = embeddings[0].len();
    let mut centroid = vec![0.0f32; dim];
    let mut total_weight = 0.0f32;

    // Apply exponential decay (most recent = highest weight)
    for (i, embedding) in embeddings.iter().rev().enumerate() {
        let weight = decay.powi(i as i32);
        total_weight += weight;

        for (j, &val) in embedding.iter().enumerate() {
            centroid[j] += val * weight;
        }
    }

    // Normalize by total weight
    if total_weight > 0.0 {
        for val in &mut centroid {
            *val /= total_weight;
        }
    }

    Some(centroid)
}
```

---

## Configuration Reference

From M05-T02 SurpriseConfig:

```rust
pub struct SurpriseConfig {
    pub kl_weight: f32,              // 0.6
    pub distance_weight: f32,        // 0.4
    pub kl: KlConfig,
    pub context_window_size: usize,  // 50
    pub context_decay: f32,          // 0.95
    pub max_surprise_no_context: f32, // 0.9
    pub min_context_for_kl: usize,   // 3
}

pub struct KlConfig {
    pub epsilon: f32,                // 1e-10
    pub max_kl_value: f32,           // 10.0
    pub temperature: f32,            // 1.0
}
```

---

## Test Cases

### File Location

**Test File:** `crates/context-graph-utl/tests/surprise_tests.rs`

### Required Tests

```rust
#[cfg(test)]
mod surprise_computation_tests {
    use super::*;
    use crate::config::{SurpriseConfig, KlConfig};

    fn default_config() -> SurpriseConfig {
        SurpriseConfig::default()
    }

    // ========== Centroid Tests ==========

    #[test]
    fn test_compute_centroid_empty() {
        let result = compute_centroid(&[]);
        assert!(result.is_none());
    }

    #[test]
    fn test_compute_centroid_single() {
        let embeddings = vec![vec![1.0, 2.0, 3.0]];
        let centroid = compute_centroid(&embeddings).unwrap();
        assert_eq!(centroid, vec![1.0, 2.0, 3.0]);
    }

    #[test]
    fn test_compute_centroid_multiple() {
        let embeddings = vec![
            vec![1.0, 0.0, 0.0],
            vec![0.0, 1.0, 0.0],
            vec![0.0, 0.0, 1.0],
        ];
        let centroid = compute_centroid(&embeddings).unwrap();
        let expected = vec![1.0/3.0, 1.0/3.0, 1.0/3.0];
        for (a, b) in centroid.iter().zip(expected.iter()) {
            assert!((a - b).abs() < 1e-6);
        }
    }

    // ========== KL Surprise Tests ==========

    #[test]
    fn test_surprise_kl_empty_context() {
        let config = default_config();
        let observed = vec![0.1; 128];
        let result = compute_surprise_kl(&observed, &[], &config);
        assert_eq!(result, config.max_surprise_no_context);
    }

    #[test]
    fn test_surprise_kl_identical_to_context() {
        let config = default_config();
        let embedding = vec![0.5; 128];
        let context = vec![embedding.clone(); 10];
        let result = compute_surprise_kl(&embedding, &context, &config);
        // Identical should yield very low surprise
        assert!(result < 0.1, "Expected low surprise for identical, got {}", result);
    }

    #[test]
    fn test_surprise_kl_orthogonal_context() {
        let config = default_config();
        let observed: Vec<f32> = (0..128).map(|i| if i < 64 { 1.0 } else { 0.0 }).collect();
        let context_embed: Vec<f32> = (0..128).map(|i| if i >= 64 { 1.0 } else { 0.0 }).collect();
        let context = vec![context_embed; 10];
        let result = compute_surprise_kl(&observed, &context, &config);
        // Orthogonal should yield moderate to high surprise
        assert!(result > 0.3, "Expected high surprise for orthogonal, got {}", result);
    }

    #[test]
    fn test_surprise_kl_returns_clamped() {
        let config = default_config();
        let observed = vec![1.0; 128];
        let context_embed = vec![-1.0; 128];
        let context = vec![context_embed; 10];
        let result = compute_surprise_kl(&observed, &context, &config);
        assert!(result >= 0.0 && result <= 1.0, "Result {} not in [0,1]", result);
    }

    #[test]
    fn test_surprise_kl_falls_back_to_distance() {
        let mut config = default_config();
        config.min_context_for_kl = 5;
        let observed = vec![0.5; 128];
        // Only 3 context vectors, should fall back to distance method
        let context = vec![vec![0.5; 128]; 3];
        let result = compute_surprise_kl(&observed, &context, &config);
        assert!(result >= 0.0 && result <= 1.0);
    }

    // ========== Distance Surprise Tests ==========

    #[test]
    fn test_surprise_distance_empty_context() {
        let config = default_config();
        let observed = vec![0.1; 128];
        let result = compute_surprise_distance(&observed, &[], &config);
        assert_eq!(result, config.max_surprise_no_context);
    }

    #[test]
    fn test_surprise_distance_identical() {
        let config = default_config();
        let embedding = vec![1.0; 128];
        let context = vec![embedding.clone()];
        let result = compute_surprise_distance(&embedding, &context, &config);
        assert!(result < 0.01, "Expected ~0 surprise for identical, got {}", result);
    }

    #[test]
    fn test_surprise_distance_orthogonal() {
        let config = default_config();
        let observed: Vec<f32> = (0..128).map(|i| if i < 64 { 1.0 } else { 0.0 }).collect();
        let context_embed: Vec<f32> = (0..128).map(|i| if i >= 64 { 1.0 } else { 0.0 }).collect();
        let context = vec![context_embed];
        let result = compute_surprise_distance(&observed, &context, &config);
        // Orthogonal vectors have cosine similarity ~0, so surprise should be ~0.5
        assert!((result - 0.5).abs() < 0.1, "Expected ~0.5 for orthogonal, got {}", result);
    }

    // ========== Weighted Centroid Tests ==========

    #[test]
    fn test_weighted_centroid_decay() {
        let embeddings = vec![
            vec![0.0, 0.0], // oldest
            vec![1.0, 1.0], // newest
        ];
        let centroid = compute_weighted_centroid(&embeddings, 0.5).unwrap();
        // Newest (index 0 from end) gets weight 1.0
        // Oldest (index 1 from end) gets weight 0.5
        // Weighted sum: [0*0.5 + 1*1.0, 0*0.5 + 1*1.0] = [1.0, 1.0]
        // Total weight: 1.5
        // Centroid: [1.0/1.5, 1.0/1.5] = [0.667, 0.667]
        let expected = 1.0 / 1.5;
        assert!((centroid[0] - expected).abs() < 1e-3);
    }

    // ========== Performance Tests ==========

    #[test]
    fn test_surprise_kl_performance() {
        let config = default_config();
        let observed = vec![0.5; 1536]; // Full E7_Code dimension
        let context: Vec<Vec<f32>> = (0..50)
            .map(|i| vec![(i as f32 * 0.01).sin(); 1536])
            .collect();

        let start = std::time::Instant::now();
        for _ in 0..100 {
            let _ = compute_surprise_kl(&observed, &context, &config);
        }
        let elapsed = start.elapsed();
        let per_op = elapsed / 100;

        assert!(per_op.as_millis() < 5,
            "Performance target missed: {:?} per operation", per_op);
    }
}
```

---

## Acceptance Criteria

### Functional Requirements

- [ ] `compute_centroid()` returns None for empty input
- [ ] `compute_centroid()` returns correct mean of embeddings
- [ ] `compute_surprise_kl()` returns `max_surprise_no_context` when context is empty
- [ ] `compute_surprise_kl()` falls back to distance method when context < `min_context_for_kl`
- [ ] `compute_surprise_kl()` returns delta_s in [0, 1] range
- [ ] `compute_surprise_distance()` uses cosine similarity correctly
- [ ] `compute_surprise_distance()` maps similarity to surprise correctly
- [ ] All results are clamped to valid ranges

### Performance Requirements

- [ ] `compute_surprise_kl()` executes in <5ms for 50 context vectors of dimension 1536
- [ ] No heap allocations in hot path beyond centroid vector
- [ ] Efficient SIMD-friendly loops where possible

### Code Quality Requirements

- [ ] No `unwrap()` or `expect()` in production code
- [ ] All public functions have rustdoc comments
- [ ] All edge cases handled (empty input, zero vectors, NaN prevention)
- [ ] Unit tests achieve >90% coverage

---

## Integration Notes

### Upstream Dependencies

- M05-T09: `kl_divergence()`, `softmax_normalize()`, `cosine_similarity()`

### Downstream Consumers

- M05-T11: `SurpriseCalculator` uses these methods for ensemble surprise
- M05-T22: `UtlProcessor` orchestrates surprise computation

---

## Revision History

| Date | Author | Changes |
|------|--------|---------|
| 2026-01-04 | AI Agent | Initial atomic task creation from M05 master spec |
