# M05-T20: Implement Core UTL Learning Magnitude Function

```yaml
task_id: M05-T20
title: "Implement Core UTL Learning Magnitude Function"
module: "Module 5: UTL Integration"
layer: "surface"
priority: "critical"
status: "pending"
estimated_hours: 2
created: "2026-01-04"

file_path: "crates/context-graph-utl/src/lib.rs"
test_file: "crates/context-graph-utl/tests/utl_core_tests.rs"

dependencies: []

spec_refs:
  - "TECH-UTL-005 Section 2.1"
  - "SPEC-UTL-005 Section 2"
  - "REQ-UTL-001"
```

---

## Description

Implement `compute_learning_magnitude(delta_s, delta_c, w_e, phi)` function - the core UTL computation formula.

### Formula

```
L = sigmoid((delta_s * delta_c) * w_e * cos(phi) * LEARNING_SCALE_FACTOR)
```

Where:
- `delta_s`: Surprise (entropy) - how novel the information is [0, 1]
- `delta_c`: Coherence - how well it fits existing knowledge [0, 1]
- `w_e`: Emotional weight - emotional salience factor [0.5, 1.5]
- `phi`: Phase angle - consolidation timing [0, PI]
- `LEARNING_SCALE_FACTOR`: Constant = 2.0
- `sigmoid(x)`: 1 / (1 + exp(-x))

### Input Constraints

| Parameter | Range | Clamping Required |
|-----------|-------|-------------------|
| delta_s   | [0, 1] | Yes |
| delta_c   | [0, 1] | Yes |
| w_e       | [0.5, 1.5] | Yes |
| phi       | [0, PI] | Yes |

### Functions to Implement

1. **`sigmoid(x: f32) -> f32`**
   - Standard sigmoid activation
   - Returns value in (0, 1) range

2. **`compute_learning_magnitude(delta_s: f32, delta_c: f32, w_e: f32, phi: f32) -> f32`**
   - Core UTL formula implementation
   - All inputs clamped before computation
   - Result guaranteed in [0, 1]

3. **`compute_learning_magnitude_weighted(delta_s: f32, delta_c: f32, w_e: f32, phi: f32, lambda_weights: &LifecycleLambdaWeights) -> f32`**
   - Marblestone integration variant
   - Applies lifecycle lambda weights to surprise/coherence components

---

## Implementation Requirements

### Constants

```rust
/// Learning scale factor per TECH-UTL-005
pub const LEARNING_SCALE_FACTOR: f32 = 2.0;
```

### Sigmoid Function

```rust
/// Standard sigmoid activation function
/// Returns value in (0, 1) range
#[inline]
pub fn sigmoid(x: f32) -> f32 {
    1.0 / (1.0 + (-x).exp())
}
```

### Core Function Signature

```rust
/// Compute UTL learning magnitude from components
///
/// # Arguments
/// * `delta_s` - Surprise (entropy) value [0, 1]
/// * `delta_c` - Coherence value [0, 1]
/// * `w_e` - Emotional weight [0.5, 1.5]
/// * `phi` - Phase angle [0, PI]
///
/// # Returns
/// Learning magnitude L in [0, 1]
///
/// # Anti-patterns
/// - Never returns NaN or Infinity
/// - Always clamps inputs before computation
#[inline]
pub fn compute_learning_magnitude(
    delta_s: f32,
    delta_c: f32,
    w_e: f32,
    phi: f32,
) -> f32 {
    // Clamp all inputs
    let delta_s = delta_s.clamp(0.0, 1.0);
    let delta_c = delta_c.clamp(0.0, 1.0);
    let w_e = w_e.clamp(0.5, 1.5);
    let phi = phi.clamp(0.0, std::f32::consts::PI);

    // Core UTL formula
    let raw = (delta_s * delta_c) * w_e * phi.cos() * LEARNING_SCALE_FACTOR;

    // Apply sigmoid for bounded output
    sigmoid(raw)
}
```

### Weighted Variant for Marblestone

```rust
/// Compute UTL learning magnitude with Marblestone lambda weights
///
/// Applies lifecycle-aware weighting to surprise and coherence components:
/// - `delta_s_weighted = delta_s * lambda_novelty`
/// - `delta_c_weighted = delta_c * lambda_consolidation`
#[inline]
pub fn compute_learning_magnitude_weighted(
    delta_s: f32,
    delta_c: f32,
    w_e: f32,
    phi: f32,
    lambda_weights: &LifecycleLambdaWeights,
) -> f32 {
    // Apply lambda weights before clamping
    let (weighted_s, weighted_c) = lambda_weights.apply(delta_s, delta_c);

    compute_learning_magnitude(weighted_s, weighted_c, w_e, phi)
}
```

---

## Performance Requirements

| Metric | Target |
|--------|--------|
| Execution time | < 100us |
| Memory allocation | Zero heap allocations |
| Inlining | `#[inline]` for hot path |

---

## Acceptance Criteria

- [ ] `compute_learning_magnitude()` returns L in [0, 1]
- [ ] All inputs clamped before computation
- [ ] Result is never NaN or Infinity
- [ ] `sigmoid()` helper function works correctly
- [ ] `compute_learning_magnitude_weighted()` applies lambda weights
- [ ] Performance: <100us
- [ ] `#[inline]` annotation for hot path
- [ ] Unit tests verify mathematical properties
- [ ] Edge case tests for boundary values

---

## Test Cases

### Unit Tests

```rust
#[cfg(test)]
mod tests {
    use super::*;
    use approx::assert_relative_eq;

    #[test]
    fn test_sigmoid_center() {
        // sigmoid(0) = 0.5
        assert_relative_eq!(sigmoid(0.0), 0.5, epsilon = 1e-6);
    }

    #[test]
    fn test_sigmoid_bounds() {
        // Large positive -> 1.0
        assert!(sigmoid(10.0) > 0.99);
        // Large negative -> 0.0
        assert!(sigmoid(-10.0) < 0.01);
    }

    #[test]
    fn test_learning_magnitude_zero_input() {
        // Zero surprise = zero learning signal before sigmoid
        let result = compute_learning_magnitude(0.0, 1.0, 1.0, 0.0);
        assert_relative_eq!(result, 0.5, epsilon = 1e-6); // sigmoid(0) = 0.5
    }

    #[test]
    fn test_learning_magnitude_max_input() {
        // Maximum surprise, coherence, emotional weight, encoding phase
        let result = compute_learning_magnitude(1.0, 1.0, 1.5, 0.0);
        // 1.0 * 1.0 * 1.5 * cos(0) * 2.0 = 3.0
        // sigmoid(3.0) ≈ 0.9526
        assert!(result > 0.9);
    }

    #[test]
    fn test_learning_magnitude_consolidation_phase() {
        // cos(PI) = -1, should reduce magnitude
        let result = compute_learning_magnitude(1.0, 1.0, 1.0, std::f32::consts::PI);
        // sigmoid(-2.0) ≈ 0.119
        assert!(result < 0.2);
    }

    #[test]
    fn test_clamping_out_of_range() {
        // Values out of range should be clamped
        let result = compute_learning_magnitude(-0.5, 2.0, 3.0, 10.0);
        assert!(result >= 0.0 && result <= 1.0);
        assert!(!result.is_nan());
        assert!(result.is_finite());
    }

    #[test]
    fn test_never_nan() {
        // Various edge cases should never produce NaN
        let test_cases = [
            (0.0, 0.0, 0.5, 0.0),
            (1.0, 1.0, 1.5, std::f32::consts::PI),
            (0.5, 0.5, 1.0, std::f32::consts::FRAC_PI_2),
            (f32::NAN, 0.5, 1.0, 0.0), // NaN input should clamp
        ];

        for (delta_s, delta_c, w_e, phi) in test_cases {
            let result = compute_learning_magnitude(delta_s, delta_c, w_e, phi);
            assert!(!result.is_nan(), "NaN produced for inputs: {:?}", (delta_s, delta_c, w_e, phi));
            assert!(result.is_finite());
        }
    }
}
```

### Benchmark Test

```rust
#[bench]
fn bench_compute_learning_magnitude(b: &mut Bencher) {
    b.iter(|| {
        compute_learning_magnitude(0.7, 0.6, 1.2, 0.5)
    });
}
```

---

## Verification Commands

```bash
# Run unit tests
cargo test -p context-graph-utl --lib -- compute_learning_magnitude

# Run benchmark
cargo bench -p context-graph-utl -- learning_magnitude

# Check for NaN/Infinity issues
cargo test -p context-graph-utl --lib -- never_nan
```

---

## Related Tasks

| Task | Relationship |
|------|--------------|
| M05-T21 | Uses result in LearningSignal |
| M05-T22 | Orchestrated by UtlProcessor |
| M05-T06 | Uses LifecycleLambdaWeights for weighted variant |
| M05-T25 | Benchmark verification |
